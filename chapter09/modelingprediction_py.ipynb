{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "snippet:ols"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept   -0.089560\n",
       "age          0.067620\n",
       "gender       0.176665\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "df = pd.read_csv('http://cssbook.net/d/mediause.csv')\n",
    "model = smf.ols(formula = 'newspaper ~ age + gender', data = df).fit()\n",
    "# model.summary() would give a lot more info, but we only care about the coefficients:\n",
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "snippet:olspredict"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.439508\n",
       "1    2.615248\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = pd.DataFrame([{'gender':1, 'age':20}, {'gender': 0, 'age':40} ])\n",
    "model.predict(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "snippet:preparedata"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many people used online news at all?\n",
      "True     1262\n",
      "False     803\n",
      "Name: uses-internet, dtype: int64\n",
      "We have 1652 training and 413 test cases.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('http://cssbook.net/d/mediause.csv')\n",
    "\n",
    "df['uses-internet'] = df['internet']>0\n",
    "df.dropna(inplace=True)\n",
    "print(\"How many people used online news at all?\")\n",
    "print(df['uses-internet'].value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['age', 'education', 'gender']], df['uses-internet'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'We have {len(X_train)} training and {len(X_test)} test cases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "snippet:nb"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "myclassifier = GaussianNB()\n",
    "myclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = myclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:classificationreport"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 55 106]\n",
      " [ 40 212]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.34      0.43       161\n",
      "        True       0.67      0.84      0.74       252\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       413\n",
      "   macro avg       0.62      0.59      0.59       413\n",
      "weighted avg       0.63      0.65      0.62       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:logreg"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "myclassifier = LogisticRegression(solver='lbfgs')\n",
    "myclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = myclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "snippet:svm"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damian/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# !!! We normalize our features to have M = 0 and SD = 1\n",
    "# This is necessary as our features are not measured on the same scale, which SVM requires\n",
    "# It may also be OK to rescale to a range of [0:1] or [-1:1]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "myclassifier = SVC(gamma='scale')\n",
    "myclassifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = myclassifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "snippet:randomforest"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "myclassifier = RandomForestClassifier(n_estimators=100)\n",
    "myclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = myclassifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "snippet:crossval"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69320843 0.64563107 0.65542169 0.66024096 0.66346154]\n",
      "M = 0.66, SD = 0.016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "f1scores = cross_val_score(estimator=myclassifier, X=X_train, y=y_train, scoring='f1', cv=5)\n",
    "print(f1scores)\n",
    "print(f\"M = {f1scores.mean():.2f}, SD = {f1scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "snippet:gridsearch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these hyperparameters {'bootstrap': True, 'criterion': 'gini', 'n_estimators': 200}, we get the best performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.45      0.37      0.41       161\n",
      "        True       0.64      0.71      0.67       252\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       413\n",
      "   macro avg       0.55      0.54      0.54       413\n",
      "weighted avg       0.57      0.58      0.57       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "myclassifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "grid = {\n",
    "    'n_estimators' : [10, 50, 100, 200], \n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "search = GridSearchCV(estimator=myclassifier,\n",
    "                     param_grid=grid,\n",
    "                     scoring='f1',\n",
    "                     cv=5)\n",
    "search.fit(X_train, y_train)\n",
    "print(f'Using these hyperparameters {search.best_params_}, we get the best performance:')\n",
    "print(classification_report(y_test, search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "snippet:gridsearch2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  7.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  7.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these hyperparameters {'C': 100, 'degree': 3, 'kernel': 'poly'}, we get the best performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.04      0.08       161\n",
      "        True       0.62      0.98      0.76       252\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       413\n",
      "   macro avg       0.60      0.51      0.42       413\n",
      "weighted avg       0.60      0.62      0.49       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "myclassifier = SVC(gamma='scale')\n",
    "\n",
    "grid = {\n",
    "    'C' : [100, 1e4], \n",
    "    'kernel': ['linear','rbf', 'poly'],\n",
    "    'degree': [3,4]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(estimator=myclassifier,\n",
    "                      param_grid=grid,\n",
    "                      scoring='f1',\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,  # use all cpus\n",
    "                      verbose=10)\n",
    "search.fit(X_train_scaled, y_train)\n",
    "print(f'Using these hyperparameters {search.best_params_}, we get the best performance:')\n",
    "print(classification_report(y_test, search.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
