\section{Simple exploratory data analysis}

Now that you are familiar with data structures [CHAPTER 5], data management [CHAPTER 6] and data wrangling [CHAPTER 7] you are probably eager to get some real insights of your data and expand the basic techniques that we introduced in CHAPTER2. Well, as a computational communication analyst the first step to get valuable and meaningful information of your data is to conduct exploratory statistics in order to obtain frequency tables, cross-tabulation and summary statistics (mean, median, mode, etc.)  This exploratory analysis is normally necessary even if your research questions or hypotheses require further complex analysis. In fact, from a data-driven approach we seldom have previous clear questions or well-grounded hypotheses, which makes the exploratory data analysis extremely relevant. Social scientists and data practitioners can tell that a significant amount of their journeys is invested in this stage.

Exploratory data analysis (EDA), as originally conceived by Tuke\cite{tukey1977exploratory}, can actually be a very powerful framework to prepare and evaluate data, as well as to understand its properties and generate insights at any stage of your research.  It is mandatory for every analyst to apply EDA before any sophisticated analysis to know if our data is clean enough, if we have missing values and outliers, and what are the shapes of our distributions. Furthermore, before making any proper multivariate or inferential analysis we might want to know the specific frequencies for each variable, what are their measures of central tendency, how is their dispersion or even how frequencies of different variables can be integrated into a single table to have an initial picture of their interrelations. EDA involves all these analysis and can be easily deployed in R (base code or and tidyverse) and Python (pandas and numpy/scipy).

Imagine we want to mine survey data using existing representative surveys to analyse what are the demographics of Europeans citizens and to what extend they support the arrival either of migrants or refugees to the continent. The Eurobarometer (freely available at the Leibniz Institute for the Social Sciences â€“ GESIS) contains these specific questions since 2015. We might pose questions about the variation of a single variable or also describe the covariation of different variables to find patterns in our data. In this section, we will compute basic statistics to answer to these questions and in the next section we will visualize them by plotting \textit{within} and \textit{between} variables behaviours of a selected group of features of the Eurobarometer conducted in November 2017 to 33,193 Europeans. 


For most of the EDA we will use tidyverse (including: ggplor2, dplyr, tidyr, pusss, tibble, stringr and forcats) in R and pandas in Python. After loading the survey data stored in a cvs file (using the function read\_csv2() of tidyverse in R and the function read\_csv() of pandas in R), checking the dimensions of our dataframe (33193 x 705) and selecting the variables of our interest, we would probably want to have a global picture of each of our variables by getting a frequency table. This table shows the frequency of different outcomes for every case in a distribution. This means that we can know how many cases we have for each number or category in the distribution of every variable, which is useful to have an initial understanding of our data.

\textbf{XXX  load  XXX}
		
		
In the next example, using dplyr and pandas, we selected 13 variables out of the original dataset and renamed the columns using understandable labels. Our first question was to know the distribution of the categorical variable gender, by creating tables that include absolute and relative frequencies. The frequency tables (using functions group\_by() and summarise() of dplyr in R, and value\_counts() of pandas in Python) revealed that 17,718 (53\%) women and 15,477 (46\%) men answered this survey. We can do the same with the level of support of refugees [support\_refugees] (\textit{To what extent do you agree or disagree with the following statement: Our country should help refugees}) and obtain that 4,957 (14.93\%) persons totally agreed with this statement, 12,695 (38\%) tended to agree, 5,931 (16.245) tended to disagree and 3,574 (10.77\%) totally disagreed. Well, now you can tell that you are conducting EDA and getting insights of your data.

\textbf{XXX  frequency  XXX}

However, before going further to any \textit{between} variables analysis, you might have noticed that that there were two categories in the variable support\_refugees that were not even mentioned in the previous analysis. The first was a category to tell that the question was not asked in the country of the responder (\textit{Inap. (not 1 in eu28)}), and the second category reported that the surveyed citizen did not know what to answer or maybe just did not want to (DK). There might be many discussions about what to do with these cases, but one of the most common decisions of a computational analyst is to declare those cases as missing values (NA or nan).  These values represent an important amount of data in many real social and communication analysis (just think that you cannot be forced to answer to every question in a survey!). From a statistical point of view, we can have many approaches to address missing values that go from drop either the rows or columns that contain any of them, to compensate those values doing imputation (predicting which would the value based on its relation with other variables). It goes beyond of this chapter to explain all the imputation methods, but at least we must know how to declare the missing values in our data and how to drop the cases that contain them from our dataset.

In the case of the variable support\_refugees we should first transform the previous mentioned categories into missing values using tidyverse in R (function na\_if()) and numpy in Python (function NaN()) and count count these missing data (6,576 cases) (functions is.na() and isna(), respectively).   Then we may decide to drop all the records that contain these values in our dataset using tydyr (function drop\_na) in R and pandas in Python (function dropna() to drop the records and dropna(axis='columns') if we want to drop columns). By doing this we can have a cleaner dataset and continue more sophisticated EDA with cross-tabulation and summary statistics for group of cases.	

\textbf{XXX  na  XXX}


Now let's imagine that we want to cross tabulate the gender and the support of refugees to have an initial idea of how the relation between these two variables might be. With this purpose we create a contingency table or cross-tabulation to get the frequencies in each combination of categories (using functions group\_by(), summarise() and spread() of dplyr in R, and crosstab() of pandas in Python). From this table you can easily get that 2,501 women totally supported to help refugees and 1,666 men totally did not.  Furthermore, more interesting questions about our data might now arise if we compute summary statistics for group of cases (using again functions group\_by(), summarise() and spread() of dplyr, and base mean() in R, and groupby() of pandas and base mean() in Python). For example, you might wonder what are the average ages of women that totally supported (50.4) or not (52.4) to help of refugees.  This approach will open a huge amount of possible analysis by grouping variables and estimating different statistics beyond the mean, such as count, sum, median, mode, minimum or maximum, among others.