\section{Simple exploratory data analysis}
\label{sec:simpleeda}

Now that you are familiar with data structures (chapter~\ref{chap:filetodata}) and data wrangling (chapter~\ref{chap:datawrangling}) you are probably eager to get some real insights of your data and expand the basic techniques that we introduced in chapter~\ref{chap:fundata}. Well, as a computational communication analyst the first step to get valuable and meaningful information of your data is to conduct exploratory statistics in order to obtain frequency tables, cross-tabulation and summary statistics (mean, median, mode, etc.)  This exploratory analysis is normally necessary even if your research questions or hypotheses require further complex analysis. In fact, from a data-driven approach we seldom have previous clear questions or well-grounded hypotheses, which makes the exploratory data analysis extremely relevant. Social scientists and data practitioners can tell that a significant amount of their journeys is invested in this stage.

Exploratory data analysis (EDA), as originally conceived by \citet{tukey1977exploratory}, can actually be a very powerful framework to prepare and evaluate data, as well as to understand its properties and generate insights at any stage of your research.  It is mandatory for every analyst to apply EDA before any sophisticated analysis to know if our data is clean enough, if we have missing values and outliers, and what are the shapes of our distributions. Furthermore, before making any proper multivariate or inferential analysis we might want to know the specific frequencies for each variable, what are their measures of central tendency, how is their dispersion or even how frequencies of different variables can be integrated into a single table to have an initial picture of their interrelations. EDA involves all these analyses and can be easily deployed in R (base code or and \pkg{tidyverse}) and Python (\pkg{pandas} and \pkg{numpy}/\pkg{scipy}).

Imagine we want to mine survey data using existing representative surveys to analyse what are the demographics of Europeans citizens and to what extend they support the arrival either of migrants or refugees to the continent. The Eurobarometer (freely available at the Leibniz Institute for the Social Sciences â€“ GESIS) contains these specific questions since 2015. We might pose questions about the variation of a single variable or also describe the covariation of different variables to find patterns in our data. In this section, we will compute basic statistics to answer to these questions and in the next section we will visualize them by plotting \textit{within} and \textit{between} variables behaviours of a selected group of features of the Eurobarometer conducted in November 2017 to 33,193 Europeans. 

For most of the EDA we will use \pkg{tidyverse} in R and \pkg{pandas} in Python. After loading a clean version of the survey data\footnote{Original data ZA6928\_v1-0-0.csv was cleaned and prepared for the exercise. The preparation of the data are in the notebooks cleaning\_eurobarometer\_py.ipynb and cleaning\_eurobarometer\_r.ipynb.}  stored in a cvs file (using the \pkg{tidyverse} function \fn{read\_csv} in R and the \pkg{pandas} function \fn{read\_csv} in R), checking the dimensions of our dataframe (33193 x 17), we would probably want to have a global picture of each of our variables by getting a frequency table. This table shows the frequency of different outcomes for every case in a distribution. This means that we can know how many cases we have for each number or category in the distribution of every variable, which is useful to have an initial understanding of our data.

\pyrex[output=both,caption=Load data from Eurobarometer survey and select some variables]{chapter08/load}
		
Our first question was to know the distribution of the categorical variable \textit{gender}, by creating tables that include absolute and relative frequencies. The frequency tables (using the \fn{dplyr} functions \fn{group\_by} and \fn{summarise} in R, and \pkg{pandas} function \fn{value\_counts} in Python) revealed that 17,716 (53.38\%) women and 15,477 (46.63\%) men answered this survey. We can do the same with the level of support of refugees [\textit{support\_refugees}] (\textit{To what extent do you agree or disagree with the following statement: Our country should help refugees}) and obtain that 4,957 (14.93\%) persons totally agreed with this statement, 12,695 (38.25\%) tended to agree, 5,931 (16.24\%) tended to disagree and 3,574 (10.77\%) totally disagreed. Well, now you can tell that you are conducting EDA and getting insights of your data.

%Check why tables outputs in R are not supported well
\pyrex[output=py,caption=Absolute and relative frequencies of support of refugees and gender]{chapter08/frequency}


However, before going further to any \textit{between} variables analysis, you might have noticed that there might be some missing values in the data. These values represent an important amount of data in many real social and communication analysis (just think that you cannot be forced to answer to every question in a survey!). From a statistical point of view, we can have many approaches to address missing values that go from drop either the rows or columns that contain any of them, to compensate those values doing imputation (predicting which would the value based on its relation with other variables), as we did in \refsec{calculate} by replacing the missing values with the column mean. It goes beyond of this chapter to explain all the imputation methods, but at least we must know how to identify the missing values in our data and how to drop the cases that contain them from our dataset.

In the case of the variable \textit{support\_refugees} we can count its missing data (6,576 cases) with base R function \fn{is.na} and base Python function \fn{isna}\footnote{If missing values are not corrected declared (e.g. using strings or numbers such as 999) we should first transform the initial values into proper missing values using the \pkg{tidyverse} function \fn{na\_if} in R and \pkg{numpy} function \fn{NaN} in Python. We conducted this procedure over the original Eurobarometer data in the cleaning stage.}.  Then we may decide to drop all the records that contain these values in our dataset using \pkg{tydyr} function \fn{drop\_na} in R and \pkg{pandas} function \fn{dropna} in Python to drop the records\footnote{We may also use: dropna(axis='columns') if you want to drop columns instead of rows.}. By doing this we can have a cleaner dataset and continue more sophisticated EDA with cross-tabulation and summary statistics for group of cases.	

\pyrex[output=both,caption=Drop missing values]{chapter08/na}

Now let us imagine that we want to cross tabulate the \textit{gender} and \textit{support\_refugees} to have an initial idea of how the relation between these two variables might be. With this purpose we create a contingency table or cross-tabulation to get the frequencies in each combination of categories (using \pkg{dplyr} functions \fn{group\_by}, \fn{summarise} and \fn{spread} in R, and \pkg{pandas} function \fn{crosstab} in Python). From this table you can easily get that 2,178 women totally supported to help refugees and 1,524 men totally did not.  Furthermore, other interesting questions about our data might now arise if we compute summary statistics for group of cases (using again \pkg{dplyr} functions \fn{group\_by}, \fn{summarise} and \fn{spread}, and base \fn{mean} in R; and \pkg{pandas} function \fn{groupby} and base \fn{mean} in Python). For example, you might wonder what are the average ages of women that totally supported (52.42) or not (53.2) to help of refugees.  This approach will open a huge amount of possible analysis by grouping variables and estimating different statistics beyond the mean, such as count, sum, median, mode, minimum or maximum, among others.

%Check why tables outputs in R are not supported well
\pyrex[output=py,caption=Cross tabulation of support of refugees and gender\, and summary statistics]{chapter08/cross}
