\section{Simple exploratory data analysis}
\label{sec:simpleeda}

Now that you are familiar with data structures (\refchap{filetodata}) and data wrangling (\refchap{datawrangling}) you are probably eager to get some real insights of your data beyond the basic techniques we briefly introduced in \refchap{fundata}.

As we outlined in \refchap{introduction}, the computational analysis
of communication can be both bottom-up or top-down, inductive or
deductive.  Just as in traditional research methods \cite[for an
  overview, see, for example,][]{Bryman2012}, sometimes, an inductive
bottom-up approach is a goal in itself: After all, explorative
analyses are invaluable for generating hypothesis that can be tested
in follow-up research. But even when you are conducting a deductive,
hypothesis-testing study, it is a good idea to start by
\emph{describing} your dataset using the tools of exploratory data
analysis to get a better picture of your data. In fact, we could even
go as far as saying that obtaining descriptives like frequency tables,
cross-tabulations, and summary statistics (mean, median, mode, etc.)
is always necessary, even if your research questions or hypotheses
require further complex analysis. For the computational analysis of
communication, a significant amount of time may actually be invested
at this stage.

Exploratory data analysis (EDA), as originally conceived by \citet{tukey1977exploratory}, can be a very powerful framework to prepare and evaluate data, as well as to understand its properties and generate insights at any stage of your research.
It is mandatory to do some EDA before any sophisticated analysis to know if the data is clean enough, if there are missing values and outliers, and how the distributions are shaped.
Furthermore, before making any multivariate or inferential analysis we might want to know the specific frequencies for each variable, their measures of central tendency, their dispersion, and so on. We might also want to integrate frequencies of different variables into a single table to have an initial picture of their interrelations.

To illustrate how to do this in R and Python, we will use existing representative survey data to analyse how the demographics of Europeans citizens and relate to support the arrivalof  either migrants or refugees to the continent. The Eurobarometer (freely available at the Leibniz Institute for the Social Sciences -- GESIS) contains these specific questions since 2015. We might pose questions about the variation of a single variable or also describe the covariation of different variables to find patterns in our data. In this section, we will compute basic statistics to answer to these questions and in the next section we will visualize them by plotting \textit{within} and \textit{between} variables behaviours of a selected group of features of the Eurobarometer conducted in November 2017 to 33,193 Europeans. 

For most of the EDA we will use \pkg{tidyverse} in R and \pkg{pandas} as well \pkg{numpy} and \pkg{scipy} in Python. After loading a clean version of the survey data\footnote{Original data ZA6928\_v1-0-0.csv was cleaned and prepared for the exercise. The preparation of the data are in the notebooks cleaning\_eurobarometer\_py.ipynb and cleaning\_eurobarometer\_r.ipynb.}  stored in a cvs file (using the \pkg{tidyverse} function \fn{read\_csv} in R and the \pkg{pandas} function \fn{read\_csv} in R), checking the dimensions of our dataframe (33193 x 17), we probably want to get a global picture of each of our variables by getting a frequency table. This table shows the frequency of different outcomes for every case in a distribution. This means that we can know how many cases we have for each number or category in the distribution of every variable, which is useful to have an initial understanding of our data.

\begin{feature}
  \textbf{pandas versus pure numpy/scipy} In this book, we use pandas
  dataframes a lot: they make our lifes easier compared to native
  datatypes (\refsec{datatypes}), and they already integrate a lot of
  functionality of underlying math and statistics packages such as
  \pkg{numpy} and \pkg{scipy}. However, you do not have to force your
  data into a datatype if a different structure makes more sense in
  your script. \pkg{numpy} and \pkg{scipy} will happily calculate
  mean, media, skewness, and kurtosis of the values in a list, or the
  correlation between two lists. It's up to you.
\end{feature}


\pyrex[output=both,caption=Load data from Eurobarometer survey and select some variables]{chapter08/load}
		
Let us first get the distribution of the categorical variable \textit{gender} by creating tables that include absolute and relative frequencies. The frequency tables (using the \fn{dplyr} functions \fn{group\_by} and \fn{summarise} in R, and \pkg{pandas} function \fn{value\_counts} in Python) reveales that 17,716 (53.38\%) women and 15,477 (46.63\%) men answered this survey. We can do the same with the level of support of refugees [\textit{support\_refugees}] (\textit{To what extent do you agree or disagree with the following statement: Our country should help refugees}) and obtain that 4,957 (14.93\%) persons totally agreed with this statement, 12,695 (38.25\%) tended to agree, 5,931 (16.24\%) tended to disagree and 3,574 (10.77\%) totally disagreed. 

\begin{ccsexample}
\doublecodex{chapter08/frequency}
\codexoutputtable[Absolute and relative frequencies of gender:]{chapter08/frequency.r}
\doublecodex{chapter08/frequency2}
\codexoutputtable[Absolute and relative frequencies of support of refugees:]{chapter08/frequency2.r}
\caption{Absolute and relative frequencies of support of refugees and gender}
\end{ccsexample}


Before diving any further into any \textit{between} variables analysis, you might have noticed that there might be some missing values in the data. These values represent an important amount of data in many real social and communication analysis (just think that you cannot be forced to answer to every question in a telephone or face-to-face survey!). From a statistical point of view, we can have many approaches to address missing values that go from drop either the rows or columns that contain any of them, to compensate those values doing imputation (predicting which would the value based on its relation with other variables), as we did in \refsec{calculate} by replacing the missing values with the column mean. It goes beyon this chapter to explain all the imputation methods (and, in fact, mean imputation has some serious drawbacks when used in subsequent analysis), but at least we need to know how to identify the missing values in our data and how to drop the cases that contain them from our dataset.

In the case of the variable \textit{support\_refugees} we can count its missing data (6,576 cases) with base R function \fn{is.na} and the pandas method \fn{isna}\footnote{If missing values are not corrected declared (e.g. using strings or numbers such as 999) we should first transform the initial values into proper missing values using the \pkg{tidyverse} function \fn{na\_if} in R and the \pkg{numpy} object \fn{nan} in Python. We conducted did this when cleaning the original Eurobarometer dataset for you.}.  Then we may decide to drop all the records that contain these values in our dataset using \pkg{tydyr} function \fn{drop\_na} in R and \pkg{pandas} function \fn{dropna} in Python to drop the records\footnote{We may also use: dropna(axis='columns') if you want to drop columns instead of rows.}. By doing this we can have a cleaner dataset and continue more sophisticated EDA with cross-tabulation and summary statistics for group of cases.	

\pyrex[output=both,caption=Drop missing values]{chapter08/na}

Now let us crosstabulate the \textit{gender} and \textit{support\_refugees} to have an initial idea of how the relation between these two variables might be. With this purpose we create a contingency table or cross-tabulation to get the frequencies in each combination of categories (using \pkg{dplyr} functions \fn{group\_by}, \fn{summarise} and \fn{spread} in R, and \pkg{pandas} function \fn{crosstab} in Python). From this table you can easily get that 2,178 women totally supported to help refugees and 1,524 men totally did not.  Furthermore, other interesting questions about our data might now arise if we compute summary statistics for group of cases (using again \pkg{dplyr} functions \fn{group\_by}, \fn{summarise} and \fn{spread}, and base \fn{mean} in R; and \pkg{pandas} function \fn{groupby} and base \fn{mean} in Python). For example, you might wonder what are the average ages of women that totally supported (52.42) or not (53.2) to help of refugees.  This approach will open a huge amount of possible analysis by grouping variables and estimating different statistics beyond the mean, such as count, sum, median, mode, minimum or maximum, among others.

%Check why tables outputs in R are not supported well
\pyrex[output=both,caption=Cross tabulation of support of refugees and gender\, and summary statistics]{chapter08/cross}
