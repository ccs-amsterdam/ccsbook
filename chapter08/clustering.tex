\section{Clustering and dimensionality reduction}

So far in this chapter we have reviewed traditional statistical exploratory and visualization techniques that any social scientist must manage. However, a more computational approach in EDA is using machine learning (ML) to let our computer “learn” about our data and in turn give us more initial insights.  ML is a branch of artificial intelligence that uses algorithms to interact with data and obtain some patterns or rules that characterize that data. We normally distinguish between supervised machine learning (SML) and unsupervised machine learning (UML). The main difference between these two approaches is that in SML we give the algorithm some “examples” in order to learn from them, and in UML we just give the algorithm the whole data and ask to learn the patterns directly from it. We will cover SML in the next chapter [CHAPTER 9]. In this chapter, we will focus on UML as a means of finding groups and latent dimensions in our data, which can also help to reduce our number of variables. Specifically, we will use base R and Python’s scikit-learn to conduct k-means clustering, hierarchical clustering and principal component analysis (PCA). 

In data mining, we use clustering as a UML technique that aims to find the relationship between a set of descriptive variables (instead of finding the relationship between those descriptive features and a target variable, as in SML). By doing cluster analysis we can identify underlying groups in our data that we will call \textit{clusters}. Imagine we want to explore how European countries can be grouped based on their average support to refugees/migrants, age and educational level. We might create some \textit{a priori} groups (such as Southern vs. Northern countries), but cluster analysis would be a great method to let the data “talk” and then create the most appropriate groups for this specific case. As in all UML, the groups will come unlabelled and the computational analyst will be in charge of finding an appropriate and meaningful label for each cluster for a better communication of results. In spite of this challenge, clustering is a very powerful approach for many EDA tasks and can help the analysis to discover patterns in data.

K-means is the most known algorithm to perform cluster analysis. It is a method that takes any number of observations (cases) and group them into a given number of clusters based on the proximity of each observation to the mean of the formed cluster (centroid).  Instead of taking the mean, some variations of this algorithm take the median (k-medians) or a representative observation, also called medoid (k-medoids or partitioning around medoids, PAM) as a way to optimize the initial method.

The first thing is to prepare a proper dataset, using only continuous variables, scaling the data (for comparability) and avoiding missing values (drop or impute). In the above example, we will use the variables support to refugees, support to migrants, age and educational level (number of years of education) and will create a dataframe with the mean of all theses variables for each country (each observation will be a country). Before conducting cluster analysis, we should establish how many clusters we want to have. This is a tricky question, since you have to tell k-means how many of them you want to and there are different ways to estimate this number (besides of arbitrarily decide it!). The simplest method to obtain the optimal number of clusters is by estimating the variability within the groups for different executions of the k-means function. This means that we must run k-means for different number of clusters (i.e. 1 to 15 clusters) and then choose the number of clusters that decreases the variability maintaining the highest number of clusters. If it is confusing, don’t worry!  When you generate and plot a vector with the variability, or in more technical words, the within-cluster sum of squares (WSS) obtained after each execution, it is very easy to identify the optimal number: Just look at the bend (\textit{knee} or \textit{elbow}) and you will find the point where it decreases the most and then get the optimal number of clusters (3 clusters in our example).

\pyrex[output=py,caption=Getting the optimal number of clusters]{chapter08/elbow}

Now we can finally conduct k-means. We generate 25 initial random centroids (the algorithm will choose the one that optimizes the cost). The default of this parameter is 1, but it is recommended to set it with a higher number (i.e. 20 to 50) to guaranty the maximum benefit of the method. The function kmeans() of base R and KMeans() of scikit-learn in Python will produce the clustering. You can observe the mean (scaled) for each variable in each cluster, as well as the corresponding cluster for each observation. Using the function fviz\_cluster() of the library factoextra in R, or the fuction scatter() of pyplot  in Python, you can have a great visualization of the clusters! In the provided example you can clearly identify that the clusters correspond to Nordic countries (more support to foreigners, more education and age), Central and Southern European countries (middle support, lower education and age), and Eastern European countries (less support, lower education and age)\footnote{We can re-run this cluster analysis using k-medoids or partitioning around medoids (PAM) and get similar results (the three medoids are: Slovakia, Belgic and Denmark), both in data and visualization. In R you must install the package cluster than contains the function pam(), and in Python the package scikit-learn-extra with the function Kmedoids().} .

\pyrex[output=py,caption=Using Kmeans to group countries based on the average support of refugees and migrants\, age and educational level]{chapter08/kmeans}

Another method to deploy cluster analysis is hierarchical clustering, which builds a hierarchy of clusters that we can visualize in a dendogram.  This algorithm has two versions: a bottom-up approach (observations begin in their own clusters), also called \textit{agglomerative}, and a top-down approach (all observations begin in one cluster), also called \textit{divisive}. We will follow the bottom-up approach in this chapter and when you will look at the dendogram you will realize how this strategy repeatedly combines the two \textit{nearest} clusters at the bottom into a larger one in the top. The distance between clusters is initially estimated for every pair observation points and then put every point in its own cluster in order to get the closest pair of points and iteratively compute the distance between each new cluster and the previous ones. This is the internal rule of the algorithm and we must choose a specific linkage method (complete, single, average or centroid in cluster in R, or ward, complete, average or single in scikit-learn in Python). In the example we will use the function hcut() of factoextra in R and AgglomerativeClustering() of scikit-learn in Python, to compute the hierarchical clustering. We can then plot the dendogram  with fuction plot() of base R and function dendogram() of the module cluster.hierarchy of scipy in Python. The summary of the initial model suggest us 2 clusters (size=2) but by looking at the dendogram you can choose the number of clusters you want to work with by choosing a height (for example 4 to get 3 clusters). 

\pyrex[output=py,caption=Using hierarchical clustering to group countries based on the average support of refugees and migrants\, age and educational level]{chapter08/hc}

If you re-run the hierarchical clustering for 3 clusters and visualize it you will get a graph similar to the one produced by k-means!

\pyrex[output=py,caption=Re-run hierarchical clustering with 3 clusters]{chapter08/hc3}

Finally, we will cover principal component analysis (PCA). This unsupervised method is useful to reduce the dimensionality of your data by creating new uncorrelated variables or \textit{components} that describe the original dataset. PCA uses lineal transformation to create principal components that are ordered by the level of explained variance (the first component will catch the largest variance). We will get as many principal components as number of variables we have in the dataset, but when we look at the cumulative variance we can easily select only few of these components to explain most of the variance and thus work with a smaller and summarised dataframe that might be more convenient for many tasks (i.e. those that require avoiding multicollinearity or just need to be more computationally efficient). By simplifying the complexity of our data we can have a first understanding of how our variables are related and also of how our observations might be grouped. All components have specific loads for each original variable, which can tell you how the old variables are represented in the new components. This statistical technique is especially useful in EDA when working with high dimensional datasets but it can be used in many other situations.  

Let's now conduct a PCA over the Eurobarometer data.  In the provided example we will re-use the sub dataframe containing the means of 4 variables (support to refugees, support to migrants, age and educational level) for each of the 30 European countries (30 x 4). The question is if we can have a new dataframe containing less than 4 variables but that explain most of the variance, or in other words, that represent well our original dataset with less dimensions. As long as our features are usually measured in different scales, it is normally suggested to center (to mean 0) and scale (to standard deviation 1) the data. We can perform the PCA in R using the base function prcomp() and in Python using the function PCA() of the module decomposition of scikit-learn. 

\pyrex[output=py,caption=Principal component analysis (PCA) of a dataframe with 30 records and 4 variables]{chapter08/pca}

The generated object with the PCA contains different elements (in R "sdev",     "rotation", "center",  "scale" and   "x") or attributes in Python (components\_, explained\_variance\_, explained\_variance\_ratio, singular\_values\_, mean\_, n\_components\_, n\_features\_, n\_samples\_ and noise\_variance\_). In the resulting object we can see the values of 4 principal components of each country, and the values of the loadings, technically called eigenvalues, for the variables in each principal component. In our example we can notice that support to refugees and migrants are more represented on PC1, while age and educational level on PC2. If we plot the first two principal components (using base function biplot() in R and and ad hoc function created with pyplot in Python) we can clearly see how the variables are associated with either PC1 or with PC2 (we might also want to plot any pair of the four components!). But we can also get a picture of how countries are grouped based only in this two new variables.

\pyrex[output=py,caption=Plot PC1 and PC2]{chapter08/plot_pca}

So far we are not sure of how many components are enough to accurately represent our data, so we need to know how much variance (which is the square of the standard deviation) is explained by each component. We can get the values and plot both the proportion of explained variance and the cumulative explained variance. We get that the first component explain 55.85\% of the variance, the second 28.11\%, the third 12.07\% and the fourth just 3.97\%. 

\pyrex[output=py,caption=Proportion of variance explained]{chapter08/prop}

When we plot the cumulative explained variance it is easy to identify that with just the two first components we explain 83.96\% of the variance. It might now seem a good deal to reduce our dataset from 4 to 2 variables, or let’s say half of the data, but retaining most of the original information.

\pyrex[output=py,caption=Cumulative explained variance]{chapter08/acum}

And what if we want to use this PCA and deploy a clustering (as explained above) with just these two new variables instead of the four original ones?  Just repeat the k-means procedure but now using a new smaller dataframe selecting PC1 and PC2 from the PCA. After estimating the optimal number of clusters (3 again!) we can compute and visualize the clusters, and get a very similar picture to the one obtained in the previous examples, with little differences such as the change of cluster of the Netherlands (more similar now to the Nordic countries!). 

\pyrex[output=py,caption=Combining PCA to reduce dimensionality and k-means to group countries]{chapter08/new}

This last exercise is a good example of how to combine different techniques in EDA. In the next chapter [CHAPTER 9] we will continue explaining machine learning, but we will focus on supervised methods that will be of mandatory use for many computational analyses of both structured and unstructured data.
