\section{Weighting and selecting documents and terms}
\label{sec:dtmselect}

So far, the DTMs you made in this chapter simply show the count of each word in each document.
Many words, however, are not informative for many questions.
This is especially apparent if you look at a \concept{word cloud},
essentially a plot of the most frequent words in a  \concept{corpus} (set of documents).

\note{
  \textbf{Vectors and a geometric interpretation of document-term matrices}
  We said that a document is represented by a `vector' of numbers, where each number (for a document-term matrix)
  is the frequency of a specific word in that document. This term is also seen in the name for the tokenizer \sklearn:
  a \concept{vectorizer} or function to turn texts into vectors. \\%
  %
  The term \concept{vector} here can be read as just a fancy word for a group of numbers.
  In this meaning, the term is also often used in R, where a column of a dataframe is called a vector,
  and where functions that can be called on a whole vector at once are called \emph{vectorized}. \\%
  %
  More generally, however, a vector in geometry is a point (or line from the origin) in an $n$-dimensional space,
  where n is the length or dimensionality of the vector.
  This is also a very useful interpretation for vectors in text analysis:
  the dimensionality of the space is the number of unique words (columns) in the document-term matrix,
  and each document is a point in that $n$-dimensional space.\\%
  %
  In that interpretation, various geometric distances between documents can be calculated as an indicator for how similar
  two documents are. Techniques that reduce the number of columns in the matrix (such as clustering or topic modeling)
  can then be seen as dimensionality reduction techniques since they turn the DTM into a matrix with lower dimensionality
  (while hopefully retaining as much of the relevant information as possible).
  }\vspace{1em}
  
More formally, a document-term matrix can be seen as a representation of data points about documents:
each document (row) is represented as a vector containing any count per word (column).
Although it is a simplification compared to the original text,
an unfiltered document-term matrix contains all relevant information.
For example, if a president uses the word `terrorism' more often than the word `economy', that could be an indication of their policy priorities.

However, there is also a lot of \emph{noise} crowding out this \emph{signal}:
as seen in the word cloud in the previous section the most frequent words are generally quite uninformative.
The same holds for words that hardly occur in any document (but still require a column to be represented)
and noisy `words' such as punctuation or technical artefacts like HTML code. 

This section will discuss a number of techniques for cleaning a corpus or document-term matrix in order to minimize the amount of noise: removing stop words, cleaning punctuation and other artefacts, and trimming and weighting.  
As a running example in this section, we will use a collection of tweets from U.S. president Donald Trump.
\refex{trumptweets} shows how to load these tweets into a data frame containing the ID and text of the tweets.
As you can see, this data set contains a lot of non-textual features such as hyperlinks and hash tags as well as regular punctuation and stop words.
Before we can start analysing this data, we need to decide on and perform multiple cleaning steps such as detailed below. 

\pyrex[caption=Top words used in Trump Tweets,format=table,output=py]{chapter11/trumptweets}

Please note that although tweets are perhaps overused as a source of scientific information,
we use them here as an example since they nicely exemplify issues around non-textual elements such as hyperlinks.
See \refchap{scraping} for information on how to use the Twitter and other APIs to collect your own data.

\subsection{Removing stopwords}

A first step in cleaning a DTM is often \concept{stop word removal}.
Words such as `a' and `the' are often called stop words, i.e. words that do not tell us much about the content.
Both \quanteda\ and \sklearn\ include built-in lists of stop words, making it very easy to remove the most common words.
\refex{stopwords} shows the result of specifying `english' stop words to be removed for both packages.

\pyrex[caption=Simple stop word removal,format=png,output=r]{chapter11/stopwords}



Note, however, that it might seem easy to list words like `a` and `and',
but as it turns out there is no single well-defined list of stop words,
and (as always) the best choice depends on your data and your research question.

Linguistically, stop words are generally function words or closed word classes such as determiner or pronoun,
with closed classes meaning that while you can coin new nouns, you can't simply invent a new determiners or prepositions.
However, there are many different stop word lists around, which make different choices and are compatible with
different kinds of preprocessing.
The Python word cloud in \refex{stopwords} shows a nice example of the importance of matching stopwords with the used
tokenization: a central `word' in the cloud is the contraction \emph{'s}.
We are using the NLTK tokenizer, which splits \emph{'s} from the word it was attached to, but the \sklearn\ stop word list
does not include that term.
So, it is important to make sure that the words created by the tokenization match how words appear in the stop word list.

As an example of the substantive choices inherent in using a stop word lists,
consider the word `will'.
As an auxilliary verb, this is probably indeed a stop word: for most substantive questions, there is no difference
whether you will do something or simply do it.
However, `will' can also be a noun (a testament) and a name (e.g. Will Smith).
Simply dropping such words from the corpus can be problematic; see \refsec{nlp} for ways of telling nouns and verbs apart
for more fine-grained filtering.

Moreover, some research questions might actually be interested in certain stop words.
If you are interesting in references to the future or specific modalities,
the word might actually be a key indicator. 
Similarly, if you are studying self-expression on Internet forums, social identity theory, or populist rhetoric,
words like `I', `us' and `them' can actually be very informative.

For this reason, it is always a good idea to understand and inspect what stop word list you are using,
and use a different one or customize it as needed \citep[see also][]{nothman18}.
\refex{stopwords2} shows how you can inspect and customize stop word lists.
For more details on which lists are available and what choices these lists make,
see the package documentation for the \pkg{stopwords} package in Python (part of NLTK) and R (part of quanteda)


\pyrex[caption=Inspecting and Customizing stop word lists]{chapter11/stopwords2}

%J. Nothman, H. Qin and R. Yurchak (2018). “Stop Word Lists in Free Open-source Software Packages”. In Proc. Workshop for NLP Open Source Software.

\subsection{Removing punctuation and noise}


Next to stop words, text often contains punctuation and other things that can be considered `noise' for most research questions.
For example, it could contain emoticons or emoji, twitter hashtags or at-mentions, or html tags or other annotations.

In both Python and R, we can use regular expressions to remove (parts of) words. 
As explained above in \refsec{regular}, regular expressions are a powerful way to specify (sequences of) characters which are to be kept or removed.
You can use this, for example, to remove things like punctuation, emoji, or HTML tags.
This can be done either before or after tokenizing (splitting the text into words):
in other words, we can clean the raw texts or the individual words (tokens). 

In general, if you only want to keep or remove certain words, it is often easiest to do so after tokenization
using a regular expression to select the words to keep or remove.
If you want to remove parts of words (e.g. to remove the leading `\#` in hashtags) it is easiest to do that before tokenization,
that is, as a preprocessing step before the tokenization.
Similarly, if you want to remove a term that would be split by the tokenization (such as hyperlinks),
if can be better to remove them before the tokenization occurs.

\refex{noise} shows how we can use regular expressions to remove noise in Python and R.
To more clearly show the results of each step, it shows the result of each processing step on a single tweet that exemplifies many of the problems described above.
To better understand the tokenization process, we print the tokens in that tweet separated by a vertical bar (\verb+|+). 
As a first cleaning step, we will use a regular expression to remove hyperlinks and HTML entities like \verb|&amp;| from the untokenized texts.
Since both hyperlinks and HTML entities are split over multiple tokens, it would be hard to remove them after tokenization.

\begin{ccsexample}
\doublecodex{chapter11/noise1}
\codex[caption=Output]{chapter11/noise1.r.out}
\doublecodex{chapter11/noise2}
\codex[caption=Output]{chapter11/noise2.r.out}
\doublecodex{chapter11/noise3}
\codex[caption=Output]{chapter11/noise3.r.out}
\caption{Cleaning a single tweet at the text and token level}\label{ex:noise}
\end{ccsexample}

Regular expressions are explained fully in \refsec{regular}, so we will keep the explanation short:
The bar \verb#|# splits the pattern in two parts, i.e. it will match if it finds either of the subpatterns.
The first pattern looks for the literal text \verb#http#, followed by an optional \verb#s# and the sequence \verb#://#.
Then, it takes all non-whitespace characters it finds, i.e. the pattern ends at the next whitespace or end of the text.
The second pattern looks for an ampersand (\verb#&#) followed by one or more letters (\verb#\\w+#), followed by a semicolon (\verb#;#).
This matches HTML escapes like \verb#&amp;# for an ampersand.

In the next step, we process the tokenized text to remove every token that is either a stopword or does not start with a letter. 
In Python, this is done by using a list comprehension (\verb#[process(item) for item in list]#) for tokenizing each document; and a nested list comprehension for filtering each token in each document.
In R this is not needed as the \fn{tokens\_*} functions are \concept{vectorized}, that is, they directly run over all the tokens.

Comparing R and Python, we see that the different tokenization functions mean that \verb|#trump| is removed in R (since it is a token that does not start with a letter),
but in Python the tokenization splits the \verb|#| from the name and the resulting token \verb|trump| is kept.
If we would have used a different tokenizer for Python (e.g. the \cls{WhitespaceTokenizer}) this would have been different again. 
This underscores the importance of inspecting and understanding the results of the specific tokenizer used,
and to make sure that the later steps match these tokenization choices.
Concretely, with the \cls{TreebankWordtokenizer} we would have had to also remove hashtags at the text level rather than the token level. 

\pyrex[caption=Cleaning the whole corpus and making a tag cloud,output=r,format=png]{chapter11/tagcloud}

As a final example, \refex{tagcloud} shows how to filter tokens for the whole corpus, but rather than removing hashtags it keeps only the hash tags to produce a tag cloud. 
In R, this is mostly a pipeline of \quanteda\ functions to create the corpus, tokenize, keep only hashtags, and create a dfm.
To spice up the output we use the \pkg{RColorBrewer} package to set random colors for the tags.
In Python, you can see that we now have a nested list comprehension, where the outer loop iterates over the texts and the inner loop iterates over the tokens in each text.
Next, we make a \verb|do_nothing| function for the vectorizer since the results are already tokenized.
Note that we need to disable lowercasing as otherwise it will try to call \verb|.lower()| on the token lists. 



 

\subsection{Trimming a DTM}

The techniques above both drop terms from the DTM based on specific choices or patterns.
It can also be beneficial to trim a DTM by removing words that occur very infrequently or overly frequently.
For the former, the reason is that if a word only occurs in a very small percentage of documents it is unlikely to be very informative.
Overly frequent words, for example occurring in more than half or 75\% of all documents, function basically like stopwords for this corpus.
In many cases, this can be a result of the selection strategy. If we select all tweets containing 'Trump', the word Trump itself is no longer informative about their content.
It can also be that some words are used as standard phrases, for example `fellow Americans' in state of the union speeches.
If every president in the corpus uses those terms, they are no longer informative about differences between presidents.

\pyrex[caption=Trimming a Document-Term Matrix]{chapter11/trimming}

\refex{trimming} shows how you can use the \concept{relative document frequency} to trim a DTM in Python and R.
We keep only words with a document frequency of between 1\% and 75\%.

Although these are reasonable numbers every choice depends on the corpus and the research question, so it can be a good idea to check which words are dropped.
%\refex{docfreq} shows how you can make a table of all word frequencies in the corpus, and filter for excluded words.
%If you see words that are seemingly relevant to your research question in the top of this list,
%if can be a good idea to tweak the selection criteria. 
Note that dropping words that occur almost never should normally not influence the results that much, since those words do not occur anyway.
However, trimming a DTM to e.g. at least 1\% document frequency often radically reduces the number of words (columns) in the DTM.
Since many algorithms have to assign weights or parameters to each word, this can provide a significant improvement in computing speed or memory use. 

\subsection{Weighting a DTM}

The DTMs created above all use the raw frequencies as cell values.
It can also be useful to weigh the words so more informative words have a higher weight than less informative ones.
A common technique for this is \concept{tf$\cdot$idf} weighting.
This stands for \emph{term frequency - inverse document frequency} and weights each occurrency by its raw frequency (term frequency) corrected for how often it occurs in all documents (inverse document frequency). In a formula, the most common implementation of this weight is given as follows:

$tf\cdot idf(t,d)=tf(t,d)\cdot idf(t)=f_{t,d}\cdot -\log \frac{n_t}{N}$

Where $f_{t,d}$ is the frequency of term $t$ in document $d$, $N$ is the total number of documents, and $n_t$ is the number of documents in which term $t$ occurs. In other words, the term frequency is weighted by the negative log of the fraction of documents in which that term occurs. Since $\log(1)$ is zero, terms that occur in every document are disregarded, and in general the less frequent a term is, the higher the weight will be. 


\pyrex[caption=Tf$\cdot$Idf weighting,output=r,format=table]{chapter11/tfidf}

tf$\cdot$idf weighting is a fairly common technique and can improve the results of subsequent analyses such as supervised machine learning.
As such, it is no surprise that it is easy to apply this in both Python and R, as shown in \refex{tfidf}.
This example uses the same data as \refex{sotu} above, so you can compare the resulting weighted values with the results reported there.
As you can see, the tf$\cdot$idf weighting in both languages have roughly the same effect:
very frequent terms such as \emph{the} are made less important compared to less frequent words such as \emph{submit}.
For example, in the raw frequencies for the 1965 Johnson speech, \emph{the} occurred 355 times compared to \emph{submit} only once.
In the weighted matrix, the weight for \emph{submit} is 4 times as low as the weight for \emph{the}.

There are two more things to note if you compare the examples from R and Python.
First, to make the two cases somewhat comparable we have to use two options for R, namely to set the term frequency to proportional (\verb|scheme_tf='prop'|),
and to add smoothing to the document frequencies (\verb|smooth=1|).
Without those options, the counts for the first columns would all be zero (since they occur in all documents, and $\log \frac{85}{85}=0$),
and the other counts would be greater than one since they would only be weighted, not normalized.

Even with those options the results are still different (in details if not in proportions),
mainly because R normalizes the frequencies before weighting, while Python normalizes after the weighting.
Moreover, Python by default uses L2 normalization, meaning that the length of the document vectors will be one,
while R uses L1 normalization, that is, the row sums are one (before weighting).
Both R and Python have various parameters to control these choices which are explained in their respective help pages.
However, although the differences in absolute values look large, the relative effect of making more frequent terms less important is the same,
and the specific weighting scheme and options will probably not matter that much for the final results.
However, it is always good to be aware of the specific options available and try out which work best for your specific research question.




