{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import eli5\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "# ugly copy paste for now from dictionary_py as I did not manage to import from a .ipynb file\n",
    "# (even though the modules imdb and/or nbimport should be able to do this)\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import bz2\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def get_review_data(filename = \"reviewdata.pickle.bz2\", url = \"http://cssbook.net/d/aclImdb_v1.tar.gz\"):\n",
    "    '''\n",
    "    Checks whether review dataset has already been downloaded.\n",
    "    If not, downloads it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        name of cached file\n",
    "    url : string\n",
    "        url of IMDB dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of lists of strings\n",
    "        reviews_train, reviews_test, label_train, label_test\n",
    "    '''\n",
    "\n",
    "    if Path(filename).exists():\n",
    "        print(f\"Using cached file {filename}\")\n",
    "        with bz2.BZ2File(filename, 'r') as f:\n",
    "            reviews_train, reviews_test, label_train, label_test = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Downloading from {url}\")\n",
    "        fn, _headers = urllib.request.urlretrieve(url, filename=None)\n",
    "        t = tarfile.open(fn, mode=\"r:gz\")\n",
    "        reviews_train, reviews_test, label_train, label_test = [], [], [], []\n",
    "        for file in t.getmembers():\n",
    "            try:\n",
    "                _imdb, dataset, label, _fn = Path(file.name).parts\n",
    "            except ValueError:\n",
    "                # if the Path cannot be parsed, e.g. because it does not consist of exactly four parts, then it is not a part of the dataset but for instance a folder name. Let's skip it then\n",
    "                continue\n",
    "            if dataset == \"train\" and (label=='pos' or label=='neg'):\n",
    "                reviews_train.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "                label_train.append(label)\n",
    "            elif dataset == \"test\" and (label=='pos' or label=='neg'):\n",
    "                reviews_test.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "                label_test.append(label)\n",
    "        print(f\"Saving {len(label_train)} training and {len(label_test)} test cases to {filename}\")\n",
    "        with bz2.BZ2File(filename, 'w') as f:\n",
    "            pickle.dump((reviews_train, reviews_test, label_train, label_test), f)\n",
    "    return reviews_train, reviews_test, label_train, label_test\n",
    "\n",
    "reviews_train, reviews_test, y_train, y_test = get_review_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "snippet:imdbbaseline"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.88      0.83     12500\n",
      "         pos       0.86      0.76      0.81     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "snippet:basiccomparisons"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.77\n",
      "negative reviews:\t 0.79 \t\t 0.88\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.78\n",
      "negative reviews:\t 0.80 \t\t 0.88\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.85\n",
      "negative reviews:\t 0.85 \t\t 0.87\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.89 \t\t 0.88\n",
      "negative reviews:\t 0.88 \t\t 0.89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def short_classification_report (y_test, y_pred):\n",
    "    print(\"                \\t precision \\t recall\")\n",
    "    print(f\"positive reviews:\\t {metrics.precision_score(y_test,y_pred, pos_label='pos'):0.2f} \\t\\t {metrics.recall_score(y_test,y_pred, pos_label='pos'):0.2f}\")\n",
    "    print(f\"negative reviews:\\t {metrics.precision_score(y_test,y_pred, pos_label='neg'):0.2f} \\t\\t {metrics.recall_score(y_test,y_pred, pos_label='neg'):0.2f}\")\n",
    "\n",
    "configurations = [('NB with Count', CountVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "                 ('NB with TfIdf', TfidfVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "                 ('LogReg with Count', CountVectorizer(min_df=5, max_df=.5), LogisticRegression(solver='liblinear')),\n",
    "                 ('LogReg with TfIdf', TfidfVectorizer(min_df=5, max_df=.5), LogisticRegression(solver='liblinear'))]\n",
    "\n",
    "for description, vectorizer, classifier in configurations:\n",
    "    print(description)\n",
    "    X_train = vectorizer.fit_transform(reviews_train)\n",
    "    X_test = vectorizer.transform(reviews_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:basicpipe"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.77\n",
      "negative reviews:\t 0.79 \t\t 0.88\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.78\n",
      "negative reviews:\t 0.80 \t\t 0.88\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.85\n",
      "negative reviews:\t 0.85 \t\t 0.87\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.89 \t\t 0.88\n",
      "negative reviews:\t 0.88 \t\t 0.89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for description, vectorizer, classifier in configurations:\n",
    "    print(description)\n",
    "    pipe = make_pipeline(vectorizer, classifier)\n",
    "    pipe.fit(reviews_train, y_train)\n",
    "    y_pred = pipe.predict(reviews_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:gridsearchlogreg"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 11.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these hyperparameters {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 2)}, we get the best performance:\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.90 \t\t 0.90\n",
      "negative reviews:\t 0.90 \t\t 0.90\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps = [('vectorizer', TfidfVectorizer()), ('classifier', LogisticRegression(solver='liblinear'))])\n",
    "grid = {\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier__C': [0.01, 1, 100]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(estimator=pipeline,\n",
    "                      param_grid=grid,\n",
    "                      scoring='accuracy',   # all classes are balanced, let's just score on accuracy\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,  # use all cpus\n",
    "                      verbose=10)\n",
    "search.fit(reviews_train, y_train)\n",
    "print(f'Using these hyperparameters {search.best_params_}, we get the best performance:')\n",
    "print(short_classification_report(y_test, search.predict(reviews_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "snippet:vader"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/damian/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0]\n",
      " [    6  6688  5806]\n",
      " [    5  1745 10750]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   dont know       0.00      0.00      0.00         0\n",
      "         neg       0.79      0.54      0.64     12500\n",
      "         pos       0.65      0.86      0.74     12500\n",
      "\n",
      "    accuracy                           0.70     25000\n",
      "   macro avg       0.48      0.47      0.46     25000\n",
      "weighted avg       0.72      0.70      0.69     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damian/onderwijs_github/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "y_vader = []\n",
    "for review in reviews_test:\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    if sentiment['compound']>0:\n",
    "        y_vader.append('pos')\n",
    "    elif sentiment['compound']<0:\n",
    "        y_vader.append('neg')\n",
    "    else:\n",
    "        y_vader.append('dont know')\n",
    "# For advanced readers: The whole for loop can be replaced by just two lines (at the expense of being much less readable)\n",
    "# labels = ({1:'pos',-1:'neg',0:'dont know'})\n",
    "# y_vader = [labels.get(np.sign(analyzer.polarity_scores(review)['compound'])) for review in test_texts]\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_vader))\n",
    "print(metrics.classification_report(y_test, y_vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "snippet:reuse"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a great movie' probably belongs to class 'pos'.\n",
      "'I hated this one.' probably belongs to class 'neg'.\n",
      "'What an awful fail' probably belongs to class 'neg'.\n"
     ]
    }
   ],
   "source": [
    "# let's take a vectorizer and classifier...\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=.5)\n",
    "classifier = LogisticRegression(solver='liblinear')\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "classifier.fit(X_train, y_train)\n",
    "# .... and save it:\n",
    "pickle.dump(vectorizer,open(\"myvectorizer.pkl\",mode=\"wb\"))\n",
    "joblib.dump(nb, \"myclassifier.pkl\")\n",
    "\n",
    "#Then, later on, instead of fitting a new vectorizer, you can simply load the old one and use it:\n",
    "\n",
    "listwithnewdata = ['This is a great movie', 'I hated this one.', 'What an awful fail']\n",
    "\n",
    "myvectorizer = pickle.load(open(\"myvectorizer.pkl\",mode=\"rb\"))\n",
    "new_features = vectorizer.transform(listwithnewdata)\n",
    "\n",
    "myclassifier = joblib.load(\"myclassifier.pkl\")\n",
    "predictions = myclassifier.predict(new_features)\n",
    "\n",
    "for review, label in zip(listwithnewdata, predictions):\n",
    "    print(f\"'{review}' probably belongs to class '{label}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "output:html",
     "snippet:eli5"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.173\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        great\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +6.101\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        excellent\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        best\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.791\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        perfect\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13663 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13574 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.337\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        poor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.733\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        boring\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.315\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        waste\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.349\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        awful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -7.347\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        bad\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -9.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worst\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(pipe, top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:eli5b",
     "output:html"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.140</b>, score <b>-1.817</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.830\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"opacity: 0.80\">i </span><span style=\"background-color: hsl(120, 100.00%, 74.97%); opacity: 0.90\" title=\"0.138\">love</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 84.32%); opacity: 0.85\" title=\"-0.071\">am</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.56%); opacity: 0.83\" title=\"0.034\">willing</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 93.55%); opacity: 0.81\" title=\"0.020\">put</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.84%); opacity: 0.80\" title=\"-0.002\">up</span><span style=\"opacity: 0.80\"> with a </span><span style=\"background-color: hsl(120, 100.00%, 84.06%); opacity: 0.85\" title=\"0.072\">lot</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.44%); opacity: 0.81\" title=\"0.012\">movies</span><span style=\"opacity: 0.80\">/</span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(0, 100.00%, 90.89%); opacity: 0.82\" title=\"-0.033\">usually</span><span style=\"opacity: 0.80\"> underfunded, </span><span style=\"background-color: hsl(120, 100.00%, 98.46%); opacity: 0.80\" title=\"0.003\">under</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 78.84%); opacity: 0.88\" title=\"0.109\">appreciated</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 94.35%); opacity: 0.81\" title=\"0.016\">misunderstood</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 83.48%); opacity: 0.86\" title=\"-0.076\">tried</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 96.37%); opacity: 0.81\" title=\"-0.009\">like</span><span style=\"opacity: 0.80\"> this, i </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.20%); opacity: 0.83\" title=\"-0.047\">did</span><span style=\"opacity: 0.80\">, but it is to </span><span style=\"background-color: hsl(120, 100.00%, 85.25%); opacity: 0.85\" title=\"0.065\">good</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 89.60%); opacity: 0.83\" title=\"-0.039\">babylon</span><span style=\"opacity: 0.80\"> 5 is to </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.008\">star</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.57%); opacity: 0.83\" title=\"0.040\">trek</span><span style=\"opacity: 0.80\"> (the </span><span style=\"background-color: hsl(0, 100.00%, 81.88%); opacity: 0.86\" title=\"-0.087\">original</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(0, 100.00%, 77.74%); opacity: 0.89\" title=\"-0.117\">silly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.29%); opacity: 0.81\" title=\"-0.009\">prosthetics</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.30%); opacity: 0.90\" title=\"-0.135\">cheap</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.76%); opacity: 0.88\" title=\"-0.102\">cardboard</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.58%); opacity: 0.80\" title=\"0.000\">sets</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 82.16%); opacity: 0.86\" title=\"-0.085\">stilted</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.59%); opacity: 0.82\" title=\"-0.029\">dialogues</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 98.03%); opacity: 0.80\" title=\"0.004\">cg</span><span style=\"opacity: 0.80\"> that </span><span style=\"background-color: hsl(0, 100.00%, 84.47%); opacity: 0.85\" title=\"-0.070\">doesn</span><span style=\"opacity: 0.80\">&#x27;t </span><span style=\"background-color: hsl(120, 100.00%, 88.93%); opacity: 0.83\" title=\"0.043\">match</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(120, 100.00%, 95.63%); opacity: 0.81\" title=\"0.011\">background</span><span style=\"opacity: 0.80\">, and </span><span style=\"background-color: hsl(0, 100.00%, 81.55%); opacity: 0.87\" title=\"-0.089\">painfully</span><span style=\"opacity: 0.80\"> one-</span><span style=\"background-color: hsl(0, 100.00%, 82.32%); opacity: 0.86\" title=\"-0.084\">dimensional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.35%); opacity: 0.81\" title=\"-0.012\">characters</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.88%); opacity: 0.84\" title=\"-0.049\">cannot</span><span style=\"opacity: 0.80\"> be </span><span style=\"background-color: hsl(120, 100.00%, 93.49%); opacity: 0.81\" title=\"0.020\">overcome</span><span style=\"opacity: 0.80\"> with a &#x27;</span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\">&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 98.34%); opacity: 0.80\" title=\"0.003\">setting</span><span style=\"opacity: 0.80\">. (i&#x27;m </span><span style=\"background-color: hsl(120, 100.00%, 95.32%); opacity: 0.81\" title=\"0.013\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.94%); opacity: 0.83\" title=\"-0.038\">there</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(120, 100.00%, 89.83%); opacity: 0.83\" title=\"0.038\">those</span><span style=\"opacity: 0.80\"> of you </span><span style=\"background-color: hsl(0, 100.00%, 97.93%); opacity: 0.80\" title=\"-0.004\">out</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.94%); opacity: 0.83\" title=\"-0.038\">there</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.97%); opacity: 0.81\" title=\"0.018\">who</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.55%); opacity: 0.84\" title=\"0.057\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.60%); opacity: 0.83\" title=\"-0.039\">babylon</span><span style=\"opacity: 0.80\"> 5 is </span><span style=\"background-color: hsl(120, 100.00%, 85.25%); opacity: 0.85\" title=\"0.065\">good</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\">. it&#x27;s not. it&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 87.19%); opacity: 0.84\" title=\"-0.053\">clich√©d</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 90.71%); opacity: 0.82\" title=\"-0.033\">uninspiring</span><span style=\"opacity: 0.80\">.) </span><span style=\"background-color: hsl(120, 100.00%, 91.75%); opacity: 0.82\" title=\"0.028\">while</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.71%); opacity: 0.83\" title=\"0.044\">us</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.42%); opacity: 0.81\" title=\"0.009\">viewers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.42%); opacity: 0.86\" title=\"-0.077\">might</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.37%); opacity: 0.81\" title=\"-0.009\">like</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.35%); opacity: 0.82\" title=\"0.021\">emotion</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.019\">character</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.18%); opacity: 0.85\" title=\"-0.072\">development</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> is a </span><span style=\"background-color: hsl(120, 100.00%, 81.94%); opacity: 0.86\" title=\"0.087\">genre</span><span style=\"opacity: 0.80\"> that </span><span style=\"background-color: hsl(0, 100.00%, 94.59%); opacity: 0.81\" title=\"-0.015\">does</span><span style=\"opacity: 0.80\"> not </span><span style=\"background-color: hsl(120, 100.00%, 94.52%); opacity: 0.81\" title=\"0.016\">take</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.61%); opacity: 0.81\" title=\"-0.015\">itself</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.20%); opacity: 0.85\" title=\"-0.072\">seriously</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 98.50%); opacity: 0.80\" title=\"-0.002\">cf</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.008\">star</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.57%); opacity: 0.83\" title=\"0.040\">trek</span><span style=\"opacity: 0.80\">). it </span><span style=\"background-color: hsl(120, 100.00%, 83.07%); opacity: 0.86\" title=\"0.079\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.88%); opacity: 0.86\" title=\"0.080\">treat</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.33%); opacity: 0.82\" title=\"0.025\">important</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.49%); opacity: 0.82\" title=\"0.025\">issues</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 89.39%); opacity: 0.83\" title=\"0.040\">yet</span><span style=\"opacity: 0.80\"> not as a </span><span style=\"background-color: hsl(0, 100.00%, 98.87%); opacity: 0.80\" title=\"-0.002\">serious</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.07%); opacity: 0.83\" title=\"-0.037\">philosophy</span><span style=\"opacity: 0.80\">. it&#x27;s </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.81%); opacity: 0.80\" title=\"-0.004\">difficult</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 85.34%); opacity: 0.85\" title=\"-0.064\">care</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.37%); opacity: 0.80\" title=\"0.003\">about</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(0, 100.00%, 95.35%); opacity: 0.81\" title=\"-0.012\">characters</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.96%); opacity: 0.83\" title=\"-0.037\">here</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 91.27%); opacity: 0.82\" title=\"-0.031\">they</span><span style=\"opacity: 0.80\"> are not </span><span style=\"background-color: hsl(0, 100.00%, 86.77%); opacity: 0.84\" title=\"-0.055\">simply</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.78%); opacity: 0.81\" title=\"-0.015\">foolish</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 84.01%); opacity: 0.85\" title=\"-0.073\">just</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.72%); opacity: 0.81\" title=\"-0.015\">missing</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 91.72%); opacity: 0.82\" title=\"-0.028\">spark</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 82.17%); opacity: 0.86\" title=\"0.085\">life</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 93.88%); opacity: 0.81\" title=\"0.018\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.80%); opacity: 0.82\" title=\"0.033\">actions</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 97.25%); opacity: 0.80\" title=\"0.006\">reactions</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(0, 100.00%, 72.96%); opacity: 0.91\" title=\"-0.154\">wooden</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 71.45%); opacity: 0.92\" title=\"-0.167\">predictable</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 83.66%); opacity: 0.86\" title=\"0.075\">often</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.41%); opacity: 0.90\" title=\"-0.135\">painful</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 88.71%); opacity: 0.83\" title=\"0.044\">watch</span><span style=\"opacity: 0.80\">. the </span><span style=\"background-color: hsl(0, 100.00%, 86.87%); opacity: 0.84\" title=\"-0.055\">makers</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(0, 100.00%, 90.65%); opacity: 0.83\" title=\"-0.034\">earth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.70%); opacity: 0.83\" title=\"0.039\">know</span><span style=\"opacity: 0.80\"> it&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 77.95%); opacity: 0.89\" title=\"-0.115\">rubbish</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 91.27%); opacity: 0.82\" title=\"-0.031\">they</span><span style=\"opacity: 0.80\"> have to </span><span style=\"background-color: hsl(120, 100.00%, 80.41%); opacity: 0.87\" title=\"0.097\">always</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.42%); opacity: 0.80\" title=\"-0.005\">say</span><span style=\"opacity: 0.80\"> &quot;</span><span style=\"background-color: hsl(120, 100.00%, 90.64%); opacity: 0.83\" title=\"0.034\">gene</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.70%); opacity: 0.80\" title=\"0.002\">roddenberry</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 90.65%); opacity: 0.83\" title=\"-0.034\">earth</span><span style=\"opacity: 0.80\">...&quot; </span><span style=\"background-color: hsl(0, 100.00%, 82.18%); opacity: 0.86\" title=\"-0.085\">otherwise</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.03%); opacity: 0.81\" title=\"0.018\">people</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.85%); opacity: 0.85\" title=\"-0.067\">would</span><span style=\"opacity: 0.80\"> not </span><span style=\"background-color: hsl(120, 100.00%, 96.70%); opacity: 0.81\" title=\"0.008\">continue</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.038\">watching</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 98.70%); opacity: 0.80\" title=\"0.002\">roddenberry</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 97.27%); opacity: 0.80\" title=\"-0.006\">ashes</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.15%); opacity: 0.84\" title=\"0.047\">must</span><span style=\"opacity: 0.80\"> be </span><span style=\"background-color: hsl(0, 100.00%, 91.20%); opacity: 0.82\" title=\"-0.031\">turning</span><span style=\"opacity: 0.80\"> in </span><span style=\"background-color: hsl(120, 100.00%, 93.88%); opacity: 0.81\" title=\"0.018\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.28%); opacity: 0.80\" title=\"-0.006\">orbit</span><span style=\"opacity: 0.80\"> as this </span><span style=\"background-color: hsl(0, 100.00%, 61.98%); opacity: 0.99\" title=\"-0.251\">dull</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.30%); opacity: 0.90\" title=\"-0.135\">cheap</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.270\">poorly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.05%); opacity: 0.81\" title=\"0.014\">edited</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.038\">watching</span><span style=\"opacity: 0.80\"> it </span><span style=\"background-color: hsl(120, 100.00%, 89.47%); opacity: 0.83\" title=\"0.040\">without</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.94%); opacity: 0.82\" title=\"0.032\">advert</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.12%); opacity: 0.80\" title=\"-0.003\">breaks</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.36%); opacity: 0.85\" title=\"0.064\">brings</span><span style=\"opacity: 0.80\"> this </span><span style=\"background-color: hsl(120, 100.00%, 96.48%); opacity: 0.81\" title=\"0.008\">home</span><span style=\"opacity: 0.80\">) trudging trabant of a </span><span style=\"background-color: hsl(120, 100.00%, 94.59%); opacity: 0.81\" title=\"0.015\">show</span><span style=\"opacity: 0.80\"> lumbers </span><span style=\"background-color: hsl(0, 100.00%, 99.48%); opacity: 0.80\" title=\"-0.001\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.77%); opacity: 0.81\" title=\"-0.007\">space</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 94.79%); opacity: 0.81\" title=\"-0.015\">spoiler</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.43%); opacity: 0.81\" title=\"-0.009\">so</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 92.80%); opacity: 0.82\" title=\"-0.023\">kill</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.67%); opacity: 0.85\" title=\"-0.062\">off</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 89.39%); opacity: 0.83\" title=\"-0.040\">main</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.019\">character</span><span style=\"opacity: 0.80\">. and </span><span style=\"background-color: hsl(0, 100.00%, 86.73%); opacity: 0.84\" title=\"-0.056\">then</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.64%); opacity: 0.81\" title=\"-0.008\">bring</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.69%); opacity: 0.83\" title=\"0.044\">him</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.30%); opacity: 0.81\" title=\"0.013\">back</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 92.66%); opacity: 0.82\" title=\"-0.024\">another</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.60%); opacity: 0.85\" title=\"-0.063\">actor</span><span style=\"opacity: 0.80\">. jeeez! </span><span style=\"background-color: hsl(0, 100.00%, 90.59%); opacity: 0.83\" title=\"-0.034\">dallas</span><span style=\"opacity: 0.80\"> all </span><span style=\"background-color: hsl(0, 100.00%, 93.19%); opacity: 0.82\" title=\"-0.022\">over</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.91%); opacity: 0.83\" title=\"0.038\">again</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_prediction(classifier, reviews_test[0], vec=vectorizer, targets=['pos'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
