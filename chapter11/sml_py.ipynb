{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 22:43:48.142912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-06 22:43:48.142927: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import eli5\n",
    "import nltk\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"reviewdata.pickle.bz2\"\n",
    "with bz2.BZ2File(filename, \"r\") as f:\n",
    "    text_train, text_test, y_train, y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "snippet:imdbbaseline"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.88      0.83     12500\n",
      "         pos       0.86      0.76      0.81     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "rep=metrics.classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "snippet:basiccomparisons"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.79\t\t0.88\n",
      "pos:\t0.87\t\t0.77\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.80\t\t0.88\n",
      "pos:\t0.87\t\t0.78\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.85\t\t0.87\n",
      "pos:\t0.87\t\t0.85\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.88\t\t0.89\n",
      "pos:\t0.89\t\t0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def short_classification_report (y_test, y_pred):\n",
    "  print(\"    \\tPrecision\\tRecall\")\n",
    "  for label in set(y_pred):\n",
    "    pr = metrics.precision_score(y_test, y_pred, pos_label=label)\n",
    "    re = metrics.recall_score(y_test,y_pred, pos_label=label)\n",
    "    print(f\"{label}:\\t{pr:0.2f}\\t\\t{re:0.2f}\")\n",
    "\n",
    "configs = [\n",
    "    (\"NB with Count\", CountVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "    (\"NB with TfIdf\", TfidfVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "    (\"LogReg with Count\", CountVectorizer(min_df=5, max_df=.5), \n",
    "     LogisticRegression(solver=\"liblinear\")),\n",
    "    (\"LogReg with TfIdf\", TfidfVectorizer(min_df=5, max_df=.5), \n",
    "     LogisticRegression(solver=\"liblinear\"))]\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(text_train)\n",
    "    X_test = vectorizer.transform(text_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:basicpipe"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.79\t\t0.88\n",
      "pos:\t0.87\t\t0.77\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.80\t\t0.88\n",
      "pos:\t0.87\t\t0.78\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.85\t\t0.87\n",
      "pos:\t0.87\t\t0.85\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.88\t\t0.89\n",
      "pos:\t0.89\t\t0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    pipe = make_pipeline(vectorizer, classifier)\n",
    "    pipe.fit(text_train, y_train)\n",
    "    y_pred = pipe.predict(text_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:gridsearchlogreg"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters: {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 2)}\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.90\t\t0.90\n",
      "pos:\t0.90\t\t0.90\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps = [(\"vectorizer\", TfidfVectorizer()), \n",
    "                             (\"classifier\", LogisticRegression(solver=\"liblinear\"))])\n",
    "grid = {\"vectorizer__ngram_range\" : [(1,1), (1,2)],\n",
    "        \"vectorizer__max_df\": [0.5, 1.0],\n",
    "        \"vectorizer__min_df\": [0, 5],\n",
    "        \"classifier__C\": [0.01, 1, 100]\n",
    "       }\n",
    "search = GridSearchCV(estimator=pipeline, param_grid=grid, cv=5,\n",
    "                      scoring=\"accuracy\", n_jobs=-1, verbose=10)\n",
    "search.fit(text_train, y_train)\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "pred = search.predict(text_test)\n",
    "print(short_classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "snippet:vader"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/wva/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0]\n",
      " [    6  6706  5788]\n",
      " [    5  1748 10747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   dont know       0.00      0.00      0.00         0\n",
      "         neg       0.79      0.54      0.64     12500\n",
      "         pos       0.65      0.86      0.74     12500\n",
      "\n",
      "    accuracy                           0.70     25000\n",
      "   macro avg       0.48      0.47      0.46     25000\n",
      "weighted avg       0.72      0.70      0.69     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pred = []\n",
    "for review in text_test:\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    if sentiment[\"compound\"]>0:\n",
    "        pred.append(\"pos\")\n",
    "    elif sentiment[\"compound\"]<0:\n",
    "        pred.append(\"neg\")\n",
    "    else:\n",
    "        pred.append(\"dont know\")\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, pred))\n",
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "snippet:reuse"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a great movie' is probably 'pos'.\n",
      "'I hated this one.' is probably 'neg'.\n",
      "'What an awful fail' is probably 'neg'.\n"
     ]
    }
   ],
   "source": [
    "# Make a vectorizer and train a classifier\n",
    "vectorizer=TfidfVectorizer(min_df=5, max_df=.5)\n",
    "classifier=LogisticRegression(solver=\"liblinear\")\n",
    "X_train=vectorizer.fit_transform(text_train)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save them to disk\n",
    "with open(\"myvectorizer.pkl\",mode=\"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(\"myclassifier.pkl\",mode=\"wb\") as f:\n",
    "    joblib.dump(classifier, f)\n",
    "  \n",
    "# Later on, re-load this classifier and apply:\n",
    "new_texts = [\"This is a great movie\", \n",
    "            \"I hated this one.\", \n",
    "            \"What an awful fail\"]\n",
    "\n",
    "with open(\"myvectorizer.pkl\",mode=\"rb\") as f:\n",
    "    myvectorizer = pickle.load(f)\n",
    "with open(\"myclassifier.pkl\",mode=\"rb\") as f:\n",
    "    myclassifier = joblib.load(f)\n",
    "    \n",
    "new_features = myvectorizer.transform(new_texts)\n",
    "pred = myclassifier.predict(new_features)\n",
    "\n",
    "for review, label in zip(new_texts, pred):\n",
    "    print(f\"'{review}' is probably '{label}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "output:html",
     "snippet:eli5"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.173\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        great\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +6.101\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        excellent\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        best\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.791\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        perfect\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13663 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13574 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.337\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        poor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.733\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        boring\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.315\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        waste\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.349\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        awful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -7.347\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        bad\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -9.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worst\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, max_df=.5), \n",
    "                     LogisticRegression(solver=\"liblinear\"))\n",
    "pipe.fit(text_train, y_train)\n",
    "eli5.show_weights(pipe, top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:eli5b",
     "output:html"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 1/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.826 total time=   3.2s\n",
      "[CV 2/5; 3/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 3/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.826 total time=   2.9s\n",
      "[CV 4/5; 3/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 3/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.809 total time=   3.0s\n",
      "[CV 2/5; 4/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 4/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.839 total time=  10.0s\n",
      "[CV 1/5; 6/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 6/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.786 total time=  15.8s\n",
      "[CV 5/5; 8/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 8/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.800 total time=   8.5s\n",
      "[CV 3/5; 11/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 11/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.859 total time=   3.2s\n",
      "[CV 1/5; 12/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 12/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.872 total time=  12.2s\n",
      "[CV 4/5; 13/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 13/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.875 total time=   3.6s\n",
      "[CV 5/5; 14/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 14/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.875 total time=  19.1s\n",
      "[CV 1/5; 17/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 17/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.832 total time=   4.4s\n",
      "[CV 2/5; 18/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 18/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.874 total time=  30.5s\n",
      "[CV 4/5; 21/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 21/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.847 total time=   5.9s\n",
      "[CV 1/5; 23/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 23/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.832 total time=   4.3s\n",
      "[CV 4/5; 23/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 23/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.848 total time=   5.6s\n",
      "[CV 2/5; 24/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 24/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.865 total time=  23.1s\n",
      "[CV 5/5; 1/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 1/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.811 total time=   3.0s\n",
      "[CV 4/5; 2/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 2/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.822 total time=  14.6s\n",
      "[CV 4/5; 5/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 5/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.787 total time=   3.0s\n",
      "[CV 5/5; 6/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 6/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.795 total time=  14.9s\n",
      "[CV 2/5; 9/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 9/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.861 total time=   3.2s\n",
      "[CV 3/5; 10/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 10/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.863 total time=  17.9s\n",
      "[CV 5/5; 12/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 12/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.879 total time=   9.4s\n",
      "[CV 3/5; 15/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 15/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.859 total time=   3.6s\n",
      "[CV 2/5; 16/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 16/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.880 total time=  13.1s\n",
      "[CV 5/5; 17/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 17/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.842 total time=   4.3s\n",
      "[CV 1/5; 19/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 19/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.826 total time=   3.9s\n",
      "[CV 3/5; 19/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 19/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.823 total time=   4.6s\n",
      "[CV 1/5; 20/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 20/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.872 total time=  20.8s\n",
      "[CV 2/5; 21/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 21/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.826 total time=   5.8s\n",
      "[CV 5/5; 22/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 22/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.882 total time=  33.7s\n",
      "[CV 1/5; 2/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 2/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.815 total time=  13.4s\n",
      "[CV 4/5; 4/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 4/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.824 total time=   9.3s\n",
      "[CV 2/5; 7/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 7/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.799 total time=   3.1s\n",
      "[CV 5/5; 7/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 7/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.785 total time=   3.0s\n",
      "[CV 3/5; 8/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 8/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.783 total time=   9.7s\n",
      "[CV 2/5; 10/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 10/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.879 total time=  19.1s\n",
      "[CV 3/5; 13/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 13/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.858 total time=   3.5s\n",
      "[CV 3/5; 14/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 14/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.863 total time=  20.8s\n",
      "[CV 2/5; 17/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 17/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.819 total time=   4.3s\n",
      "[CV 3/5; 18/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 18/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.874 total time=  30.0s\n",
      "[CV 1/5; 21/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 21/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.836 total time=   5.7s\n",
      "[CV 3/5; 22/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 22/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.875 total time=  34.0s\n",
      "[CV 3/5; 1/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 1/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.804 total time=   3.3s\n",
      "[CV 3/5; 3/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 3/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.805 total time=   2.9s\n",
      "[CV 5/5; 3/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 3/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.813 total time=   2.9s\n",
      "[CV 3/5; 4/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 4/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.813 total time=  10.0s\n",
      "[CV 2/5; 6/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 6/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.803 total time=  14.9s\n",
      "[CV 4/5; 8/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 8/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.793 total time=   8.5s\n",
      "[CV 2/5; 11/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 11/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.861 total time=   3.3s\n",
      "[CV 5/5; 11/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 11/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.866 total time=   3.8s\n",
      "[CV 3/5; 12/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 12/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.868 total time=  11.4s\n",
      "[CV 4/5; 14/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 14/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.877 total time=  20.2s\n",
      "[CV 5/5; 16/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 16/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.880 total time=   9.6s\n",
      "[CV 4/5; 19/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 19/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.845 total time=   4.7s\n",
      "[CV 2/5; 20/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 20/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.868 total time=  20.6s\n",
      "[CV 3/5; 21/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 21/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.832 total time=   5.6s\n",
      "[CV 2/5; 22/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 22/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.875 total time=  34.1s\n",
      "[CV 1/5; 1/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 1/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.798 total time=   3.2s\n",
      "[CV 1/5; 3/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 3/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.796 total time=   3.2s\n",
      "[CV 1/5; 4/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 4/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.815 total time=   9.8s\n",
      "[CV 2/5; 5/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 5/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.798 total time=   3.0s\n",
      "[CV 3/5; 6/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 6/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.780 total time=  16.5s\n",
      "[CV 3/5; 9/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 9/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.858 total time=   3.2s\n",
      "[CV 4/5; 10/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 10/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.881 total time=  18.5s\n",
      "[CV 2/5; 13/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 13/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.862 total time=   3.5s\n",
      "[CV 2/5; 14/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 14/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.875 total time=  21.0s\n",
      "[CV 3/5; 17/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 17/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.826 total time=   4.6s\n",
      "[CV 4/5; 18/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 18/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.890 total time=  30.2s\n",
      "[CV 5/5; 21/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 21/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.846 total time=   5.5s\n",
      "[CV 4/5; 22/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 22/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.889 total time=  34.1s\n",
      "[CV 4/5; 1/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 1/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.808 total time=   3.1s\n",
      "[CV 5/5; 2/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 2/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.823 total time=  13.9s\n",
      "[CV 3/5; 5/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 5/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.775 total time=   3.1s\n",
      "[CV 4/5; 6/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 6/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.789 total time=  15.1s\n",
      "[CV 1/5; 9/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 9/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.860 total time=   3.0s\n",
      "[CV 1/5; 10/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 10/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.867 total time=  17.8s\n",
      "[CV 4/5; 12/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 12/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.887 total time=   9.1s\n",
      "[CV 2/5; 15/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 15/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.863 total time=   3.5s\n",
      "[CV 5/5; 15/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 15/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.868 total time=   3.6s\n",
      "[CV 3/5; 16/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 16/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.867 total time=  11.7s\n",
      "[CV 1/5; 18/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 18/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.875 total time=  27.3s\n",
      "[CV 4/5; 20/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 20/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.885 total time=  12.6s\n",
      "[CV 2/5; 23/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 23/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.820 total time=   4.4s\n",
      "[CV 5/5; 23/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 23/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.844 total time=   6.1s\n",
      "[CV 3/5; 24/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 24/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.871 total time=  21.6s\n",
      "[CV 3/5; 2/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 2/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.811 total time=  13.7s\n",
      "[CV 5/5; 4/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 4/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.826 total time=   9.1s\n",
      "[CV 3/5; 7/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 7/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.776 total time=   3.0s\n",
      "[CV 1/5; 8/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 8/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.789 total time=  10.3s\n",
      "[CV 4/5; 9/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 9/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   3.1s\n",
      "[CV 5/5; 10/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 10/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.875 total time=  17.5s\n",
      "[CV 1/5; 13/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 13/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.860 total time=   3.4s\n",
      "[CV 1/5; 14/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 14/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.863 total time=  20.6s\n",
      "[CV 4/5; 16/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 16/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.882 total time=   9.3s\n",
      "[CV 2/5; 19/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/5; 19/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.816 total time=   4.7s\n",
      "[CV 5/5; 19/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 19/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.839 total time=   6.5s\n",
      "[CV 3/5; 20/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/5; 20/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.871 total time=  16.5s\n",
      "[CV 1/5; 22/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 22/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.876 total time=  30.7s\n",
      "[CV 4/5; 24/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 4/5; 24/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.886 total time=  11.7s\n",
      "[CV 2/5; 2/24] START classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 2/24] END classifier__C=0.01, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.839 total time=  14.5s\n",
      "[CV 1/5; 5/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 5/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.772 total time=   3.3s\n",
      "[CV 5/5; 5/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 5/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.780 total time=   3.2s\n",
      "[CV 1/5; 7/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 7/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.774 total time=   2.8s\n",
      "[CV 4/5; 7/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 7/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.786 total time=   3.2s\n",
      "[CV 2/5; 8/24] START classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 8/24] END classifier__C=0.01, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.806 total time=  10.2s\n",
      "[CV 5/5; 9/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 9/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.865 total time=   3.2s\n",
      "[CV 1/5; 11/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 11/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.860 total time=   3.2s\n",
      "[CV 4/5; 11/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 11/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   3.2s\n",
      "[CV 2/5; 12/24] START classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/5; 12/24] END classifier__C=1, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.882 total time=  11.9s\n",
      "[CV 5/5; 13/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 5/5; 13/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.867 total time=   3.5s\n",
      "[CV 1/5; 15/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 1/5; 15/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.861 total time=   3.2s\n",
      "[CV 4/5; 15/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 15/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.875 total time=   3.5s\n",
      "[CV 1/5; 16/24] START classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 16/24] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.869 total time=  12.8s\n",
      "[CV 4/5; 17/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1)\n",
      "[CV 4/5; 17/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 1);, score=0.845 total time=   4.4s\n",
      "[CV 5/5; 18/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 18/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__ngram_range=(1, 2);, score=0.884 total time=  29.5s\n",
      "[CV 5/5; 20/24] START classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 20/24] END classifier__C=100, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.883 total time=  10.4s\n",
      "[CV 3/5; 23/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/5; 23/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.827 total time=   5.0s\n",
      "[CV 1/5; 24/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/5; 24/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.873 total time=  22.9s\n",
      "[CV 5/5; 24/24] START classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 5/5; 24/24] END classifier__C=100, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.883 total time=   9.0s\n"
     ]
    }
   ],
   "source": [
    "eli5.show_prediction(classifier, text_test[0], vec=vectorizer, targets=[\"pos\"])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
