{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 20:49:36.933490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-30 20:49:36.933504: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import eli5\n",
    "import nltk\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"reviewdata.pickle.bz2\"\n",
    "with bz2.BZ2File(filename, \"r\") as f:\n",
    "  txt_train,txt_test,y_train,y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "snippet:imdbbaseline"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.88      0.83     12500\n",
      "         pos       0.86      0.76      0.81     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(txt_train)\n",
    "X_test = vectorizer.transform(txt_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "rep=metrics.classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:basiccomparisons"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.79\t\t0.88\n",
      "pos:\t0.87\t\t0.77\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.80\t\t0.88\n",
      "pos:\t0.87\t\t0.78\n",
      "\n",
      "\n",
      "LR-Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.85\t\t0.87\n",
      "pos:\t0.87\t\t0.85\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.88\t\t0.89\n",
      "pos:\t0.89\t\t0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def short_classification_report (y_test, y_pred):\n",
    "  print(\"    \\tPrecision\\tRecall\")\n",
    "  for label in set(y_pred):\n",
    "    pr = metrics.precision_score(y_test, y_pred, \n",
    "                                 pos_label=label)\n",
    "    re = metrics.recall_score(y_test,y_pred, \n",
    "                              pos_label=label)\n",
    "    print(f\"{label}:\\t{pr:0.2f}\\t\\t{re:0.2f}\")\n",
    "\n",
    "configs = [\n",
    "  (\"NB-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"NB-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"LR-Count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"LR-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\"))]\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(txt_train)\n",
    "    X_test = vectorizer.transform(txt_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:basicpipe"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.79\t\t0.88\n",
      "pos:\t0.87\t\t0.77\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.80\t\t0.88\n",
      "pos:\t0.87\t\t0.78\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.85\t\t0.87\n",
      "pos:\t0.87\t\t0.85\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.88\t\t0.89\n",
      "pos:\t0.89\t\t0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    pipe = make_pipeline(vectorizer, classifier)\n",
    "    pipe.fit(text_train, y_train)\n",
    "    y_pred = pipe.predict(text_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:gridsearchlogreg"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 2)}\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.90\t\t0.90\n",
      "pos:\t0.90\t\t0.90\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "  (\"vectorizer\", TfidfVectorizer()), \n",
    "  (\"classifier\", LogisticRegression(\n",
    "      solver=\"liblinear\"))])\n",
    "grid = {\"vectorizer__ngram_range\": [(1,1), (1,2)],\n",
    "        \"vectorizer__max_df\": [0.5, 1.0],\n",
    "        \"vectorizer__min_df\": [0, 5],\n",
    "        \"classifier__C\": [0.01, 1, 100]\n",
    "       }\n",
    "search=GridSearchCV(estimator=pipeline, n_jobs=-1,\n",
    "  param_grid=grid,scoring=\"accuracy\", cv=5)\n",
    "search.fit(txt_train, y_train)\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "pred = search.predict(txt_test)\n",
    "print(short_classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "snippet:vader"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/wva/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0]\n",
      " [    6  6706  5788]\n",
      " [    5  1748 10747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   dont know       0.00      0.00      0.00         0\n",
      "         neg       0.79      0.54      0.64     12500\n",
      "         pos       0.65      0.86      0.74     12500\n",
      "\n",
      "    accuracy                           0.70     25000\n",
      "   macro avg       0.48      0.47      0.46     25000\n",
      "weighted avg       0.72      0.70      0.69     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pred = []\n",
    "for review in text_test:\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    if sentiment[\"compound\"]>0:\n",
    "        pred.append(\"pos\")\n",
    "    elif sentiment[\"compound\"]<0:\n",
    "        pred.append(\"neg\")\n",
    "    else:\n",
    "        pred.append(\"dont know\")\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, pred))\n",
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "snippet:reuse"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a great movie' is probably 'pos'.\n",
      "'I hated this one.' is probably 'neg'.\n",
      "'What an awful fail' is probably 'neg'.\n"
     ]
    }
   ],
   "source": [
    "# Make a vectorizer and train a classifier\n",
    "vectorizer=TfidfVectorizer(min_df=5, max_df=.5)\n",
    "classifier=LogisticRegression(solver=\"liblinear\")\n",
    "X_train=vectorizer.fit_transform(txt_train)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save them to disk\n",
    "with open(\"myvectorizer.pkl\",mode=\"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(\"myclassifier.pkl\",mode=\"wb\") as f:\n",
    "    joblib.dump(classifier, f)\n",
    "  \n",
    "# Later on, re-load this classifier and apply:\n",
    "new_texts = [\"This is a great movie\", \n",
    "            \"I hated this one.\", \n",
    "            \"What an awful fail\"]\n",
    "\n",
    "with open(\"myvectorizer.pkl\",mode=\"rb\") as f:\n",
    "    myvectorizer = pickle.load(f)\n",
    "with open(\"myclassifier.pkl\",mode=\"rb\") as f:\n",
    "    myclassifier = joblib.load(f)\n",
    "    \n",
    "new_features = myvectorizer.transform(new_texts)\n",
    "pred = myclassifier.predict(new_features)\n",
    "\n",
    "for review, label in zip(new_texts, pred):\n",
    "    print(f\"'{review}' is probably '{label}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "output:html",
     "snippet:eli5"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.173\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        great\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +6.101\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        excellent\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        best\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.791\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        perfect\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13663 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13574 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.337\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        poor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.733\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        boring\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.315\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        waste\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.349\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        awful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -7.347\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        bad\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -9.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worst\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(min_df=5, max_df=.5), \n",
    "    LogisticRegression(solver=\"liblinear\"))\n",
    "pipe.fit(txt_train, y_train)\n",
    "eli5.show_weights(pipe, top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:eli5b",
     "output:html"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.140</b>, score <b>-1.817</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.830\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"opacity: 0.80\">i </span><span style=\"background-color: hsl(120, 100.00%, 74.97%); opacity: 0.90\" title=\"0.138\">love</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 84.32%); opacity: 0.85\" title=\"-0.071\">am</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.56%); opacity: 0.83\" title=\"0.034\">willing</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 93.55%); opacity: 0.81\" title=\"0.020\">put</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.84%); opacity: 0.80\" title=\"-0.002\">up</span><span style=\"opacity: 0.80\"> with a </span><span style=\"background-color: hsl(120, 100.00%, 84.06%); opacity: 0.85\" title=\"0.072\">lot</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.44%); opacity: 0.81\" title=\"0.012\">movies</span><span style=\"opacity: 0.80\">/</span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(0, 100.00%, 90.89%); opacity: 0.82\" title=\"-0.033\">usually</span><span style=\"opacity: 0.80\"> underfunded, </span><span style=\"background-color: hsl(120, 100.00%, 98.46%); opacity: 0.80\" title=\"0.003\">under</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 78.84%); opacity: 0.88\" title=\"0.109\">appreciated</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 94.35%); opacity: 0.81\" title=\"0.016\">misunderstood</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 83.48%); opacity: 0.86\" title=\"-0.076\">tried</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 96.37%); opacity: 0.81\" title=\"-0.009\">like</span><span style=\"opacity: 0.80\"> this, i </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.20%); opacity: 0.83\" title=\"-0.047\">did</span><span style=\"opacity: 0.80\">, but it is to </span><span style=\"background-color: hsl(120, 100.00%, 85.25%); opacity: 0.85\" title=\"0.065\">good</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 89.60%); opacity: 0.83\" title=\"-0.039\">babylon</span><span style=\"opacity: 0.80\"> 5 is to </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.008\">star</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.57%); opacity: 0.83\" title=\"0.040\">trek</span><span style=\"opacity: 0.80\"> (the </span><span style=\"background-color: hsl(0, 100.00%, 81.88%); opacity: 0.86\" title=\"-0.087\">original</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(0, 100.00%, 77.74%); opacity: 0.89\" title=\"-0.117\">silly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.29%); opacity: 0.81\" title=\"-0.009\">prosthetics</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.30%); opacity: 0.90\" title=\"-0.135\">cheap</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.76%); opacity: 0.88\" title=\"-0.102\">cardboard</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.58%); opacity: 0.80\" title=\"0.000\">sets</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 82.16%); opacity: 0.86\" title=\"-0.085\">stilted</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.59%); opacity: 0.82\" title=\"-0.029\">dialogues</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 98.03%); opacity: 0.80\" title=\"0.004\">cg</span><span style=\"opacity: 0.80\"> that </span><span style=\"background-color: hsl(0, 100.00%, 84.47%); opacity: 0.85\" title=\"-0.070\">doesn</span><span style=\"opacity: 0.80\">&#x27;t </span><span style=\"background-color: hsl(120, 100.00%, 88.93%); opacity: 0.83\" title=\"0.043\">match</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(120, 100.00%, 95.63%); opacity: 0.81\" title=\"0.011\">background</span><span style=\"opacity: 0.80\">, and </span><span style=\"background-color: hsl(0, 100.00%, 81.55%); opacity: 0.87\" title=\"-0.089\">painfully</span><span style=\"opacity: 0.80\"> one-</span><span style=\"background-color: hsl(0, 100.00%, 82.32%); opacity: 0.86\" title=\"-0.084\">dimensional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.35%); opacity: 0.81\" title=\"-0.012\">characters</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.88%); opacity: 0.84\" title=\"-0.049\">cannot</span><span style=\"opacity: 0.80\"> be </span><span style=\"background-color: hsl(120, 100.00%, 93.49%); opacity: 0.81\" title=\"0.020\">overcome</span><span style=\"opacity: 0.80\"> with a &#x27;</span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\">&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 98.34%); opacity: 0.80\" title=\"0.003\">setting</span><span style=\"opacity: 0.80\">. (i&#x27;m </span><span style=\"background-color: hsl(120, 100.00%, 95.32%); opacity: 0.81\" title=\"0.013\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.94%); opacity: 0.83\" title=\"-0.038\">there</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(120, 100.00%, 89.83%); opacity: 0.83\" title=\"0.038\">those</span><span style=\"opacity: 0.80\"> of you </span><span style=\"background-color: hsl(0, 100.00%, 97.93%); opacity: 0.80\" title=\"-0.004\">out</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.94%); opacity: 0.83\" title=\"-0.038\">there</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.97%); opacity: 0.81\" title=\"0.018\">who</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.55%); opacity: 0.84\" title=\"0.057\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.60%); opacity: 0.83\" title=\"-0.039\">babylon</span><span style=\"opacity: 0.80\"> 5 is </span><span style=\"background-color: hsl(120, 100.00%, 85.25%); opacity: 0.85\" title=\"0.065\">good</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\">. it&#x27;s not. it&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 87.19%); opacity: 0.84\" title=\"-0.053\">clich√©d</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 90.71%); opacity: 0.82\" title=\"-0.033\">uninspiring</span><span style=\"opacity: 0.80\">.) </span><span style=\"background-color: hsl(120, 100.00%, 91.75%); opacity: 0.82\" title=\"0.028\">while</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.71%); opacity: 0.83\" title=\"0.044\">us</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.42%); opacity: 0.81\" title=\"0.009\">viewers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.42%); opacity: 0.86\" title=\"-0.077\">might</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.37%); opacity: 0.81\" title=\"-0.009\">like</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.35%); opacity: 0.82\" title=\"0.021\">emotion</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.019\">character</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.18%); opacity: 0.85\" title=\"-0.072\">development</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> is a </span><span style=\"background-color: hsl(120, 100.00%, 81.94%); opacity: 0.86\" title=\"0.087\">genre</span><span style=\"opacity: 0.80\"> that </span><span style=\"background-color: hsl(0, 100.00%, 94.59%); opacity: 0.81\" title=\"-0.015\">does</span><span style=\"opacity: 0.80\"> not </span><span style=\"background-color: hsl(120, 100.00%, 94.52%); opacity: 0.81\" title=\"0.016\">take</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.61%); opacity: 0.81\" title=\"-0.015\">itself</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.20%); opacity: 0.85\" title=\"-0.072\">seriously</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 98.50%); opacity: 0.80\" title=\"-0.002\">cf</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.008\">star</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.57%); opacity: 0.83\" title=\"0.040\">trek</span><span style=\"opacity: 0.80\">). it </span><span style=\"background-color: hsl(120, 100.00%, 83.07%); opacity: 0.86\" title=\"0.079\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.88%); opacity: 0.86\" title=\"0.080\">treat</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.33%); opacity: 0.82\" title=\"0.025\">important</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.49%); opacity: 0.82\" title=\"0.025\">issues</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 89.39%); opacity: 0.83\" title=\"0.040\">yet</span><span style=\"opacity: 0.80\"> not as a </span><span style=\"background-color: hsl(0, 100.00%, 98.87%); opacity: 0.80\" title=\"-0.002\">serious</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.07%); opacity: 0.83\" title=\"-0.037\">philosophy</span><span style=\"opacity: 0.80\">. it&#x27;s </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.81%); opacity: 0.80\" title=\"-0.004\">difficult</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 85.34%); opacity: 0.85\" title=\"-0.064\">care</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.37%); opacity: 0.80\" title=\"0.003\">about</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(0, 100.00%, 95.35%); opacity: 0.81\" title=\"-0.012\">characters</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.96%); opacity: 0.83\" title=\"-0.037\">here</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 91.27%); opacity: 0.82\" title=\"-0.031\">they</span><span style=\"opacity: 0.80\"> are not </span><span style=\"background-color: hsl(0, 100.00%, 86.77%); opacity: 0.84\" title=\"-0.055\">simply</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.78%); opacity: 0.81\" title=\"-0.015\">foolish</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 84.01%); opacity: 0.85\" title=\"-0.073\">just</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.72%); opacity: 0.81\" title=\"-0.015\">missing</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 91.72%); opacity: 0.82\" title=\"-0.028\">spark</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 82.17%); opacity: 0.86\" title=\"0.085\">life</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 93.88%); opacity: 0.81\" title=\"0.018\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.80%); opacity: 0.82\" title=\"0.033\">actions</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 97.25%); opacity: 0.80\" title=\"0.006\">reactions</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(0, 100.00%, 72.96%); opacity: 0.91\" title=\"-0.154\">wooden</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 71.45%); opacity: 0.92\" title=\"-0.167\">predictable</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 83.66%); opacity: 0.86\" title=\"0.075\">often</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.41%); opacity: 0.90\" title=\"-0.135\">painful</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 88.71%); opacity: 0.83\" title=\"0.044\">watch</span><span style=\"opacity: 0.80\">. the </span><span style=\"background-color: hsl(0, 100.00%, 86.87%); opacity: 0.84\" title=\"-0.055\">makers</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(0, 100.00%, 90.65%); opacity: 0.83\" title=\"-0.034\">earth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.70%); opacity: 0.83\" title=\"0.039\">know</span><span style=\"opacity: 0.80\"> it&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 77.95%); opacity: 0.89\" title=\"-0.115\">rubbish</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 91.27%); opacity: 0.82\" title=\"-0.031\">they</span><span style=\"opacity: 0.80\"> have to </span><span style=\"background-color: hsl(120, 100.00%, 80.41%); opacity: 0.87\" title=\"0.097\">always</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.42%); opacity: 0.80\" title=\"-0.005\">say</span><span style=\"opacity: 0.80\"> &quot;</span><span style=\"background-color: hsl(120, 100.00%, 90.64%); opacity: 0.83\" title=\"0.034\">gene</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.70%); opacity: 0.80\" title=\"0.002\">roddenberry</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 90.65%); opacity: 0.83\" title=\"-0.034\">earth</span><span style=\"opacity: 0.80\">...&quot; </span><span style=\"background-color: hsl(0, 100.00%, 82.18%); opacity: 0.86\" title=\"-0.085\">otherwise</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.03%); opacity: 0.81\" title=\"0.018\">people</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.85%); opacity: 0.85\" title=\"-0.067\">would</span><span style=\"opacity: 0.80\"> not </span><span style=\"background-color: hsl(120, 100.00%, 96.70%); opacity: 0.81\" title=\"0.008\">continue</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.038\">watching</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 98.70%); opacity: 0.80\" title=\"0.002\">roddenberry</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 97.27%); opacity: 0.80\" title=\"-0.006\">ashes</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.15%); opacity: 0.84\" title=\"0.047\">must</span><span style=\"opacity: 0.80\"> be </span><span style=\"background-color: hsl(0, 100.00%, 91.20%); opacity: 0.82\" title=\"-0.031\">turning</span><span style=\"opacity: 0.80\"> in </span><span style=\"background-color: hsl(120, 100.00%, 93.88%); opacity: 0.81\" title=\"0.018\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.28%); opacity: 0.80\" title=\"-0.006\">orbit</span><span style=\"opacity: 0.80\"> as this </span><span style=\"background-color: hsl(0, 100.00%, 61.98%); opacity: 0.99\" title=\"-0.251\">dull</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.30%); opacity: 0.90\" title=\"-0.135\">cheap</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.270\">poorly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.05%); opacity: 0.81\" title=\"0.014\">edited</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.038\">watching</span><span style=\"opacity: 0.80\"> it </span><span style=\"background-color: hsl(120, 100.00%, 89.47%); opacity: 0.83\" title=\"0.040\">without</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.94%); opacity: 0.82\" title=\"0.032\">advert</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.12%); opacity: 0.80\" title=\"-0.003\">breaks</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.36%); opacity: 0.85\" title=\"0.064\">brings</span><span style=\"opacity: 0.80\"> this </span><span style=\"background-color: hsl(120, 100.00%, 96.48%); opacity: 0.81\" title=\"0.008\">home</span><span style=\"opacity: 0.80\">) trudging trabant of a </span><span style=\"background-color: hsl(120, 100.00%, 94.59%); opacity: 0.81\" title=\"0.015\">show</span><span style=\"opacity: 0.80\"> lumbers </span><span style=\"background-color: hsl(0, 100.00%, 99.48%); opacity: 0.80\" title=\"-0.001\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.77%); opacity: 0.81\" title=\"-0.007\">space</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 94.79%); opacity: 0.81\" title=\"-0.015\">spoiler</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.43%); opacity: 0.81\" title=\"-0.009\">so</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 92.80%); opacity: 0.82\" title=\"-0.023\">kill</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.67%); opacity: 0.85\" title=\"-0.062\">off</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 89.39%); opacity: 0.83\" title=\"-0.040\">main</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.019\">character</span><span style=\"opacity: 0.80\">. and </span><span style=\"background-color: hsl(0, 100.00%, 86.73%); opacity: 0.84\" title=\"-0.056\">then</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.64%); opacity: 0.81\" title=\"-0.008\">bring</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.69%); opacity: 0.83\" title=\"0.044\">him</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.30%); opacity: 0.81\" title=\"0.013\">back</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 92.66%); opacity: 0.82\" title=\"-0.024\">another</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.60%); opacity: 0.85\" title=\"-0.063\">actor</span><span style=\"opacity: 0.80\">. jeeez! </span><span style=\"background-color: hsl(0, 100.00%, 90.59%); opacity: 0.83\" title=\"-0.034\">dallas</span><span style=\"opacity: 0.80\"> all </span><span style=\"background-color: hsl(0, 100.00%, 93.19%); opacity: 0.82\" title=\"-0.022\">over</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.91%); opacity: 0.83\" title=\"0.038\">again</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_prediction(classifier, txt_test[0], \n",
    "                vec=vectorizer, targets=[\"pos\"])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
