{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11. Automatic analysis of text\n",
    "#### Notebook for Python\n",
    "\n",
    "Van Atteveldt, W., Trilling, D. & Arcila, C. (2022). <a href=\"https://cssbook.net\" target=\"_blank\">Computational Analysis of Communication</a>. Wiley.\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ccs-amsterdam/ccsbook/blob/master/chapter11/chapter_11_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:chapter11install"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/wva/ccsbook/env/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: scikit-learn in /home/wva/ccsbook/env/lib/python3.8/site-packages (0.24.2)\n",
      "Requirement already satisfied: pandas in /home/wva/ccsbook/env/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: click in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (2021.7.6)\n",
      "Requirement already satisfied: tqdm in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: joblib in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn) (1.7.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: gensim in /home/wva/ccsbook/env/lib/python3.8/site-packages (4.0.1)\n",
      "Requirement already satisfied: eli5 in /home/wva/ccsbook/env/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: keras in /home/wva/ccsbook/env/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: tensorflow in /home/wva/ccsbook/env/lib/python3.8/site-packages (2.5.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from gensim) (1.7.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: six in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (1.15.0)\n",
      "Requirement already satisfied: graphviz in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (0.16)\n",
      "Requirement already satisfied: jinja2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (0.24.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (0.8.9)\n",
      "Requirement already satisfied: attrs>16.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (21.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/wva/ccsbook/env/lib/python3.8/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: h5py in /home/wva/ccsbook/env/lib/python3.8/site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (44.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from jinja2->eli5) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk scikit-learn pandas \n",
    "!pip3 install gensim eli5 keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "snippet:chapter11library"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# General packages and dictionary analysis\n",
    "import os\n",
    "import tarfile\n",
    "import bz2\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import eli5\n",
    "import joblib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer)\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import (\n",
    "    make_pipeline, Pipeline)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import eli5\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Deep learning with Keras\n",
    "from keras.layers import (Dense, Input, \n",
    "    GlobalMaxPooling1D, Conv1D, Embedding)\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import (\n",
    "    pad_sequences)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.keyedvectors import (\n",
    "    KeyedVectors)\n",
    "\n",
    "# Topic Modeling\n",
    "import gensim\n",
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import (\n",
    "    CoherenceModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "snippet:reviewdata"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://cssbook.net/d/aclImdb_v1.tar.gz\n",
      "Saving to reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "filename = \"reviewdata.pickle.bz2\"\n",
    "if os.path.exists(filename):\n",
    "  print(f\"Using cached file {filename}\")\n",
    "  with bz2.BZ2File(filename, \"r\") as zipfile:\n",
    "    data = pickle.load(zipfile)\n",
    "    text_train, text_test, y_train, y_test = data\n",
    "else:\n",
    "  url = \"https://cssbook.net/d/aclImdb_v1.tar.gz\"\n",
    "  print(f\"Downloading from {url}\")\n",
    "  fn, _headers = urllib.request.urlretrieve(url, \n",
    "                     filename=None)\n",
    "  t = tarfile.open(fn, mode=\"r:gz\")\n",
    "  text_train,text_test = [], []\n",
    "  y_train, y_test = [], []\n",
    "  for f in t.getmembers():\n",
    "    m=re.match(\"aclImdb/(\\w+)/(pos|neg)/\", f.name)\n",
    "    if not m:\n",
    "        # skip folder names, other categories\n",
    "        continue\n",
    "    dataset, label = m.groups()\n",
    "    text = t.extractfile(f).read().decode(\"utf-8\")\n",
    "    if dataset == \"train\":\n",
    "      text_train.append(text)\n",
    "      y_train.append(label)\n",
    "    elif dataset == \"test\":\n",
    "      text_test.append(text)\n",
    "      y_test.append(label)\n",
    "  print(f\"Saving to {filename}\")\n",
    "  with bz2.BZ2File(filename, \"w\") as zipfile:\n",
    "    data = text_train, text_test, y_train, y_test\n",
    "    pickle.dump(data, zipfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "snippet:sentsimple"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3,\n",
       " -4,\n",
       " 1,\n",
       " 3,\n",
       " -2,\n",
       " -7,\n",
       " -6,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 5,\n",
       " -1,\n",
       " 2,\n",
       " 7,\n",
       " -4,\n",
       " 2,\n",
       " 21,\n",
       " 1,\n",
       " -1,\n",
       " 2,\n",
       " -3,\n",
       " -2,\n",
       " -11,\n",
       " -2,\n",
       " -3,\n",
       " -7,\n",
       " 2,\n",
       " 4,\n",
       " -22,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " -5,\n",
       " -8,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " -4,\n",
       " 3,\n",
       " -7,\n",
       " -11,\n",
       " -6,\n",
       " 0,\n",
       " 3,\n",
       " -1,\n",
       " 0,\n",
       " 6,\n",
       " -1,\n",
       " -8,\n",
       " 7,\n",
       " -5,\n",
       " 2,\n",
       " 10,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " -8,\n",
       " 4,\n",
       " 3,\n",
       " 18,\n",
       " 2,\n",
       " 0,\n",
       " -3,\n",
       " -2,\n",
       " 5,\n",
       " 0,\n",
       " -2,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " -3,\n",
       " -4,\n",
       " -6,\n",
       " -2,\n",
       " 2,\n",
       " -7,\n",
       " -1,\n",
       " -10,\n",
       " -5,\n",
       " 3,\n",
       " 4,\n",
       " -3,\n",
       " -17,\n",
       " 1,\n",
       " -1,\n",
       " 7,\n",
       " -3,\n",
       " 4,\n",
       " 12,\n",
       " 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poswords = \"https://cssbook.net/d/positive.txt\"\n",
    "negwords = \"https://cssbook.net/d/negative.txt\"\n",
    "pos = set(requests.get(poswords).text.split(\"\\n\"))\n",
    "neg = set(requests.get(negwords).text.split(\"\\n\"))\n",
    "sentimentdict = {word:+1 for word in pos}\n",
    "sentimentdict.update({word:-1 for word in neg})\n",
    "\n",
    "scores = []\n",
    "mytokenizer = TreebankWordTokenizer()\n",
    "# For speed, we only take the first 100 reviews\n",
    "for review in text_train[:100]:\n",
    "    words = mytokenizer.tokenize(review)\n",
    "    # we look up each word in the sentiment dict \n",
    "    # and assign its value (with default 0)\n",
    "    scores.append(sum(sentimentdict.get(word,0) \n",
    "                      for word in words))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"reviewdata.pickle.bz2\"\n",
    "with bz2.BZ2File(filename, \"r\") as f:\n",
    "  txt_train,txt_test,y_train,y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "snippet:imdbbaseline"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.88      0.83     12500\n",
      "         pos       0.86      0.76      0.81     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(txt_train)\n",
    "X_test = vectorizer.transform(txt_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "rep=metrics.classification_report(y_test, y_pred)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:basiccomparisons"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.79\t\t0.88\n",
      "pos:\t0.87\t\t0.77\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.80\t\t0.88\n",
      "pos:\t0.87\t\t0.78\n",
      "\n",
      "\n",
      "LR-Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.85\t\t0.87\n",
      "pos:\t0.87\t\t0.85\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.88\t\t0.89\n",
      "pos:\t0.89\t\t0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def short_classification_report (y_test, y_pred):\n",
    "  print(\"    \\tPrecision\\tRecall\")\n",
    "  for label in set(y_pred):\n",
    "    pr = metrics.precision_score(y_test, y_pred, \n",
    "                                 pos_label=label)\n",
    "    re = metrics.recall_score(y_test,y_pred, \n",
    "                              pos_label=label)\n",
    "    print(f\"{label}:\\t{pr:0.2f}\\t\\t{re:0.2f}\")\n",
    "\n",
    "configs = [\n",
    "  (\"NB-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"NB-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"LR-Count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"LR-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\"))]\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(txt_train)\n",
    "    X_test = vectorizer.transform(txt_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:basicpipe"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.79\t\t0.88\n",
      "pos:\t0.87\t\t0.77\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.80\t\t0.88\n",
      "pos:\t0.87\t\t0.78\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.85\t\t0.87\n",
      "pos:\t0.87\t\t0.85\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.88\t\t0.89\n",
      "pos:\t0.89\t\t0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    pipe = make_pipeline(vectorizer, classifier)\n",
    "    pipe.fit(text_train, y_train)\n",
    "    y_pred = pipe.predict(text_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:gridsearchlogreg"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 2)}\n",
      "    \tPrecision\tRecall\n",
      "neg:\t0.90\t\t0.90\n",
      "pos:\t0.90\t\t0.90\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "  (\"vectorizer\", TfidfVectorizer()), \n",
    "  (\"classifier\", LogisticRegression(\n",
    "      solver=\"liblinear\"))])\n",
    "grid = {\"vectorizer__ngram_range\": [(1,1), (1,2)],\n",
    "        \"vectorizer__max_df\": [0.5, 1.0],\n",
    "        \"vectorizer__min_df\": [0, 5],\n",
    "        \"classifier__C\": [0.01, 1, 100]\n",
    "       }\n",
    "search=GridSearchCV(estimator=pipeline, n_jobs=-1,\n",
    "  param_grid=grid,scoring=\"accuracy\", cv=5)\n",
    "search.fit(txt_train, y_train)\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "pred = search.predict(txt_test)\n",
    "print(short_classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "snippet:vader"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/wva/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0]\n",
      " [    6  6706  5788]\n",
      " [    5  1748 10747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   dont know       0.00      0.00      0.00         0\n",
      "         neg       0.79      0.54      0.64     12500\n",
      "         pos       0.65      0.86      0.74     12500\n",
      "\n",
      "    accuracy                           0.70     25000\n",
      "   macro avg       0.48      0.47      0.46     25000\n",
      "weighted avg       0.72      0.70      0.69     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wva/ccsbook/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pred = []\n",
    "for review in text_test:\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    if sentiment[\"compound\"]>0:\n",
    "        pred.append(\"pos\")\n",
    "    elif sentiment[\"compound\"]<0:\n",
    "        pred.append(\"neg\")\n",
    "    else:\n",
    "        pred.append(\"dont know\")\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, pred))\n",
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "snippet:reuse"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a great movie' is probably 'pos'.\n",
      "'I hated this one.' is probably 'neg'.\n",
      "'What an awful fail' is probably 'neg'.\n"
     ]
    }
   ],
   "source": [
    "# Make a vectorizer and train a classifier\n",
    "vectorizer=TfidfVectorizer(min_df=5, max_df=.5)\n",
    "classifier=LogisticRegression(solver=\"liblinear\")\n",
    "X_train=vectorizer.fit_transform(txt_train)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save them to disk\n",
    "with open(\"myvectorizer.pkl\",mode=\"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(\"myclassifier.pkl\",mode=\"wb\") as f:\n",
    "    joblib.dump(classifier, f)\n",
    "  \n",
    "# Later on, re-load this classifier and apply:\n",
    "new_texts = [\"This is a great movie\", \n",
    "            \"I hated this one.\", \n",
    "            \"What an awful fail\"]\n",
    "\n",
    "with open(\"myvectorizer.pkl\",mode=\"rb\") as f:\n",
    "    myvectorizer = pickle.load(f)\n",
    "with open(\"myclassifier.pkl\",mode=\"rb\") as f:\n",
    "    myclassifier = joblib.load(f)\n",
    "    \n",
    "new_features = myvectorizer.transform(new_texts)\n",
    "pred = myclassifier.predict(new_features)\n",
    "\n",
    "for review, label in zip(new_texts, pred):\n",
    "    print(f\"'{review}' is probably '{label}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "output:html",
     "snippet:eli5"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.173\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        great\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +6.101\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        excellent\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        best\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.791\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        perfect\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13663 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13574 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.337\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        poor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.733\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        boring\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.315\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        waste\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.349\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        awful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -7.347\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        bad\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -9.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worst\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(min_df=5, max_df=.5), \n",
    "    LogisticRegression(solver=\"liblinear\"))\n",
    "pipe.fit(txt_train, y_train)\n",
    "eli5.show_weights(pipe, top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:eli5b",
     "output:html"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.140</b>, score <b>-1.817</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.830\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"opacity: 0.80\">i </span><span style=\"background-color: hsl(120, 100.00%, 74.97%); opacity: 0.90\" title=\"0.138\">love</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 84.32%); opacity: 0.85\" title=\"-0.071\">am</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.56%); opacity: 0.83\" title=\"0.034\">willing</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 93.55%); opacity: 0.81\" title=\"0.020\">put</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.84%); opacity: 0.80\" title=\"-0.002\">up</span><span style=\"opacity: 0.80\"> with a </span><span style=\"background-color: hsl(120, 100.00%, 84.06%); opacity: 0.85\" title=\"0.072\">lot</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.44%); opacity: 0.81\" title=\"0.012\">movies</span><span style=\"opacity: 0.80\">/</span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(0, 100.00%, 90.89%); opacity: 0.82\" title=\"-0.033\">usually</span><span style=\"opacity: 0.80\"> underfunded, </span><span style=\"background-color: hsl(120, 100.00%, 98.46%); opacity: 0.80\" title=\"0.003\">under</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 78.84%); opacity: 0.88\" title=\"0.109\">appreciated</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 94.35%); opacity: 0.81\" title=\"0.016\">misunderstood</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 83.48%); opacity: 0.86\" title=\"-0.076\">tried</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 96.37%); opacity: 0.81\" title=\"-0.009\">like</span><span style=\"opacity: 0.80\"> this, i </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.20%); opacity: 0.83\" title=\"-0.047\">did</span><span style=\"opacity: 0.80\">, but it is to </span><span style=\"background-color: hsl(120, 100.00%, 85.25%); opacity: 0.85\" title=\"0.065\">good</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 89.60%); opacity: 0.83\" title=\"-0.039\">babylon</span><span style=\"opacity: 0.80\"> 5 is to </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.008\">star</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.57%); opacity: 0.83\" title=\"0.040\">trek</span><span style=\"opacity: 0.80\"> (the </span><span style=\"background-color: hsl(0, 100.00%, 81.88%); opacity: 0.86\" title=\"-0.087\">original</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(0, 100.00%, 77.74%); opacity: 0.89\" title=\"-0.117\">silly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.29%); opacity: 0.81\" title=\"-0.009\">prosthetics</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.30%); opacity: 0.90\" title=\"-0.135\">cheap</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.76%); opacity: 0.88\" title=\"-0.102\">cardboard</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.58%); opacity: 0.80\" title=\"0.000\">sets</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 82.16%); opacity: 0.86\" title=\"-0.085\">stilted</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.59%); opacity: 0.82\" title=\"-0.029\">dialogues</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 98.03%); opacity: 0.80\" title=\"0.004\">cg</span><span style=\"opacity: 0.80\"> that </span><span style=\"background-color: hsl(0, 100.00%, 84.47%); opacity: 0.85\" title=\"-0.070\">doesn</span><span style=\"opacity: 0.80\">&#x27;t </span><span style=\"background-color: hsl(120, 100.00%, 88.93%); opacity: 0.83\" title=\"0.043\">match</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(120, 100.00%, 95.63%); opacity: 0.81\" title=\"0.011\">background</span><span style=\"opacity: 0.80\">, and </span><span style=\"background-color: hsl(0, 100.00%, 81.55%); opacity: 0.87\" title=\"-0.089\">painfully</span><span style=\"opacity: 0.80\"> one-</span><span style=\"background-color: hsl(0, 100.00%, 82.32%); opacity: 0.86\" title=\"-0.084\">dimensional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.35%); opacity: 0.81\" title=\"-0.012\">characters</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.88%); opacity: 0.84\" title=\"-0.049\">cannot</span><span style=\"opacity: 0.80\"> be </span><span style=\"background-color: hsl(120, 100.00%, 93.49%); opacity: 0.81\" title=\"0.020\">overcome</span><span style=\"opacity: 0.80\"> with a &#x27;</span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\">&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 98.34%); opacity: 0.80\" title=\"0.003\">setting</span><span style=\"opacity: 0.80\">. (i&#x27;m </span><span style=\"background-color: hsl(120, 100.00%, 95.32%); opacity: 0.81\" title=\"0.013\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.94%); opacity: 0.83\" title=\"-0.038\">there</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(120, 100.00%, 89.83%); opacity: 0.83\" title=\"0.038\">those</span><span style=\"opacity: 0.80\"> of you </span><span style=\"background-color: hsl(0, 100.00%, 97.93%); opacity: 0.80\" title=\"-0.004\">out</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.94%); opacity: 0.83\" title=\"-0.038\">there</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.97%); opacity: 0.81\" title=\"0.018\">who</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.55%); opacity: 0.84\" title=\"0.057\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.60%); opacity: 0.83\" title=\"-0.039\">babylon</span><span style=\"opacity: 0.80\"> 5 is </span><span style=\"background-color: hsl(120, 100.00%, 85.25%); opacity: 0.85\" title=\"0.065\">good</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.012\">tv</span><span style=\"opacity: 0.80\">. it&#x27;s not. it&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 87.19%); opacity: 0.84\" title=\"-0.053\">clich√©d</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 90.71%); opacity: 0.82\" title=\"-0.033\">uninspiring</span><span style=\"opacity: 0.80\">.) </span><span style=\"background-color: hsl(120, 100.00%, 91.75%); opacity: 0.82\" title=\"0.028\">while</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.71%); opacity: 0.83\" title=\"0.044\">us</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.42%); opacity: 0.81\" title=\"0.009\">viewers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.42%); opacity: 0.86\" title=\"-0.077\">might</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.37%); opacity: 0.81\" title=\"-0.009\">like</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.35%); opacity: 0.82\" title=\"0.021\">emotion</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.019\">character</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.18%); opacity: 0.85\" title=\"-0.072\">development</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.014\">sci</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 94.57%); opacity: 0.81\" title=\"0.016\">fi</span><span style=\"opacity: 0.80\"> is a </span><span style=\"background-color: hsl(120, 100.00%, 81.94%); opacity: 0.86\" title=\"0.087\">genre</span><span style=\"opacity: 0.80\"> that </span><span style=\"background-color: hsl(0, 100.00%, 94.59%); opacity: 0.81\" title=\"-0.015\">does</span><span style=\"opacity: 0.80\"> not </span><span style=\"background-color: hsl(120, 100.00%, 94.52%); opacity: 0.81\" title=\"0.016\">take</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.61%); opacity: 0.81\" title=\"-0.015\">itself</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.20%); opacity: 0.85\" title=\"-0.072\">seriously</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 98.50%); opacity: 0.80\" title=\"-0.002\">cf</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.69%); opacity: 0.81\" title=\"-0.008\">star</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.57%); opacity: 0.83\" title=\"0.040\">trek</span><span style=\"opacity: 0.80\">). it </span><span style=\"background-color: hsl(120, 100.00%, 83.07%); opacity: 0.86\" title=\"0.079\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.88%); opacity: 0.86\" title=\"0.080\">treat</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.33%); opacity: 0.82\" title=\"0.025\">important</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.49%); opacity: 0.82\" title=\"0.025\">issues</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 89.39%); opacity: 0.83\" title=\"0.040\">yet</span><span style=\"opacity: 0.80\"> not as a </span><span style=\"background-color: hsl(0, 100.00%, 98.87%); opacity: 0.80\" title=\"-0.002\">serious</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.07%); opacity: 0.83\" title=\"-0.037\">philosophy</span><span style=\"opacity: 0.80\">. it&#x27;s </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.81%); opacity: 0.80\" title=\"-0.004\">difficult</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 85.34%); opacity: 0.85\" title=\"-0.064\">care</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.37%); opacity: 0.80\" title=\"0.003\">about</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(0, 100.00%, 95.35%); opacity: 0.81\" title=\"-0.012\">characters</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.96%); opacity: 0.83\" title=\"-0.037\">here</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 91.27%); opacity: 0.82\" title=\"-0.031\">they</span><span style=\"opacity: 0.80\"> are not </span><span style=\"background-color: hsl(0, 100.00%, 86.77%); opacity: 0.84\" title=\"-0.055\">simply</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.78%); opacity: 0.81\" title=\"-0.015\">foolish</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 84.01%); opacity: 0.85\" title=\"-0.073\">just</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.72%); opacity: 0.81\" title=\"-0.015\">missing</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 91.72%); opacity: 0.82\" title=\"-0.028\">spark</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 82.17%); opacity: 0.86\" title=\"0.085\">life</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 93.88%); opacity: 0.81\" title=\"0.018\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.80%); opacity: 0.82\" title=\"0.033\">actions</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 97.25%); opacity: 0.80\" title=\"0.006\">reactions</span><span style=\"opacity: 0.80\"> are </span><span style=\"background-color: hsl(0, 100.00%, 72.96%); opacity: 0.91\" title=\"-0.154\">wooden</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 71.45%); opacity: 0.92\" title=\"-0.167\">predictable</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 83.66%); opacity: 0.86\" title=\"0.075\">often</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 75.41%); opacity: 0.90\" title=\"-0.135\">painful</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(120, 100.00%, 88.71%); opacity: 0.83\" title=\"0.044\">watch</span><span style=\"opacity: 0.80\">. the </span><span style=\"background-color: hsl(0, 100.00%, 86.87%); opacity: 0.84\" title=\"-0.055\">makers</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(0, 100.00%, 90.65%); opacity: 0.83\" title=\"-0.034\">earth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.70%); opacity: 0.83\" title=\"0.039\">know</span><span style=\"opacity: 0.80\"> it&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 77.95%); opacity: 0.89\" title=\"-0.115\">rubbish</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 91.27%); opacity: 0.82\" title=\"-0.031\">they</span><span style=\"opacity: 0.80\"> have to </span><span style=\"background-color: hsl(120, 100.00%, 80.41%); opacity: 0.87\" title=\"0.097\">always</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.42%); opacity: 0.80\" title=\"-0.005\">say</span><span style=\"opacity: 0.80\"> &quot;</span><span style=\"background-color: hsl(120, 100.00%, 90.64%); opacity: 0.83\" title=\"0.034\">gene</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.70%); opacity: 0.80\" title=\"0.002\">roddenberry</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 90.65%); opacity: 0.83\" title=\"-0.034\">earth</span><span style=\"opacity: 0.80\">...&quot; </span><span style=\"background-color: hsl(0, 100.00%, 82.18%); opacity: 0.86\" title=\"-0.085\">otherwise</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.03%); opacity: 0.81\" title=\"0.018\">people</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.85%); opacity: 0.85\" title=\"-0.067\">would</span><span style=\"opacity: 0.80\"> not </span><span style=\"background-color: hsl(120, 100.00%, 96.70%); opacity: 0.81\" title=\"0.008\">continue</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.038\">watching</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 98.70%); opacity: 0.80\" title=\"0.002\">roddenberry</span><span style=\"opacity: 0.80\">&#x27;s </span><span style=\"background-color: hsl(0, 100.00%, 97.27%); opacity: 0.80\" title=\"-0.006\">ashes</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.15%); opacity: 0.84\" title=\"0.047\">must</span><span style=\"opacity: 0.80\"> be </span><span style=\"background-color: hsl(0, 100.00%, 91.20%); opacity: 0.82\" title=\"-0.031\">turning</span><span style=\"opacity: 0.80\"> in </span><span style=\"background-color: hsl(120, 100.00%, 93.88%); opacity: 0.81\" title=\"0.018\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.28%); opacity: 0.80\" title=\"-0.006\">orbit</span><span style=\"opacity: 0.80\"> as this </span><span style=\"background-color: hsl(0, 100.00%, 61.98%); opacity: 0.99\" title=\"-0.251\">dull</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.30%); opacity: 0.90\" title=\"-0.135\">cheap</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.270\">poorly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.05%); opacity: 0.81\" title=\"0.014\">edited</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 89.89%); opacity: 0.83\" title=\"-0.038\">watching</span><span style=\"opacity: 0.80\"> it </span><span style=\"background-color: hsl(120, 100.00%, 89.47%); opacity: 0.83\" title=\"0.040\">without</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.94%); opacity: 0.82\" title=\"0.032\">advert</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.12%); opacity: 0.80\" title=\"-0.003\">breaks</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.45%); opacity: 0.80\" title=\"0.005\">really</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.36%); opacity: 0.85\" title=\"0.064\">brings</span><span style=\"opacity: 0.80\"> this </span><span style=\"background-color: hsl(120, 100.00%, 96.48%); opacity: 0.81\" title=\"0.008\">home</span><span style=\"opacity: 0.80\">) trudging trabant of a </span><span style=\"background-color: hsl(120, 100.00%, 94.59%); opacity: 0.81\" title=\"0.015\">show</span><span style=\"opacity: 0.80\"> lumbers </span><span style=\"background-color: hsl(0, 100.00%, 99.48%); opacity: 0.80\" title=\"-0.001\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.77%); opacity: 0.81\" title=\"-0.007\">space</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 94.79%); opacity: 0.81\" title=\"-0.015\">spoiler</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.43%); opacity: 0.81\" title=\"-0.009\">so</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 92.80%); opacity: 0.82\" title=\"-0.023\">kill</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.67%); opacity: 0.85\" title=\"-0.062\">off</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(0, 100.00%, 89.39%); opacity: 0.83\" title=\"-0.040\">main</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.019\">character</span><span style=\"opacity: 0.80\">. and </span><span style=\"background-color: hsl(0, 100.00%, 86.73%); opacity: 0.84\" title=\"-0.056\">then</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.64%); opacity: 0.81\" title=\"-0.008\">bring</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.69%); opacity: 0.83\" title=\"0.044\">him</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.30%); opacity: 0.81\" title=\"0.013\">back</span><span style=\"opacity: 0.80\"> as </span><span style=\"background-color: hsl(0, 100.00%, 92.66%); opacity: 0.82\" title=\"-0.024\">another</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.60%); opacity: 0.85\" title=\"-0.063\">actor</span><span style=\"opacity: 0.80\">. jeeez! </span><span style=\"background-color: hsl(0, 100.00%, 90.59%); opacity: 0.83\" title=\"-0.034\">dallas</span><span style=\"opacity: 0.80\"> all </span><span style=\"background-color: hsl(0, 100.00%, 93.19%); opacity: 0.82\" title=\"-0.022\">over</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.91%); opacity: 0.83\" title=\"0.038\">again</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_prediction(classifier, txt_test[0], \n",
    "                vec=vectorizer, targets=[\"pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "snippet:rnndata",
     "output:table"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "      <th>lemmata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007</td>\n",
       "      <td>0</td>\n",
       "      <td>Rabobank voorspellen flink stijging hypotheekr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10027</td>\n",
       "      <td>0</td>\n",
       "      <td>D66 willen reserve provincie aanspreken voor g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10037</td>\n",
       "      <td>1</td>\n",
       "      <td>UWV dit jaar veel baan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10059</td>\n",
       "      <td>1</td>\n",
       "      <td>proost op geslaagd beursgang bols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10099</td>\n",
       "      <td>0</td>\n",
       "      <td>helft werknemer gaan na 65ste met pensioen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  value                                            lemmata\n",
       "0  10007      0  Rabobank voorspellen flink stijging hypotheekr...\n",
       "1  10027      0  D66 willen reserve provincie aanspreken voor g...\n",
       "2  10037      1                             UWV dit jaar veel baan\n",
       "3  10059      1                  proost op geslaagd beursgang bols\n",
       "4  10099      0         helft werknemer gaan na 65ste met pensioen"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://cssbook.net/d/dutch_sentiment.csv\"\n",
    "h = pd.read_csv(url)\n",
    "h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "snippet:rnnmodel"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RNN model\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 320)         2176640   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 128)          123008    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,307,969\n",
      "Trainable params: 2,307,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tokenize texts\n",
    "tokenizer=Tokenizer(num_words=9999)\n",
    "tokenizer.fit_on_texts(h.lemmata)\n",
    "word_index=tokenizer.word_index\n",
    "sequences=tokenizer.texts_to_sequences(h.lemmata)\n",
    "tokens=pad_sequences(sequences, maxlen=1000)\n",
    "\n",
    "# Prepare embeddings layer\n",
    "fn = \"w2v_320d_trimmed\"\n",
    "if not os.path.exists(fn):\n",
    "    url = f\"https://cssbook.net/d/{fn}\"\n",
    "    print(f\"Downloading embeddings from {url}\")\n",
    "    urllib.request.urlretrieve(url, fn)\n",
    "embeddings = KeyedVectors.load_word2vec_format(fn)\n",
    "emb_matrix = np.zeros(\n",
    "    (len(tokenizer.word_index) + 1, \n",
    "     embeddings.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embeddings:\n",
    "        emb_matrix[i] = embeddings[word]\n",
    "embedding_layer = Embedding(\n",
    "    emb_matrix.shape[0], emb_matrix.shape[1],\n",
    "    input_length=tokens.shape[1], trainable=True,\n",
    "    weights=[emb_matrix])\n",
    "    \n",
    "print(\"Building CNN model\")\n",
    "sequence_input = Input(shape=(tokens.shape[1],), \n",
    "                       dtype=\"int32\")\n",
    "seq = embedding_layer(sequence_input)\n",
    "m = Conv1D(filters=128, kernel_size=3,\n",
    "           activation=\"relu\")(seq)\n",
    "m = GlobalMaxPooling1D()(m)\n",
    "m = Dense(64, activation=\"relu\")(m)\n",
    "preds = Dense(1, activation=\"tanh\")(m)\n",
    "m = Model(sequence_input, preds)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "snippet:rnn"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "32/32 [==============================] - 27s 828ms/step - loss: 0.6807\n",
      "Validate against test data\n",
      "Accuracy: 0.46468561584840656\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "train_data = tokens[:4000]\n",
    "test_data = tokens[4000:]\n",
    "train_labels = h.value[:4000]\n",
    "test_labels = h.value[4000:]\n",
    "\n",
    "# Train model\n",
    "m.compile(loss=\"mean_absolute_error\", \n",
    "          optimizer=RMSprop(lr=.004))\n",
    "labels = np.asarray([[x] for x in train_labels])\n",
    "m.fit(train_data,labels,epochs=5,batch_size=128)\n",
    "\n",
    "# Validate against test data\n",
    "output = m.predict(test_data)\n",
    "# Bin output into -1, 0, 1\n",
    "pred=[1 if x[0]>.3 else (0 if x[0]>-.3 else -1) \n",
    "      for x in output]\n",
    "correct=[x==y for (x,y) in zip(pred,test_labels)]\n",
    "acc = sum(correct) / len(pred)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "snippet:lda1"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<738x684 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://cssbook.net/d/sotu.csv\"\n",
    "sotu = pd.read_csv(url)\n",
    "p_obama = (sotu[sotu.President == \"Obama\"]\n",
    "           .text.str.split(\"\\n\\n\").explode())\n",
    "\n",
    "\n",
    "cv = CountVectorizer(min_df=.01, \n",
    "                     stop_words=\"english\")\n",
    "dtm = cv.fit_transform(p_obama)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:lda2",
     "output:table"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>america</td>\n",
       "      <td>american</td>\n",
       "      <td>year</td>\n",
       "      <td>people</td>\n",
       "      <td>america</td>\n",
       "      <td>energy</td>\n",
       "      <td>new</td>\n",
       "      <td>america</td>\n",
       "      <td>america</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>jobs</td>\n",
       "      <td>americans</td>\n",
       "      <td>new</td>\n",
       "      <td>time</td>\n",
       "      <td>jobs</td>\n",
       "      <td>years</td>\n",
       "      <td>americans</td>\n",
       "      <td>tax</td>\n",
       "      <td>jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "      <td>years</td>\n",
       "      <td>know</td>\n",
       "      <td>ve</td>\n",
       "      <td>ve</td>\n",
       "      <td>people</td>\n",
       "      <td>make</td>\n",
       "      <td>hard</td>\n",
       "      <td>work</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>like</td>\n",
       "      <td>people</td>\n",
       "      <td>just</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "      <td>jobs</td>\n",
       "      <td>people</td>\n",
       "      <td>states</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>let</td>\n",
       "      <td>new</td>\n",
       "      <td>american</td>\n",
       "      <td>year</td>\n",
       "      <td>years</td>\n",
       "      <td>work</td>\n",
       "      <td>years</td>\n",
       "      <td>americans</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>world</td>\n",
       "      <td>ve</td>\n",
       "      <td>american</td>\n",
       "      <td>make</td>\n",
       "      <td>years</td>\n",
       "      <td>america</td>\n",
       "      <td>need</td>\n",
       "      <td>nation</td>\n",
       "      <td>businesses</td>\n",
       "      <td>families</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>help</td>\n",
       "      <td>americans</td>\n",
       "      <td>years</td>\n",
       "      <td>time</td>\n",
       "      <td>security</td>\n",
       "      <td>just</td>\n",
       "      <td>people</td>\n",
       "      <td>work</td>\n",
       "      <td>god</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy</td>\n",
       "      <td>people</td>\n",
       "      <td>like</td>\n",
       "      <td>care</td>\n",
       "      <td>change</td>\n",
       "      <td>work</td>\n",
       "      <td>let</td>\n",
       "      <td>ve</td>\n",
       "      <td>bless</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>let</td>\n",
       "      <td>year</td>\n",
       "      <td>ve</td>\n",
       "      <td>like</td>\n",
       "      <td>working</td>\n",
       "      <td>world</td>\n",
       "      <td>america</td>\n",
       "      <td>world</td>\n",
       "      <td>united</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>know</td>\n",
       "      <td>home</td>\n",
       "      <td>make</td>\n",
       "      <td>country</td>\n",
       "      <td>work</td>\n",
       "      <td>clean</td>\n",
       "      <td>economy</td>\n",
       "      <td>time</td>\n",
       "      <td>people</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0    Topic 1    Topic 2   Topic 3   Topic 4  Topic 5  Topic 6  \\\n",
       "0   america   american       year    people   america   energy      new   \n",
       "1  american       jobs  americans       new      time     jobs    years   \n",
       "2    people      years       know        ve        ve   people     make   \n",
       "3       new       like     people      just       new      new     jobs   \n",
       "4     right        let        new  american      year    years     work   \n",
       "5     world         ve   american      make     years  america     need   \n",
       "6      help  americans      years      time  security     just   people   \n",
       "7   economy     people       like      care    change     work      let   \n",
       "8       let       year         ve      like   working    world  america   \n",
       "9      know       home       make   country      work    clean  economy   \n",
       "\n",
       "     Topic 7     Topic 8     Topic 9  \n",
       "0    america     america         new  \n",
       "1  americans         tax        jobs  \n",
       "2       hard        work        just  \n",
       "3     people      states        help  \n",
       "4      years   americans     country  \n",
       "5     nation  businesses    families  \n",
       "6       work         god        make  \n",
       "7         ve       bless       world  \n",
       "8      world      united  government  \n",
       "9       time      people        fact  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = matutils.Sparse2Corpus(dtm, \n",
    "    documents_columns=False)\n",
    "vocab = dict(enumerate(cv.get_feature_names()))\n",
    "\n",
    "lda = LdaModel(corpus,id2word=vocab,num_topics=10,\n",
    "    random_state=123, alpha=\"asymmetric\")\n",
    "pd.DataFrame({f\"Topic {n}\":[w for (w,tw) in words]\n",
    "    for (n, words) in \n",
    "    lda.show_topics(formatted=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "snippet:ldaresults1",
     "output:table"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstName</th>\n",
       "      <th>President</th>\n",
       "      <th>Date</th>\n",
       "      <th>delivery</th>\n",
       "      <th>type</th>\n",
       "      <th>party</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>2009-02-24</td>\n",
       "      <td>spoken</td>\n",
       "      <td>other</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>0.022992</td>\n",
       "      <td>0.905655</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.009984</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.005879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>2009-02-24</td>\n",
       "      <td>spoken</td>\n",
       "      <td>other</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.922522</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.004410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>2009-02-24</td>\n",
       "      <td>spoken</td>\n",
       "      <td>other</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>0.516337</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.254523</td>\n",
       "      <td>0.207272</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>2009-02-24</td>\n",
       "      <td>spoken</td>\n",
       "      <td>other</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>0.946834</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.003528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barack</td>\n",
       "      <td>Obama</td>\n",
       "      <td>2009-02-24</td>\n",
       "      <td>spoken</td>\n",
       "      <td>other</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.006493</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.962295</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.002205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FirstName President        Date delivery   type       party         0  \\\n",
       "0    Barack     Obama  2009-02-24   spoken  other  Democratic  0.022992   \n",
       "1    Barack     Obama  2009-02-24   spoken  other  Democratic  0.017557   \n",
       "2    Barack     Obama  2009-02-24   spoken  other  Democratic  0.516337   \n",
       "3    Barack     Obama  2009-02-24   spoken  other  Democratic  0.946834   \n",
       "4    Barack     Obama  2009-02-24   spoken  other  Democratic  0.008808   \n",
       "\n",
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.905655  0.013877  0.011605  0.009984  0.008761  0.007804  0.007036   \n",
       "1  0.012980  0.010400  0.008707  0.007489  0.922522  0.005853  0.005277   \n",
       "2  0.005989  0.254523  0.207272  0.003456  0.003033  0.002702  0.002436   \n",
       "3  0.010359  0.008321  0.006964  0.005991  0.005256  0.004683  0.004222   \n",
       "4  0.006493  0.005202  0.962295  0.003744  0.003285  0.002927  0.002639   \n",
       "\n",
       "          8         9  \n",
       "0  0.006406  0.005879  \n",
       "1  0.004805  0.004410  \n",
       "2  0.002218  0.002035  \n",
       "3  0.003844  0.003528  \n",
       "4  0.002402  0.002205  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = pd.DataFrame(\n",
    "    [dict(lda.get_document_topics(doc, \n",
    "          minimum_probability=0.0))\n",
    "     for doc in corpus])\n",
    "meta = (sotu.iloc[p_obama.index]\n",
    "        .drop(columns=[\"text\"])\n",
    "        .reset_index(drop=True))\n",
    "tpd = pd.concat([meta, topics], axis=1)\n",
    "tpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:ldaresults2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622: I intend to protect a free and open Internet, extend its reach to every classroom and every community and help folks build the fastest networks so that the next generation of digital innovators and entrepreneurs have the platform to keep reshaping our world. I want Americans to win the race for the kinds of discoveries that unleash new jobs: converting sunlight into liquid fuel; creating revolutionary prosthetics so that a veteran who gave his arms for his country can play catch with his kids again; pushing out into the solar system not just to visit, but to stay. Last month, we launched a new spacecraft as part of a reenergized space program that will send American astronauts to Mars. And in 2 months, to prepare us for those missions, Scott Kelly will begin a year-long stay in space. So good luck, Captain. Make sure to Instagram it. We're proud of you.\n",
      "11: Because of this plan, there are teachers who can now keep their jobs and educate our kids, health care professionals can continue caring for our sick. There are 57 police officers who are still on the streets of Minneapolis tonight because this plan prevented the layoffs their department was about to make. Because of this plan, 95 percent of working households in America will receive a tax cut; a tax cut that you will see in your paychecks beginning on April 1st. Because of this plan, families who are struggling to pay tuition costs will receive a $2,500 tax credit for all 4 years of college, and Americans who have lost their jobs in this recession will be able to receive extended unemployment benefits and continued health care coverage to help them weather this storm.\n",
      "322: I want every American looking for work to have the same opportunity as Jackie did. Join me in a national commitment to train 2 million Americans with skills that will lead directly to a job. My administration has already lined up more companies that want to help. Model partnerships between businesses like Siemens and community colleges in places like Charlotte, and Orlando, and Louisville are up and running. Now you need to give more community colleges the resources they need to become community career centers -- places that teach people skills that businesses are looking for right now, from data management to high-tech manufacturing.\n"
     ]
    }
   ],
   "source": [
    "for docid in [622, 11, 322]:\n",
    "    print(f\"{docid}: {list(p_obama)[docid]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "snippet:ldacoherence",
     "output:png"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55670/986365987.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"perplexity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coherence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "for k in [5, 10, 15, 20, 25, 30]:\n",
    "  m = LdaModel(corpus,num_topics=k,id2word=vocab, \n",
    "          random_state=123, alpha=\"asymmetric\")\n",
    "  perplexity = m.log_perplexity(corpus)\n",
    "  coherence=CoherenceModel(model=m,corpus=corpus,\n",
    "      coherence=\"u_mass\").get_coherence()\n",
    "  result.append(dict(k=k, perplexity=perplexity, \n",
    "                     coherence=coherence))\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "result.plot(x=\"k\", y=[\"perplexity\", \"coherence\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
