\section{Advanced representation of text}
\label{ngram}

The examples above all created document-term matrices where each column actually represents a word.
There is more information in a text, however, than pure word counts.
The phrases: \emph{the movie was not good, it was in fact quite bad} and \emph{the movie was not bad, in fact it was quite good}
have exactly the same word frequencies, but are quite different in meaning.
Similarly, \emph{the new residents of York} and \emph{the residents of New York} are talking about quite different people. 

Of course, in the end which aspect of the meaning of a text is important depends on your research question:
if you want to know the sentiment about the movie, it is important to take a word like `not' into account;
but if you are interested in the topic or genre of the review, or the extremity of the language used, this might not be relevant.

The core idea of this section is that in many cases this information can be captured in a DTM by having the columns represent different information than just words, for example word combinations or groups of related words.
This is often called \concept{feature engineering}, as we are using our domain expertise to find the right features (columns, independent variables) to capture the relevant meaning for our research question.
If we are using other columns than words it is also technically more correct to use the name \concept{Document-feature matrix}, as \quanteda\ does, but we will stick to the most common name here and simply continue using the name DTM.

\subsection{N-grams}

The first feature we will discuss are n-grams.
The simplest case is a bigram (or 2-gram), where each feature is a pair of adjacent words.
The example used above, \emph{the movie was not bad}, will yield the following bigrams: \emph{the-movie}, \emph{movie-was}, \emph{was-not}, and \emph{not-bad}.
Each of those bigrams is then treated as a feature, that is, a DTM would contain one column for each word pair.

As you can see in this example, we can now see the difference between \emph{not-bad} and \emph{not-good}.
The downside of using n-grams is that there are many more unique word pairs than unique words,
so the resulting DTM will have many more columns.
Moreover, there is a bigger \concept{data scarcity problem}, as each of those pairs will be less frequent,
making it more difficult to find sufficient examples of each to generalize over.

Although bigrams are the most frequent use case, trigrams (3-grams) and (rarely) higher-order n-grams can also be used.
As you can imagine, this will create even bigger DTMS and worse data scarcity problems,
so require even more attention paid to feature selection and/or trimming.


\pyrex[caption=Generating n-grams]{ch_dtm/ngram}

\refex{ngram} shows how n-grams can be created and used in Python and R.
In Python, you can pass the \verb|ngram=(n, m)| option to the vectorizer,
while R has a \verb|tokens_ngrams(n:m)| function.
Both will post-process the tokens to create all n-grams in the range of n to m.
In this example, we are asking for unigrams (i.e., the words themselves), bigrams and trigrams of a simple examp[le sentence.
Both languages produce the same output, with R separating the words with an underscore while python uses a simple space.

\pyrex[caption=Words and bigrams containing 'government',output=r,format=table]{ch_dtm/ngram2}

\refex{ngram2} shows how you can generate n-grams for a whole corpus.
In this case, we create a DTM of the state of the union matrix with all bigrams included.
A glance at the frequency table for all words containing \emph{government} shows that,
besides the word itself and it's plural and possessive forms, the bigrams include compound words (federal and local government),
phrases with  the government as subject (the government can and must), and nouns for which the government is an adjective
(government spending and government programs).

You can imagine that including all these words as features will add many possibilities for analysis of the DTM
which would not be possible in a normal bag-of-words approach.
The terms local and federal government can be quite important to understand policy positions,
but for e.g. sentiment analysis a bigram like \emph{not good} would also be insightful
(but make sure 'not' is not on your stop word list!).   

\subsection{Collocations}

A special case of n-grams are collocations.
In the strict corpus linguistic sense of the word, collocations are pairs of words that occur more frequently than expected
based on their underlying occurence.
For example, the prhase \emph{crystal clear} presumably occurs much more often than would be expected by chance given
how often \emph{crystal} and \emph{clear} occur separately.
Collocations are important for text analysis since they often have a specific meaning,
for example because they refer to names such as \emph{New York} or disambiguate a term like \emph{sound} in \emph{sound asleep},
a \emph{sound proposal}, or \emph{loud sound}.

\refex{colloc} shows how to identify the most `surprising' collocations using R and Python.
For Python, we use the \pkg{gensim} package which we will also use for topic modeling in \refsec{unsupervised}.
This package has a \cls{Phrases} class which can identify the bigrams in a list of tokens.
In R, we use the \fn{textstat\_collocations} function from \quanteda.
These packages each use a different implementation: \pkg{gensim} uses pointwise mutual information, i.e.
how much information about finding the second word does seeing the first word give you?
\quanteda\ estimates an interaction parameter in a loglinear model.
Nonetheless, both methods give very similar results, with Saddam Hussein, the Iron Curtain, Al Qaida, and red tape topping the list for each.


\begin{ccsexample}
\doublecodex{ch_dtm/colloc}
\codexoutputtable{ch_dtm/colloc.r}
\doublecodex{ch_dtm/colloc2}
\codexoutputtable{ch_dtm/colloc2.r}
\caption{Identifying and applying collocations in the US State of the Union}\label{ex:colloc}
\end{ccsexample}


The next block demonstrates how to use these collocations in further processing.
In R, we filter the collocations list on $lambda>8$ and use the \fn{tokens\_compound} function to compound bigrams from that list.
As you can see in the term frequencies filtered on `hussein', the regular terms (apart from the posessive) are removed and the compounded term now has 26 occurrences.
For Python, we use the \cls{PhraseTransformer} class, which is an adaptation of the \cls{Phrases} class to the \sklearn\ methodology.
After setting a standard threshold of 0.7, we can use \fn{fit\_transform} to change the tokens.
The term statistics again show how the individual terms are now replaced by their compound. 


\subsection{Word Embeddings}

A recent addition to the text analysis toolbox are \concept{word embeddings}.
Although it is beyond the scope of this book to give a full explanation of the algorithms behind word embeddings,
they are relatively easy to understand and use at an intuitive level.

The first core idea behind word embeddings is that the meaning of a word can be expressed using a relatively small \concept{embedding vector}, generally consisting of around 300 numbers which can be interpreted as dimensions of meaning.
The second core idea is that these embedding vectors can be derived by scanning the context of each word in millions and millions of documents.


These embedding vectors can then be used as features or DTM columns for further analysis.
Using embedding vectors instead of word frequencies has the advantages of strongly reducing the dimensionality of the DTM:
instead of (tens of) thousands of columns for each unique word we only need hundreds of columns for the embedding vectors.
This means that further processing can be more efficient as fewer parameters need to be fit,
or conversely that more complicated models can be used without blowing up the parameter space.
Another advantage is that a model can also give a result for words it never saw before, as these words most likely will have an embedding vector and so can be fed into the model.
Finally, since words with similar meanings should have similar vectors,
a model fit on embedding vectors gets a `head start' since the vectors for words like `great' and `fantastic' will already be relatively close to each other, while all columns in a normal DTM are treated independently.

\newcommand{\fnglove}{\footnote{The full embedding models can be downloaded from https://nlp.stanford.edu/projects/glove/. To make the file easier to download, we took only the 10,000 most frequent words of the smallest embeddings file (the 50 dimension version of the 6B tokens model). For serious applications you probably want to download the larger files, in our experience the 300 dimension version usually gives good results. Note that the files on that site are in a slightly different format which lacks the initial header line, so if you want to use other vectors for the examples here you can convert them with the \fn{glove2word2vec} function in the \pkg{gensim} package. For R, you can also simply omit the \verb|skip=1| argument as apart from the header line the formats are identical}}

The assumption that words with similar meanings have similar vectors can also be used directly to extract synonyms.
This can be very useful, for example for (semi-)automatically expanding a dictionary for a concept.
\refex{embedding} shows how to download and use pre-trained embedding vectors to extract synonyms.
First, we download a very small subset of the pre-trained Glove embedding vectors\fnglove,
wrapping the download call in a condition to only download it when needed.

Then, for python, we use the excellent support from the \pkg{gensim} package to load the embeddings into a \cls{KeyedVectors} object.
Although not needed for the rest of the example, we create a pandas DataFrame from the internal embedding values so the internal structure becomes clear: each row is a word, and the columns (in this case 50) are the different (semantic) dimensions that characterize that word according to the embeddings model.
This data frame is sorted on the first dimension, which shows that negative values on that dimension are related to various sports.
Next, we switch back to the \cls{KeyedVectors} object to get the most similar words to the word \emph{fraud}, which is apparently related to similar words like \emph{bribery} and \emph{corruption} but also to words like \emph{charges} and \emph{alleged}.
These similarities are a good way to (semi-)automatically expand a dictionary: start from a small list of words,
find all words that are similar to those words, and if needed manually curate that list.
Finally, we use the embeddings to solve the `analogies' that famously showcase the geometric nature of these vectors:
if you take the vector for \emph{king}, subtract the vector for \emph{man} and add that for \emph{woman},
the closest word to the resulting vector is \emph{queen}.
Amusingly, it turns out that soccer is a female form of football, probably showing the American cultural origin of the source material.

For R, there was less support from existing packages so we decided to use the opportunity to show both the conceptual simplicity of embeddings vectors and the power of matrix manipulation in R.
Thus, we directly read in the word vector file which has a head line and then on each line a word followed by its 50 values.
This is converted to a matrix with the rownames showing the word,
which we normalize to (Euclidean) length of one for each vector for easier processing. 
To determine similarity, we take the cosine distance between the vector representing a word with all other words in the matrix.
As you might remember from algebra, the cosine distance is the dot product between the vectors normalized to have length one
(just like Pearson's product-moment correlation is the dot product between the vectors normalized to z-scores per dimension).
Thus, we can simply multiply the normalized target vector with the normalized matrix to get the similarity scores.
These are then sorted, renamed, and the top values are taken using the basic functions from \refchap{tidyverse}.
Finally, analogies are solved by simply adding and subtracting the vectors as explained above, and then listing the closest words to the resulting vector
(excluding the words in the analogy itself). 

\begin{ccsexample}
\doublecodex{ch_dtm/embeddings0}
\doublecodex{ch_dtm/embeddings1}
\codexoutputtable{ch_dtm/embeddings1.r}
\doublecodex{ch_dtm/embeddings2}
\codexoutputtable{ch_dtm/embeddings2.r}
\doublecodex{ch_dtm/embeddings3}
\codex[caption=Output]{ch_dtm/embeddings3.r.out}
\caption{Using word embeddings for finding similar and analogous words}
\end{ccsexample}





\subsection{Linguistic Preprocessing}
\label{sec:nlp}

A final technique to be discussed here is the use of linguistic preprocessing steps to enrich and filter a DTM.
So far, all techniques discussed here are language independent.
However, there are also many language-specific tools for automatically enriching text developed by computational linguistics communities around the world.
Two techniques will be discussed here as they are relatively widely available for many languages and easy and quick to apply: \concept{Part-of-speech tagging} and \concept{lemmatizing}.

In \concept{part-of-speech tagging} or POS-tagging, each word is enriched with information on its function in the sentence: verb, noun, determiner etc.
For most languages, this can be determined with very high accuracy, although sometimes text can be ambiguous:
in one famous example, the flies in \emph{fruit flies} is generally a noun (fruit flies are a type of fly), but it can also be a verb (if fruit could fly). 
Although there are different sets of POS tags used by different tools, there is broad agreement on the core set of tags listed in \reftab{postags}.

POS tags are useful since it allows to e.g. analyse only the \textit{nouns} if we care about the things that are discussed, only the \textit{verbs} if we care about actions that are described, or only the \textit{adjectives} if we care about the characteristics given to a noun.
Moreover, knowing the POS tag of a word can help disambiguate it.
For example, like as a verb (I like books) is generally positive, but like as a preposition (a day like no other) has no clear sentiment attached.

\concept{Lemmatizing} is a technique for reducing each word to its root or \concept{lemma} (plural: lemmata).
For example, the lemma of the verb \emph{reads} is (to) \emph{read} and the lemma of the noun \emph{books} is \emph{book}.
Lemmatizing is useful since for most of our research question we do not care about these different conjugations of the same word.
By lemmatizing the texts, we do not need to include all conjugations in a dictionary,
and it reduces the dimensionality of the DTM -- and thus also the data scarcity.

Note that lemmatizing is related to a technique called \emph{stemming}, which removes known suffixes (endings) from words.
For example, for English it will remove the `s' from both reads and books.
Stemming is much less sophisticated than lemmatizing, however, and will trip over irregular conjugations
(e.g. \emph{are} as a form of to be) and regular word endings that look like conjugations (e.g. \emph{virus} will be stemmed to \emph{viru}).
English has relatively simple conjugations and stemming can produce adequate results.
For morphologically richer languages such as German or French, however, it is strongly advised to use lemmatizing instead of stemming.
Even for English we would generally advise lemmatization since it is so easy nowadays and will yield better results than stemming.

For \refex{udpipe}, we use the \concept{UDPipe} natural language processing toolkit \citep{udpipe},
a `Pipeline' that parses text into `Universal Dependencies', a representation of the syntactic structure of the text.
For R, we can immediately call the \fn{udpipe} function from the package of the same name.
This parses the given text and returns the result as a data frame with one token (word) per row,
and the various features in the columns.
For Python, we need to take some more steps ourselves.
First, we download the English models if they aren't present.
Second, we load the model and create a pipeline with all default settings,
and use that to parse the same sentence.
Finally, we use the \pkg{conllu} package to read the results into a form that can be turned into a data frame.

In both cases, the resulting tokens clearly show some of the potential advantages of lingusitic processing:
The lemma column shows that it correctly deals with irregular verbs and plural forms.
Looking at the upos (universal Part-of-Speech) column, John is recognized as a proper name (PROPN), brought as a verb, and knives as a noun.
Finally, the \verb|head_token_id| and \verb|dep_rel| columns represent the syntactic information in the sentence:
`Bought' (token 2) is the root of the sentence, and `John' is the subject (nsubj) while `knives' is the object of the buying.

\pyrex[caption=Using UDPipe to analyse a sentence,format=table,output=r]{ch_dtm/udpipe}

The syntactic relations can be useful if you need to differentiate between who is doing something and whom it was done to.
For example, one of the authors of this book used syntactic relations to analyse conflict coverage,
where there is an important difference between attacking and getting attacked \citep{clause}.
However, in most cases you probably don't need this information and analysing dependency graphs is relatively complex.
We would advise you to almost always consider lemmatizing and tagging your texts, as lemmatizing is simply so much better than stemming
(especially for languages other than English), and the Part-of-Speech can be very useful for analysing different aspects of a text. 

If you only need the lemmatizer and tagger, you can speed up processing by setting \verb|udpipe(.., parser='none')| (R) or setting the third argument to Pipeline (the parser) to \verb|Pipeline.NONE| (Python).
\refex{nouncloud} shows how this can be used to extract only the nouns from the most recent state of the union speeches,
create a DTM with these nouns, and then visualize them as a word cloud.
As you can see, these words (such as student, hero, childcare, healthcare, and terrorism), are much more indicative of the topic of a text than the general words used earlier.
In the next chapter we will show how you can further analyse these data, for example by analysing usage patterns per person or over time, or using an unsupervised topic model to cluster words into topics.

\pyrex[caption=Nouns used in the most recent State of the Union addresses,format=png,output=py]{ch_dtm/nouncloud}

Please note that we use UDPipe here, but nowadays there are a number of good and relatively easy to use linguistic toolkits that can be used.
Especially \concept{Spacy} \citep{spacy} and \concept{Stanza} \citep{stanza} are also very good and flexible toolkits with support for multiple (human) languages and good integration especially with Python.
If you want to learn more about natural language processing, the book `Speech and Language Processing' by Jurafsky and Martin is a very good starting point \citep{jurafsky}, with a third edition currently being written.


\section{Which preprocessing to use?}

This chapter has shown how to create a DTM and especially introduced a number of different steps that can be used to clean and preprocess the DTM before analysis.
All of these steps are used by text analysis practitioners and in the relevant literature.
However, no study ever uses all of these steps on top of each other.
This of courses raises the question of how to know which preprocessing steps to use for your research question.

First, there are a number of things that you should (almost) always do.
If your data contains noise such as boilerplate language, HTML artefacts, etc., you should generally strip these out before proceedings.
Second, text almost always has an abundance of uninformative (stop) words and a very long tail of very rare words.
Thus, it is almost always a good idea to use a combination of stop word removal, trimming based on document frequency, and/or tf-idf weighting.
Note that when using a stop word list, you should always manually inspect and/or fine-tune the word list to make sure it matches your domain and research question.

The other steps such as n-grams, collocations, and tagging and lemmatization are more optional but can be quite important depending on the specific research.
For this (and for choosing a specific combination of trimming and weighting), it is always good to know your domain well, look at the results, and think whether you think they make sense.
Using the example given above, bigrams can make more sense for sentiment analysis (since \emph{not good} is quite different from \emph{good}),
but for analysing the topic of texts it may be less important.

Ultimately, however, many of these questions have no good theoretical answer, and the only way to find a good preprocessing `pipeline' for your research question is to try many different
options and see which works best.
This might feel like `cheating' from a social science perspective, since it is generally frowned upon to just test many different statistical models and report on what works best.
There is a difference, however, between substantive statistical modeling where you actually want to understand the mechanisms,
and technical processing steps where you just want the best possible measurement of an underlying variable (presumably to be used in a subsequent substantive model).
\citet{mousetrap} uses the analogy of the mouse trap and the human condition: in engineering you want to make the best possible mouse trap, 
while in social science we want to understand the human condition.
For the mouse trap, it is OK if it is a black box of which we don't understand how it works, as long as we are sure that it works.
For the social science model, this is not the case as it is exactly the inner workings we are interested in.

Technical (pre)processing steps such as reviewed in this chapter are primarily engineering devices:
we don't really care how something like tf-idf works, as long as it produces the best possible measurement of the variables we need for our analysis.
In other words, it is an engineering challenge, not a social science research question.
As a consequence, the key criterion by which to judge these steps is validity, not explainability.
Thus, it is fine to try out different options, as long as you validate the results properly.
If you have many different choices to evaluate against some metric such as performance on a subsequent prediction task,
using split-half or crossvalidation techinques discussed in chapter \refchap{ml} are also relevant here to avoid biasing the evaluation.
