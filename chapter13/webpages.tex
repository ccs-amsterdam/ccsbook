\section{Retrieving and parsing web pages}
\label{sec:webpages}

Unfortunately, not all online services we may be interested in offer
an API -- in fact, it has even been suggested that computational
researchers has arrived in an ``post-API age'' \citep{Freelon2018}, as
API access for researchers has become increasingly restricted.

If data cannot be collected using an API (or a similar service, such
as RSS feeds), we need to restort to web scraping. Before you start a
web scraping project, make sure to ask the appropriate instances for
ethical and legal advice (see also \refsec{ethicallegalpractical}).

Web scraping (sometimes also referred to as harvesting), in essence,
boils down to automatically downloading web pages aimed at a human
audience, and extracting meaningful information out of them. One could
also say that we are reverse-engineering the way the information was
published on the web. For instance, a news site may always use a
specific formatting to denote the title of an article -- and we would
then use this to extract the title. This process is called `parsing',
which in this context is just a fancy term for `extracting meaningful
information'.

When scraping data from the web, we can distinguish two different
tasks: (1) downloading a (possibly large) number of webpages, and (2)
parsing the content of the webpages. Often, both go hand in
hand. For instance, the URL of the next page to be downloaded might
actually be parsed from the content of the current page; or some
overview page may contain the links and thus has to be parsed first in
order to download subsequent pages.

We will first discuss how to parse a single HTML page (say, the page
containing one specific product review, or one specific news article),
and then describe how to ``scale up'' and repeat the process in a
loop (to scrape, let's say, all reviews for the product; or all
articles in a specific time frame).




\subsection{Retrieving and parsing a HTML page}

In order to parse a HTML file, you need to have a basic understanding
of the structure of an HTML file. Open your web browser, visit a
website of your choice (we suggest to use a simple page, such as
\url{http://css-book.net/d/restaurants/index.html}), and
look inspect its underlying HTML code (almost all browsers have a
function called something like ``view source'', which enables you to
do so).

You will see that there are some regular patterns in there. For
example, you may see that each paragraph is enclosed with the tags
|<p>| and |</p>|. Thinking back of \refsec{regular}, you may figure
that you could, for instance, use a regular expression to extract
the text of the first paragraph. In fact, packages like \pkg{beautifulsoup}
under the hood use regular expressions to do exactly that.

But writing your own set of regular expressions to parse a HTML
page usually is not a good idea (but it can be a last resort when
everything else fails). Chances are high that you make
a mistake or do not handle some edge case correctly; and besides,
it would be a bit like re-inventing the wheel. Packages like
\pkg{rvest} (R), \pkg{beautifulsoup}, and \pkg{lxml} (both Python)
already do this for you.

In order to use them, though, you need need to have a basic
understanding of how a HTML page looks like. Here is a simplified
example:

\begin{lstlisting}
<html>
<body>
<h1>This is a title</h1>
<div>
<p> Some text with one <a href=bla.html>link </a> </p>
<img src = plaatje.jpg>an image </img>
</div>
<div>
<p> Some more text </p>
<p> Even more... </p>
<p> And more. </p>
</div>
</body>
</html>
\end{lstlisting}

For now, it is not too important to understand the function of each
specific tag (although it might help, for instance, to realize that
\texttt{a} denotes a link, \texttt{h1} a first-level heading,
\texttt{p} a paragraph and \texttt{div} some kind of section).

What is important, though, is to realize that each tag is opened and
closed (with \texttt{/}). Because tags can be nested, we can actually
draw the code as a tree. In our example, this would look like this:

\dirtree{%
.1 html.
.2 body.
.3 h1.
.3 div.
.4 p.
.5 a.
.4 img.
.3 div.
.4 p.
.4 p.
.4 p.
}


Additionally, tags can have \emph{attributes}. For instance, the
makers of a page with customer reviews may use attributes to specify
to specify what a section contains. For instance, they may write
\texttt{<div class="reviewtext"> ... </div>}  to mark the text of a review (where
|...| is the text), and \texttt{<div class="ratings"> ...</div>} to mark the
scores a product received. This enables them to format them nicely
using a technique called Cascading Style Sheets (CSS). We can exploit
this information to tell our parser where to find the elements we
are interested in. Note that |class| is just one possible attribute
attribute; you may encounter others.

We can use so-called CSS selectors to select the elements we are
interested in.  For instance, to select all elements that have an
attribute \texttt{class} with the value \texttt{ratings}, we can just
write: \texttt{.ratings} (with a dot).  And to select all \texttt{h1}
tags, we can just write: \texttt{h1} (without a dot).

For an overview of all possible CSS selectors, see
\url{https://www.w3schools.com/cssref/css_selectors.asp}.

Sometimes, you cannot express the elements you need using CSS
selectors. For instance, there may be many elements using the same
selectors, and you are only interested in a specific one. In these
cases, you may want to use the XPATH syntax tp specify your elements
of interested.

As we have seen above, we can represent the HTML code of a web page as
a tree (also referred to as the Document Object Model (DOM), or a DOM
tree). We can then describe each location in the page by describing
how to walk through the tree to arrive at the correct position.  For
instance, to describe where to find ``Even more text'' in our example
above, we could say: start at |html|, go to |body|, go to the second |div|,
and take the first |p| element.

If we translate this verbal explanation into a so-called XPATH, it reads:
|/html/body/div[2]/p[1]|

 You can compare the syntax of an XPATH with the path of a given file
 (\texttt{/home/tim/Desktop/slides.pdf}), where the leftmost element
 is the "highest" level, and where after each \texttt{/}, it indicates
 to which lower level one has to turn before finally arriving at the
 file (\texttt{slides.pdf}).

If we leave away the \texttt{[1]} after \texttt{p}, we retrieve a list
of all the three texts within the second div -- which makes it ease
to, for instance, retrieve all reviews from a web page instead of only
one specific review. An asterisk |*| can be used to indicate an
arbitrary number.

Within an XPATH, we can also specify that some specific attribute
needs to have a specific value. For instance, we could modify our
example such that it does not take the second |div| element, but the
|div| element where the attribute |class| has the value |ratings|:

|/html/body/div[@class="restaurant"]/p[1]|

Additionally, we can use two |//| to ``leave out'' part of the tree
and indicate that arbitray steps may be taken here. For instance, one
could say: \texttt{//div[@class="restaurant"]//a}, which means that
you do not care \emph{how} to get from the root to the |div| of class
|restaurant|, and that you want all |a| tags below this tag, even if
several steps need to be taken in between. In this context, one also
talks about ``parents'' and ``children'': the |a| elements are
children (or grandchildren) of the |div| element.


\refex{htmlparse1} shows how to use XPATHs and CSS selectors to parse
a HTML page. To fully understand it, open
\url{http://cssbook.net/d/restaurants/index.html} in a browser and
look at its source code (all modern browsers have a function ``View
page source'' or similar), or -- more comfortable -- right-click on an
element you are interested in (such as a restaurant name) and select
``Inspect element'' or similar. This will give you a user-friendly
view of the HTML code.

\pyrex[output=py,caption={Parsing websites using XPATHs or CSS selectors}]{chapter13/htmlparse1}


Of course, \refex{htmlparse1} only parses one possible element of interest: the restaurant names. Try to retrieve other elements as well!

\begin{feature}\textbf{Do you care about children as well?}
Regardless of whether you use XPATHS or CSS Selectors to specify which part of the page you are interested in, it is often the case that there are other elements within it. Depending on whether you want to also retrieve the text of these elements or not, you have to use different approaches (see \refex{htmlparse2}).
\end{feature}

\pyrex[output=py,caption={Getting the text of an HTML element versus getting the text of the element and its children}]{chapter13/htmlparse2}



\begin{feature}\textbf{Pretending to be a specific browser.}  When \pkg{lxml}, \pkg{rvest}, or your web browser download a HTML page, they send a so-called HTTP request. This request contains the URL, but also some meta-data, such as a so-called user-agent string. This string specifies the name and version of the browser. Some sites may block specific user agents (such as, for instance, the ones \pkg{lxml} pr \pkg{rvest} use); and sometimes, they deliver different content for different browser. By using a more powerful module for downloading the HTML code (such as \pkg{requests} or \pkg{httr}) before parsing it, you can specify your own user-agent string and thus pretend to be a specific browser. If you do a web search, you will quickly find long lists with popular strings. In \refex{htmlparse1useragent}, we rewrote \refex{htmlparse1} such that a custom user-agent can be specified. 
\end{feature}

\pyrex[output=py,caption={Specifying a user agent to pretend to be a
    specific browser}]{chapter13/htmlparse1useragent}




\subsection{Crawling websites}

Once we have mastered parsing a single HTML page, it is time to scale
up. Only rarely are we interested in parsing a single page. In most
cases, we want to use a HTML page as a starting point, parse it,
follow a link to some other interesting page, parse it as well, and so
on. There are some dedicated frameworks for this such as \pkg{scrapy},
but in our experience, it may be more of a burden to learn that
framework than to just implement your crawler yourself.

Staying with the example of a restaurant review website, we might be
interested retrieving all restaurants from a specific city, and for
all of these restaurants, all available reviews.

Our approach, thus, could look as follows:

\begin{enumerate}
	\item Retrieve the overview page.
	\item Parse the names of the hotels and the corresponding links.
	\item Loop over all the links, retrieve the corresponding pages.
	\item On each of these pages, parse the interesting content (i.e., the reviews, ratings, and so on).
\end{enumerate}

So, what if there are multiple overview pages (or multiple pages with
reviews)? Basically, there are two possibilities: The first
possibility is to look for the link to the next page, parse it,
download the next page, and so on.  The second possibility exploits
the fact that often, URLs are very systematic: For instance, the first
page of restaurants might have a URL such as
\url{http://myreviewsite.com/amsterdam/restaurants.html?page=1}.  If this
is the case, we can simply construct a list with all possible URLs
(\refex{createurls})

\pyrex[output=py,caption={Specifying a user agent to pretend to be a
    specific browser}]{chapter13/createurls}

Afterwards, we would just loop over this list and retrieve all the
pages (a bit like how we approached \refex{googleapi3} in \refsec{apis}).

However, often, things are not as straightforward, and we need to find
the correct links on a page that we have been parsing -- that's why we
\emph{crawl} through the website.

Writing a good crawler can take some time, and they will look very
differently for different pages. The best advise is to build them up
step-by-step. Carefully inspect the website you are interested in.
Take a sheet of paper, draw its structure, and try to find out which
pages you need to parse, and how you can get from one page to the next.
Also think about how the data that you want to extract should be organized.

We will illustrate this process using our mockup review website \url{http://cssbook.net/d/restaurants/}.
First, have a look at the site and try to understand its structure.

You will see that it has an overview page, |index.html|, with the
names of all restaurants and, per restaurant, a link to a page with reviews.
Click on these links, and note your observations, such as:
\begin{itemize}
\item the pages have different numbers of reviews;
\item each review consists of a author name, a review text, and a rating;
\item some, but not all, pages have a link saying ``Get older reviews''
\item \ldots
\end{itemize}

In \refex{crawling}, we implemented a scraper that crawls and parses the
website.
\pyrex[output=py,input=py,caption={Crawling a website}]{chapter13/crawling}
\todo[inline]{Does it actually make sense to translate this to R? It for sure is possible, but recursive functions and generators are not exactly popular in enduser-R I believe... (right?)}

First, we need to get a list of all restaurants and the links to their reviews. That's what is done in the function |get_restaurants|. This is actually the first thing we do (see line 34--35).

We now want to loop over these links and retrieve the reviews.  We
decided to use a \emph{generator} \todo{WE NEED TO EXPLAIN WHAT A
  GENERATOR IS WHERE WE EXPLAIN FUNCTIONS}: instead of writing a
function that collects \emph{all} reviews in a list first, we let the
function return each review immediately -- and immediately append it
to a file. This has a big advantage: if our scraper fails (for
instance, due to a time out, a block, or a programming error), then we
already saved the reviews we got so far.

We loop over the links to the restaurants (line 38) and call the
function |get_reviews| (line 40). Each review it returns (the review
is a dict) gets the name of the restaurant as an extra key, and then
gets written to a file which contains one JSON-object per line (also
known as a jsonlines-file).

The function |get_reviews| takes a link to a review page as input and
yields reviews. If we knew all pages with reviews already, then we would not need lines 27--31. However, as we have seen, some review pages contain a link to older reviews. An inspection of the HTML code shows that these links have a |span| tag with the attribute |class="backbutton"|. We therefore check if such a button exists (line 27), and if so, we get its "href" attribute (i.e., the link itself), and then \emph{recursively call the \texttt{get\_reviews} function itself on this URL}, to also yield the reviews that can be found on that page. This goes on as long as such a link is not found any more.

\todo[inline]{Say sth about R}
\todo[inline]{Say sth about ``how do we know this?''}
\todo[inline]{Say sth about how do to it yourself (write functions separately etc)}


\subsection{Dynamic web pages}
\label{sec:selenium}

You may have realized that all our scraping efforts until now proceeded in two steps: we retrieved (downloaded) the HTML source of a web page and then parsed it. However, modern websites more and more are dynamic instead of static. For example, after being loaded, they load additional content, or what is displayed changes based on what the user does. Frequently, some JavaScript is run within the user's browser to do that. However, we do not have a browser here. The HTML code we downloaded may contain some instructions for the browser that some code needs to be run, but in the absence of a browser, our Python or R script cannot do this.

A first test to check out whether this is a concern, you can simply check whether the HTML code in your browser is the same that you would get if you downloaded it with R or Python.

\todo[inline]{Add example with retrieve/ save to file/ compare to browser}


If this test indeed shows that the data you are interested in is indeed not part of the HTML code you can retrieve with R or Python, use the following checklist to find 

\begin{enumerate}
\item Does using a different user-agent string (see above) solve the issue?
\item Is the issue due to some cookie that needs to be accepted (see below)?
\item STH ELSE???
\end{enumerate}

If all of this does not help, or if you already know for sure that the content you are interested in is dynamically fetched via JavaScript or similar, you can use \pkg{selenium} to literally start a browser and extract the content you are interested in from there. Selenium has been designed for testing web sites and allows you to automate clicks etc. in a browser window, and also supports CSS selectors and XPATHS to specify parts of the web page.

ADD E=

selenium/RSelenium


headless browser
