\section{Retrieving and parsing web pages}
\label{sec:webpages}

Unfortunately, not all online services we may be interested in offer
an API -- in fact, it has even been suggested that computational
researchers has arrived in an ``post-API age'' \citep{Freelon2018}, as
API access for researchers has become increasingly restricted.

If data cannot be collected using an API (or a similar service, such
as RSS feeds), we need to resort to web scraping. Before you start a
web scraping project, make sure to ask the appropriate instances for
ethical and legal advice (see also \refsec{ethicallegalpractical}).

Web scraping (sometimes also referred to as harvesting), in essence,
boils down to automatically downloading web pages aimed at a human
audience, and extracting meaningful information out of them. One could
also say that we are reverse-engineering the way the information was
published on the web. For instance, a news site may always use a
specific formatting to denote the title of an article -- and we would
then use this to extract the title. This process is called `parsing',
which in this context is just a fancy term for `extracting meaningful
information'.

When scraping data from the web, we can distinguish two different
tasks: (1) downloading a (possibly large) number of webpages, and (2)
parsing the content of the webpages. Often, both go hand in
hand. For instance, the URL of the next page to be downloaded might
actually be parsed from the content of the current page; or some
overview page may contain the links and thus has to be parsed first in
order to download subsequent pages.

We will first discuss how to parse a single HTML page (say, the page
containing one specific product review, or one specific news article),
and then describe how to ``scale up'' and repeat the process in a
loop (to scrape, let's say, all reviews for the product; or all
articles in a specific time frame).




\subsection{Retrieving and parsing a HTML page}
\label{sec:parsehtml}
In order to parse a HTML file, you need to have a basic understanding
of the structure of an HTML file. Open your web browser, visit a
website of your choice (we suggest to use a simple page, such as
\url{http://css-book.net/d/restaurants/index.html}), and
look inspect its underlying HTML code (almost all browsers have a
function called something like ``view source'', which enables you to
do so).

You will see that there are some regular patterns in there. For
example, you may see that each paragraph is enclosed with the tags
|<p>| and |</p>|. Thinking back of \refsec{regular}, you may figure
that you could, for instance, use a regular expression to extract
the text of the first paragraph. In fact, packages like \pkg{beautifulsoup}
under the hood use regular expressions to do exactly that.

But writing your own set of regular expressions to parse a HTML
page usually is not a good idea (but it can be a last resort when
everything else fails). Chances are high that you make
a mistake or do not handle some edge case correctly; and besides,
it would be a bit like re-inventing the wheel. Packages like
\pkg{rvest} (R), \pkg{beautifulsoup}, and \pkg{lxml} (both Python)
already do this for you.

In order to use them, though, you need need to have a basic
understanding of how a HTML page looks like. Here is a simplified
example:

\begin{lstlisting}
<html>
<body>
<h1>This is a title</h1>
<div>
<p> Some text with one <a href=bla.html>link </a> </p>
<img src = plaatje.jpg>an image </img>
</div>
<div>
<p> Some more text </p>
<p> Even more... </p>
<p> And more. </p>
</div>
</body>
</html>
\end{lstlisting}

For now, it is not too important to understand the function of each
specific tag (although it might help, for instance, to realize that
\texttt{a} denotes a link, \texttt{h1} a first-level heading,
\texttt{p} a paragraph and \texttt{div} some kind of section).

What is important, though, is to realize that each tag is opened and
closed (with \texttt{/}). Because tags can be nested, we can actually
draw the code as a tree. In our example, this would look like this:

\dirtree{%
.1 html.
.2 body.
.3 h1.
.3 div.
.4 p.
.5 a.
.4 img.
.3 div.
.4 p.
.4 p.
.4 p.
}


Additionally, tags can have \emph{attributes}. For instance, the
makers of a page with customer reviews may use attributes to specify
what a section contains. For instance, they may write
\texttt{<div class="reviewtext"> ... </div>}  to mark the text of a review (where
|...| is the text), and \texttt{<div class="ratings"> ...</div>} to mark the
scores a product received. This enables them to format them nicely
using a technique called Cascading Style Sheets (CSS). We can exploit
this information to tell our parser where to find the elements we
are interested in. Note that |class| is just one possible attribute
attribute; you may encounter others.

We can use so-called CSS selectors to select the elements we are
interested in.  For instance, to select all elements that have an
attribute \texttt{class} with the value \texttt{ratings}, we can just
write: \texttt{.ratings} (with a dot).  And to select all \texttt{h1}
tags, we can just write: \texttt{h1} (without a dot).
For an overview of all possible CSS selectors, see
\url{https://www.w3schools.com/cssref/css_selectors.asp}.

Sometimes, you cannot express the elements you need using CSS
selectors. For instance, there may be many elements using the same
selectors, and you are only interested in a specific one. In these
cases, you may want to use the XPATH syntax tp specify your elements
of interested.

As we have seen above, we can represent the HTML code of a web page as
a tree (also referred to as the Document Object Model (DOM), or a DOM
tree). We can then describe each location in the page by describing
how to walk through the tree to arrive at the correct position.  For
instance, to describe where to find ``Even more text'' in our example
above, we could say: start at |html|, go to |body|, go to the second |div|,
and take the first |p| element.

If we translate this verbal explanation into a so-called XPATH, it reads:
|/html/body/div[2]/p[1]|

 You can compare the syntax of an XPATH with the path of a given file
 (\texttt{/home/tim/Desktop/slides.pdf}), where the leftmost element
 is the "highest" level, and where after each \texttt{/}, it indicates
 to which lower level one has to turn before finally arriving at the
 file (\texttt{slides.pdf}).

If we leave away the \texttt{[1]} after \texttt{p}, we retrieve a list
of all the three texts within the second div -- which makes it easy
to, for instance, retrieve all reviews from a web page instead of only
one specific review. An asterisk |*| can be used to indicate an
arbitrary number.

Within an XPATH, we can also specify that some specific attribute
needs to have a specific value. For instance, we could modify our
example such that it does not take the second |div| element, but the
|div| element where the attribute |class| has the value |ratings|:

|/html/body/div[@class="ratings"]/p[1]|

Additionally, we can use two |//| to ``leave out'' part of the tree
and indicate that arbitray steps may be taken here. For instance, one
could say: \texttt{//div[@class="restaurant"]//a}, which means that
you do not care \emph{how} to get from the root to the |div| of class
|restaurant|, and that you want all |a| tags below this tag, even if
several steps need to be taken in between. In this context, one also
talks about ``parents'' and ``children'': the |a| elements are
children (or grandchildren) of the |div| element.


\refex{htmlparse1} shows how to use XPATHs and CSS selectors to parse
a HTML page. To fully understand it, open
\url{http://cssbook.net/d/restaurants/index.html} in a browser and
look at its source code (all modern browsers have a function ``View
page source'' or similar), or -- more comfortable -- right-click on an
element you are interested in (such as a restaurant name) and select
``Inspect element'' or similar. This will give you a user-friendly
view of the HTML code.

\pyrex[output=py,caption={Parsing websites using XPATHs or CSS selectors}]{chapter13/htmlparse1}


Of course, \refex{htmlparse1} only parses one possible element of interest: the restaurant names. Try to retrieve other elements as well!

\begin{feature}\textbf{Do you care about children?}
Regardless of whether you use XPATHS or CSS Selectors to specify which part of the page you are interested in, it is often the case that there are other elements within it. Depending on whether you want to also retrieve the text of these elements or not, you have to use different approaches (see \refex{htmlparse2}).
\end{feature}

\pyrex[output=py,caption={Getting the text of an HTML element versus getting the text of the element and its children}]{chapter13/htmlparse2}


Notably, you may want to parse links. In HTML, links use a specific tag, |a|. These tags have an attribute, |href|, which contains the link itself. \refex{htmlparse3} shows how, after selecting the |a| tags, we can access these attributes. 

\pyrex[output=py,caption={Parsing link texts and links}]{chapter13/htmlparse3}



\begin{feature}\textbf{Pretending to be a specific browser.}  When \pkg{lxml}, \pkg{rvest}, or your web browser download a HTML page, they send a so-called HTTP request. This request contains the URL, but also some meta-data, such as a so-called user-agent string. This string specifies the name and version of the browser. Some sites may block specific user agents (such as, for instance, the ones \pkg{lxml} or \pkg{rvest} use); and sometimes, they deliver different content for different browsers. By using a more powerful module for downloading the HTML code (such as \pkg{requests} or \pkg{httr}) before parsing it, you can specify your own user-agent string and thus pretend to be a specific browser. If you do a web search, you will quickly find long lists with popular strings. In \refex{htmlparse1useragent}, we rewrote \refex{htmlparse1} such that a custom user-agent can be specified. 
\end{feature}

\pyrex[output=py,caption={Specifying a user agent to pretend to be a
    specific browser}]{chapter13/htmlparse1useragent}




\subsection{Crawling websites}
\label{sec:crawling}

Once we have mastered parsing a single HTML page, it is time to scale
up. Only rarely are we interested in parsing a single page. In most
cases, we want to use a HTML page as a starting point, parse it,
follow a link to some other interesting page, parse it as well, and so
on. There are some dedicated frameworks for this such as \pkg{scrapy},
but in our experience, it may be more of a burden to learn that
framework than to just implement your crawler yourself.

Staying with the example of a restaurant review website, we might be
interested retrieving all restaurants from a specific city, and for
all of these restaurants, all available reviews.

Our approach, thus, could look as follows:

\begin{enumerate}
	\item Retrieve the overview page.
	\item Parse the names of the hotels and the corresponding links.
	\item Loop over all the links, retrieve the corresponding pages.
	\item On each of these pages, parse the interesting content (i.e., the reviews, ratings, and so on).
\end{enumerate}

So, what if there are multiple overview pages (or multiple pages with
reviews)? Basically, there are two possibilities: The first
possibility is to look for the link to the next page, parse it,
download the next page, and so on.  The second possibility exploits
the fact that often, URLs are very systematic: For instance, the first
page of restaurants might have a URL such as
\url{http://myreviewsite.com/amsterdam/restaurants.html?page=1}.  If this
is the case, we can simply construct a list with all possible URLs
(\refex{createurls})

\pyrex[output=py,caption={Generating a list of URLs that follow the same pattern.}]{chapter13/createurls}

Afterwards, we would just loop over this list and retrieve all the
pages (a bit like how we approached \refex{googleapi3} in \refsec{apis}).

However, often, things are not as straightforward, and we need to find
the correct links on a page that we have been parsing -- that's why we
\emph{crawl} through the website.

Writing a good crawler can take some time, and they will look very
differently for different pages. The best advice is to build them up
step-by-step. Carefully inspect the website you are interested in.
Take a sheet of paper, draw its structure, and try to find out which
pages you need to parse, and how you can get from one page to the next.
Also think about how the data that you want to extract should be organized.

We will illustrate this process using our mockup review website \url{http://cssbook.net/d/restaurants/}.
First, have a look at the site and try to understand its structure.

You will see that it has an overview page, |index.html|, with the
names of all restaurants and, per restaurant, a link to a page with reviews.
Click on these links, and note your observations, such as:
\begin{itemize}
\item the pages have different numbers of reviews;
\item each review consists of a author name, a review text, and a rating;
\item some, but not all, pages have a link saying ``Get older reviews''
\item \ldots
\end{itemize}

If you combine what you just learned about extracting text and links from HTML
pages with your knowledge about control structures like loops and conditional
statements (\refsec{controlstructures}), you can now write your own crawler.

Writing a scraper is a craft, and there are several ways of achieving your goal.
You probably want to develop your scraper in steps: first write a function to
parse the overview page, then a function to parse the review pages, then try
to combine all elements into one script. Before you read on, try to write
such a scraper.

To show you one possible solution, we implemented a scraper in Python
that crawls and parses all reviews for all restaurants
(\refex{crawling}), which we describe in detail below.

\pyrex[output=py,input=py,caption={Crawling a website}]{chapter13/crawling}

First, we need to get a list of all restaurants and the links to their
reviews. That's what is done in the function |get_restaurants|. This
is actually the first thing we do (see line 34--35).

We now want to loop over these links and retrieve the reviews.  We
decided to use a \emph{generator} (\refsec{controlstructures}): instead of writing a
function that collects \emph{all} reviews in a list first, we let the
function yield each review immediately -- and immediately append that review
to a file. This has a big advantage: if our scraper fails (for
instance, due to a time out, a block, or a programming error), then we
already saved the reviews we got so far.

We loop over the links to the restaurants (line 38) and call the
function |get_reviews| (line 40). Each review it returns (the review
is a dict) gets the name of the restaurant as an extra key, and then
gets written to a file which contains one JSON-object per line (also
known as a jsonlines-file).

The function |get_reviews| takes a link to a review page as input and
yields reviews. If we knew all pages with reviews already, then we
would not need the while loop statement in line 16 and the lines
28--33. However, as we have seen, some review pages contain a link to
older reviews. We therefore use a loop that runs forever (that is what
|while True:| does), \emph{unless} it encounters a |break| statement
(line 33).  An inspection of the HTML code shows that these links have
a |span| tag with the attribute |class="backbutton"|. We therefore
check if such a button exists (line 27), and if so, we get its |href|
attribute (i.e., the link itself), overwrite the |url| variable with
it, and then go back to line 16, the beginning of the loop, so that we
can download and parse this next URL.  This goes on as long as such a
link is not found any more.



\subsection{Dynamic web pages}
\label{sec:selenium}

You may have realized that all our scraping efforts until now
proceeded in two steps: we retrieved (downloaded) the HTML source of a
web page and then parsed it. However, modern websites more and more
are dynamic instead of static. For example, after being loaded, they
load additional content, or what is displayed changes based on what
the user does. Frequently, some JavaScript is run within the user's
browser to do that. However, we do not have a browser here. The HTML
code we downloaded may contain some instructions for the browser that
some code needs to be run, but in the absence of a browser, our Python
or R script cannot do this.

A first test to check out whether this is a concern, you can simply
check whether the HTML code in your browser is the same that you would
get if you downloaded it with R or Python.  After having retrieved the
page (\refex{htmlparse1useragent}), you simply dump it to a file
(\refex{htmltofile}) and open this file in your browser to verify that
you indeed downloaded what you intended to download (and not, for
instance, a login page, a cookie wall, or an error message).

\pyrex[output=none,caption={Dumping the HTML source to a file}]{chapter13/htmltofile}


If this test indeed shows that the data you are interested in is
indeed not part of the HTML code you can retrieve with R or Python,
use the following checklist to find

\begin{enumerate}
\item Does using a different user-agent string (see above) solve the issue?
\item Is the issue due to some cookie that needs to be accepted or you need to log in (see below)?
\item Is a different page delivered for different browsers, devices, display settings, etc.?
\end{enumerate}

If all of this does not help, or if you already know for sure that the
content you are interested in is dynamically fetched via JavaScript or
similar, you can use \pkg{selenium} to literally start a browser and
extract the content you are interested in from there. Selenium has
been designed for testing web sites and allows you to automate clicks
etc. in a browser window, and also supports CSS selectors and XPATHS
to specify parts of the web page.

Using selenium may require some additional setup on your computer,
which may depend on your operating system and software versions you
are using -- check out the usual online sources for guidance if
needed.  It is possible to use Selenium through R using
\pkg{Rselenium}. However, doing so can be quite a hassle and requires,
for instance, running a separate Selenium server, for instance using
Docker. If you opt to use Selenium for web scraping, your safest bet
is probably to follow an online tutorial and/or to dive into the
documentation. To give you a first impression of the general working,
\refex{selenium} shows you how to (at the time of writing of this
book) open Firefox, surf to Google, google for Tintin by entering that
string and pressing the Return key, click on the first link containing
that string, and take a screenshot of the result.

\pyrex[output=none,input=py,caption={Using Selemium to literally open a browser, input text, click on a link, and take a screenshot.}]{chapter13/selenium}


\begin{feature}\textbf{Loosing your head}
If you want to run long-lasting scraping processes using Selenium on
the background (or on a server without a graphical user interface),
you may want to look into what is called a ``headless'' browser. For
instance, Selenium can start Firefox in ``headless'' mode, which means
that it will run without making any connection to a graphical
interface. Of course, that also means that you cannot watch Selenium
scrape, which may make debugging more difficult. You could opt for
developing your scraper first using a normal browser, and then
changing it to use a headless browser once everything works.
\end{feature}
