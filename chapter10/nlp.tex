\section{Basic natural language processing}
\label{sec:nlp}


This section outlines how to go a step further and use natural language processing (NLP) to, for instance, parsing sentences, stemming or lemmatization. Using such part-of-speech tagging (POS), we can for example only retain nouns and adjectives, or any other element of interest. Similarly, we can use named entity recognition (NER) to extract names of persons, organizations, or locations.

NLP basic techniques will help you to obtain a better definition of the \textit{textual elements} you are going to work with in simple and sophisticated analysis. Keep in mind that after cleaning text from undesirable noise, you will have to decide which units (stems, lemmas, words, phrases, sentences, entities, etc.) will be meaningful in your research in order to maintain only those that better represent the content you are analysing. Even if there is not a rule of thumb of how to proceed to get these units (there might be different combinations of techniques that can result useful in a given problem), the truth is that you will normally be forced to conduct some NLP before you build any mathematical representation of text, as we will introduce in Chapter~\ref{chap:dtm}). You will also apply more advanced NLP techniques in Chapter~\ref{chap:text}, but we will focus here in those that can help you during the stage of preprocessing text.

When applying NLP to text, the first thing you have to do is to define the \textit{unit of analysis} of your content in order to be able to assign values or to transform those units. Technically we call this this process \texttt{tokenization} and is a way to \texttt{parse} the text by creating tokens that will represent the content. By parsing a given string into sections you can use any quantitative approach in your analysis, from counting the frequency of each token to represent the text in a matrix. However, there are different criteria of how to tokenize your text and you must decide which one to use depending on your own needs. You may consider that a \texttt{word} will be the basic unit of the text, which means that you will tokenize by splitting the string using a blank space as a delimiter; or you can also may find useful to tokenize by \texttt{sentences} (delimited by a dot or some punctuation mark) or even by \texttt{paragraphs} (separated by a line space). Thus, the usual tokenizer is for words, but it can also be used for sentences or paragraphs, or even for characters, n-grams, lines or any separation decided by a regular expression pattern.

You could conduct this process of tokenization, as well as other basic NLP tasks, using base code, but it will be time and effort consuming. In contrast you have plenty of libraries to deploy many NLP tasks in R (\pkg{Text2Vec}, \pkg{Tidytext}, \pkg{Stringr} or \pkg{Quanteda}), Python  (\pkg{NLTK}, \pkg{Gensim}, \pkg{polyglot}, \pkg{TextBlob}, \pkg{PyNLPl} or \pkg{Quepy}) or both languages (\pkg{UDPipe} or \pkg{SpaCy}). Things are moving fast in text mining, which means that you must check which package is more convenient for you in each moment. For the sake of simplicity we will show you some initial NLP in \pkg{Quanteda} (R) and \pkg{NLTK} (Python), but will quickly move to \pkg{SpaCyr} (R) and \pkg{SpaCy} (Python), which we recommend nowadays for a full pipeline in computational text analysis.

In \refex{tokens} you can see how we tokenize a simple string (\texttt{tweets}) by words and then load a pre-defined list of stopwords to exclude them from our text. In the case of R,  \pkg{Quanteda} creates a special object \texttt{tokens} . Differently, \pkg{NLTK} in Python creates a \texttt{list} in which each element represents a token.  In both cases we load a default list of stopwords and use it to filter our tokens in a new object called \texttt{filtered\_tokens}. However, if you compare both results, you will notice that the final number of tokens is different, and this is due to the initial tokenization (\pkg{NLTK} included all strings separated by a blank space, but \pkg{Quanteda} does not separate the @ symbol from strings) and to the default list of stopwords (179 in \pkg{NLTK} and 175 in \pkg{Quanteda}). With this introductory example you can notice how different libraries will process text differently, which would suggest that using one library instead of combining several might facilitate a journey that normally begins in preprocessing text and can end in more advanced tasks such as supervised supervised sentiment analysis or topic modeling.

\pyrex[output=both,caption=Tokenizing and removing stopwords]{chapter10/tokens}	

Now, even if you consider that words are great units of analysis in NLP, you might have thought that extracting proper knowledge from them might be tricky since different words could be linked to the same idea or two identical words could have different meanings. Well, you are right! NLP is sometimes challenging because languages are complex and some times you need to have a more accurate description of your text that goes beyond simple word splitting. Think for instance that the plural form of a word (i.e. "buildings") might have the very same meaning than its singular form (i.e. "building") for the purpose of your research, and you might not want to count these two words as different units. Moreover, in order to retain the main meaning behind many words (i.e. "build", "builds", "building", "buildings", "built", etc.) you might want to catch only one string that summarizes them (i.e. "build"). Still, the same word could mean different things depending on its function of the text! (i.e. "build" as a \textit{verb} referring to construct, and "build" as a \textit{noun} referring to body shape), which make relevant to know the grammatical and syntactic structure of your text. On the other hand, you can also think of words or group of words that should always refer to the same unit or \textit{entity}, such in the case of a person like Barack Obama: you will find the word "Obama" many times referring to him, but in fact that word could mean "Michelle Obama", any person with the same last name or even the city Obama in Fukui (Japan). Well, NLP will help you with all these nuances and casuistries. 

One of the basic techniques of NLP is stemming, which consists of obtaining the base or root form of a word (also called \textit{word stem}) in order to simplify our analysis. Thus we can transform any inflected or derived word into a stem, which is the part of the word that never change (i.e. "build" is the stem of "building" or even of "builds"; and "awesom" of "awesome"). In \refex{stem} we show how to implement the popular stemmer Snowball with \pkg{Quanteda} in R and \pkg{NLTK} in Python.

\pyrex[output=both,caption=Stemming a text with Snowball Stemmer]{chapter10/stem}	

Now, let us me move into the package \pkg{SpaCy} in Python (and its wrapper for R \pkg{SpaCyr}, which is well integrated with \pkg{Quanteda} and \pkg{tidytext}). This library does not include a stemmer but has many other advantages such as the use of pretrained language models in several languages that help in most NLP tasks. These models summarize many observations to previous annotated examples in order to learn from them, and thus can have a better performance when applying NLP to your text. In order to work in this environment the first thing you have to do is to install and load the model you want to work with (i.e. \verb|en_core_web_sm| for English) and then convert your initial string to a new \pkg{spacy} object (\verb|doc|), as shown in \refex{nlp}.

\pyrex[output=both,caption=Creating a spacy object to apply NLP techniques]{chapter10/nlp}	

Having a \pkg{spacy} object created with the pretrained language model means that the package will break the string into tokens (units with semantic value) with some useful attributes for NLP, such as part-of-speech tagging (POS), which is the basis to conduct lemmatization, sentence parsing or name entity recognition. The above explained process of stemming did not depend on the types of the word, but if you want to deploy more sophisticated text analytics you will need to know the lexical categories of your words. 


\note{There are different types of words (nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions or interjections). You can find universal POS tags and English POS tags, as well as dependency labels and NER nomenclatures in the web of Spacy (https://spacy.io/api/annotation/).}


For example, the fact that the model determines if a given word (i.e. "build") is a noun (\texttt{NOUN} in universal POS and \texttt{NN} in English POS) or a verb (\texttt{VERB} in universal POS and \texttt{VB} in English POS) will allow you to get the \textit{lemmas} of your words. Lemmatization is similar to stemming, but instead of getting the root stem it will get the root word. This root word or lemma (\verb|token.lemma_|) is always a proper word (lexicographically correct) that can be found in a dictionary. In the case of the word "awesome", the lemma will be "awesome" (and not "awesom" as in the case of stemming). Moreover, the lemma depends on identifying the POS and meaning of a word in the text. For instance, the word "buildings" would produce different lemmas if it is tagged as a noun ("building"), adjective ("buildings") or a verb ("build"). \refex{poslemma} shows how to get the POS tag (universal and English) and the lemma of two specific tokens of our object \texttt{doc} using the above mention attributes.

\pyrex[output=both,caption=Creating a Spacy object and detect POS and lemmas]{chapter10/poslemma}	

Similar to POS tagging, you can analyse your sentence structure. We can mention three levels of sentence parsing: chunking, constituency and dependency. Firstly, chunking, shallow parsing or light parsing refers to \texttt{extract phrases} on your text, this is: noun phrases, verb phrases or prepositional phrases.  In other words, we split a sentence into non-overlapping syntactic phrases. In \refex{nounchunks} we use the attribute \verb|noun_chunks| to get the noun chunks of our \pkg{Spacy} object \texttt{doc}.

\pyrex[output=both,caption=Creating a spacy object to apply NLP techniques]{chapter10/nounchunks}	

The second level corresponds to constituency parsing, which uses the phrase structure grammar to obtain a constituency-based parse \textit{tree} from any given sentence, In other words, we get a representation (the constituency tree) of the syntactic structure of the sentence, which might be helpful to solve structural ambiguities. \pkg{Spacy} does not include a proper constituency parser, but you can use the standard CoreNLPParser included in \pkg{NLTK} in Python to get the tree shown in \refex{tree}

\pyrex[input=py,output=py,caption=Constituency tree of a sentence with CoreNLPParser (NLTK) in Python]{chapter10/tree}

Finally, a third level is called dependency parsing. Here we try to find \textit{dependencies} between words and their types, this is, to get the relationships between the tokens included in our sentence. There is one \textit{root word} (usually a verb) with no dependency and the rest of the words are directly or indirectly linked to the initial root, creating several links. Every link between words refers to the specific relationship between then, such as \texttt{nsubj} (nominal subject), \texttt{acomp} (adjectival complement), \texttt{amod} (adjectival modifier) or \texttt{poss} (possession modifier). In \refex{dependency} you can observe this analysis for the sentence "My second favorite color is green" using the attribute \verb|token.dep_|.

\pyrex[output=both,caption=Dependency parsing of a sentence]{chapter10/dependency}

You can notice that the verb "is" (to be) is the root word and the adjective "green" is its adjectival complement. Similarly,  "My" is the possession modifier of "color". In order to have a better picture of this dependency parsing you can use the \texttt{displacy} visualizer in Python as in \refex{visdependency}.

\pyrex[input=py,output=py,format=html,caption=Visualizing a dependency parsing of a sentence]{chapter10/visdependency}

A more advanced way to process natural language is by extracting \texttt{entities}, which are more complex units than words or phrases. This is known as named entity recognition (NER) and may include persons, organizations or locations, but also events, dates, laws, products, nationalities or monetary values, among others. You might manually create the rules to detect any of these entities, but a well trained language model can help you in this task because it has seen thousands of annotated texts and has examined not only the words belonging to each entity but also their syntactic relationships. \pkg{Spacy} has models trained over different corpus and you can easily identify entities in your content using the attribute \verb|ents|. Imagine the headline "Madrid will host Olympic Games in 2032, Pedro Sanchez announced". As you can see in \refex{ner}, is mentions a city (\texttt{GPE}), an organization (\texttt{ORG}), a specific year (\texttt{date}) and a person  (\texttt{PERSON}).  Country and year contains only one word, and the organization and the person have two words, but in all cases some internal rules must be applied in order to recognize, for example, that "Madrid" is a city (and not al last name) or that "Olympic Games" is an organization (and not two independent words).

\pyrex[output=both,caption=Named entity recognition over a headline]{chapter10/ner}

You can visualize tags over the same text using \texttt{displacy}  in Python, as shown in \refex{visner}.

\pyrex[input=py,output=py,format=html,caption=Visualizing NER]{chapter10/visner}