\chapter{Text as data}
\label{chap:dtm}

\begin{abstract}{Abstract}
  This chapter shows how you can analyze texts that are stored as a data frame column or variable using functions from the package \index{quanteda}\emph{quanteda} in R and the package \index{sklearn}\emph{sklearn} in Python and R.
  Please see Chapter~\ref{chap:protext} for more information on reading and cleaning text.

\end{abstract}

\keywords{Text as Data, Document-Term Matrix}

\begin{objectives}
\item Create a document-term matrix from text
\item Perform  document and feature selection and weighting
\item Understand and use more advanced representations such as n-grams and embeddings
\end{objectives}


\newpage
\begin{feature}
  This chapter introduces the packages \index{quanteda}\emph{quanteda} (R) and \index{sklearn}\emph{sklearn} and \index{nltk}\emph{nltk} (Python) for converting text into a document-term matrix. It also introduces the \index{udpipe}\emph{udpipe} package for natural language processing.
You can install these packages with the code below if needed  (see Section~\ref{sec:installing} for more details):

\doublecodex{chapter10/chapter10install}

\noindent After installing, you need to import (activate) the packages every session:

\doublecodex{chapter10/chapter10library}

\end{feature}

%\input{chapter10/dtm}
\section{The Bag of Words and the Term-Document Matrix}
\label{sec:dtm}

Before you can conduct any computational analysis of text, you need to solve a problem: computations are usually done on numerical data -- but you have text. Hence, you must find a way to \emph{represent} the text by numbers.
The document-term matrix (DTM, also called the term-document matrix or TDM) is such a common numerical representation of text.
It represents a \index{corpus}\emph{corpus} (or set of documents) as a matrix or table, where each row represents a document, each column represents a term (word),
and the numbers in each cell show how often that word occurs in that document.

\pyrex[caption=Example document-term matrix,output=r,format=table]{chapter10/dtm}

As an example, Example~\ref{ex:dtm} shows a DTM made from two lines from the famous poem by Mary Angelou.
The resulting matrix has two rows, one for each line; and 11 columns, one for each unique term (word).
In the columns you see the document frequencies of each term: the word ``bird'' occurs once in each line,
but the word ``with'' occurs only in the first line (text1) and not in the second (text2).

In R, you can use the \index{dfm}\texttt{dfm} function from the \index{quanteda}\emph{quanteda} package \citep{quanteda}.
This function can take a vector or column of texts and transforms it directly into a DTM
(which quanteda actually calls a document-\emph{feature} matrix, hence the function name \index{dfm}\texttt{dfm}).
In Python, you achieve the same by creating an object of the \index{CountVectorizer}\texttt{CountVectorizer} class, which has a \index{fit\_transform}\texttt{fit\_transform} function.


\subsection{Tokenization}

In order to turn a corpus into a matrix, each text needs to be \index{tokenized}\emph{tokenized},
meaning that it must be split into a list (vector) of words.
This seems trivial, as English (and most western) text generally uses spaces to demarcate words.
However, even for English there are a number of edge cases.
For example, should ``haven't'' be seen as a single word, or two?

\pyrex[caption=Differences between tokenizers]{chapter10/tokenize}

Example~\ref{ex:tokenize} shows how Python and R deal with the sentence ``I haven't seen John's derring-do''.
For Python, we first use |CountVectorizer.build_tokenizer| to access the built-in tokenizer.
As you can see in the first line of input, this tokenizes ``haven't'' to |haven|,
which of course has a radically different meaning. Moreover, it silently drops all single-letter words,
including the |'t|, |'s|, and |I|.

In the box ``Tokenizing in Python'' below, we therefore discuss some alternatives. For instance, the \index{TreebankWordTokenizer}\texttt{TreebankWordTokenizer} included in the \index{nltk}\emph{nltk} package is a more reasonable tokenizer and
splits ``haven't'' into |have| and |n't|, which is a reasonable outcome.
Unfortunately, this tokenizer assumes that text has already been split into sentences,
and it also includes punctuation as tokens by default.
To circumvent this, we can introduce a custom tokenizer based on the Treebank tokenizer,
which splits text into sentences (using \index{nltk.sent\_tokenize}\texttt{nltk.sent\_tokenize}) -- see the box for more details.

For R, we simply call the \index{tokens}\texttt{tokens} function from the \index{quanteda}\emph{quanteda}\ package.
This keeps |haven't| and |John's| as a single word, which is probably less desirable than splitting the words
but at least better than outputting the word |haven|.

As this simple example shows, even a relatively simple sentence is tokenized differently by the tokenizers considered here (and see the box on tokenization in Python).
Depending on the research question, these differences might or might not be important.
However, it is always a good idea to check the output of this (and other) preprocessing steps so you understand
what information is kept or discarded.

\begin{figure}
\begin{feature}\textbf{Tokenization in Python}\\
  As you can see in the example, the built-in tokenizer in \sklearn\ is not actually very good.
  For example, \emph{haven't} is tokenized to \emph{haven}, which is an entirely different word.
  Fortunately, there are other tokenizers in the \index{nltk.tokenize}\emph{nltk.tokenize} package that do better.

  For example, the \index{TreebankTokenizer}\texttt{TreebankTokenizer} uses the tokenization rules for the Penn Treebank
  to tokenize, which produces better results:

  \codex[caption=The nltk.tokenize.TreebankTokenizer]{chapter10/tokenizealt.py}

  Another example is the \index{WhitespaceTokenizer}\texttt{WhitespaceTokenizer}, which simply uses whitespace to tokenize,
  which can be useful if your input has already been tokenized,
  and is used in Example~\ref{ex:tagcloud} below for tweets to conserve hash tags.

  \codex[caption=The nltk.tokenize.WhitespaceTokenizer]{chapter10/tokenizealt1.py}

  You can also write your own tokenizer if needed.
  For example, the \index{TreebankTokenizer}\texttt{TreebankTokenizer} assumes that text has already been split into sentences
  (which is why the period is attached to the word \emph{derring-do.}).
  The code below shows how we can make our own tokenizer class,
  which uses \index{nltk.sent\_tokenize}\texttt{nltk.sent\_tokenize} to first split the text into sentences,
  and then uses the \index{TreebankTokenizer}\texttt{TreebankTokenizer} to tokenize each sentence,
  keeping only tokens that include at least one letter character.
  Although a bit more complicated, this approach can give you maximum flexibility.

  \codex[caption=Writing a custom Tokenizer]{chapter10/tokenizealt2.py}
\end{feature}
\end{figure}

% Listings won't work with the japanese output, so I manually created png. Sorry!
\begin{ccsexample}
  \doublecodex{chapter10/haiku}
  \begin{tcbraster}[raster columns=2,raster equal height=rows,raster valign=top]
  \begin{tcolorbox}[title=Python Output]
      \includegraphics[width=\linewidth]{{chapter10/haiku.py}.png}
  \end{tcolorbox}%
  \begin{tcolorbox}[title=R Output]
      \includegraphics[width=\linewidth]{{chapter10/haiku.r}.png}
  \end{tcolorbox}%
\end{tcbraster}
  \caption{Tokenization of Japanese verse.}\label{ex:haiku}
\end{ccsexample}



Note that for languages such as Chinese, Japanese, and Korean, which do not use spaces to delimit words, the story is more difficult.
Although a full treatment is beyond the scope of this book, Example~\ref{ex:haiku} shows a small example of tokenizing Japanese text,
in this case the famous haiku ``the sound of water'' by Bash\={o}.
The default tokenizer in quanteda actually does a good job, in contrast to the default Python tokenizer
that simply keeps the whole string as one word
(which makes sense since this tokenizer only looks for whitespace or punctuation).
For Python the best bet is to use a custom package for tokenizing Japanese, such as the \index{nagisa}\emph{nagisa} package.
This package contains a tokenizer which is able to tokenize the Japanese text, and we could use this in the \index{CountVectorizer}\texttt{CountVectorizer}
much like we used the \index{TreebankWordTokenizer}\texttt{TreebankWordTokenizer} for English earlier.
Similarly, with heavily inflected languages such as Hungarian or Arabic,
it might be better to use preprocessing tools developed specifically for these languages, but treating those is
beyond the scope of this book.


\subsection{The DTM as a Sparse Matrix}

\pyrex[caption=Example document-term matrix]{chapter10/sotu}

Example~\ref{ex:sotu} shows a more realistic example.
It downloads all US ``State of the Union'' speeches and creates a document-term matrix from them.
Since the matrix is now easily too large to print, both Python and R simply list the size of the matrix.
R lists $85$ documents (rows) and $17\,999$ features (columns), and Python reports that it's size is $85\times17\,185$.
Note the difference in the number of columns (unique terms) due to the differences in tokenization as discussed above.

\begin{ccsexample}
  \doublecodex{chapter10/freq}
    \doubleoutput{chapter10/freq}
    \doublecodex{chapter10/freq2}
      \doubleoutput{chapter10/freq}
  \caption{A look inside the DTM.}\label{ex:freq}
\end{ccsexample}


In Example~\ref{ex:freq} we show how you can look at the content of the DTM. First, we show the overall term and document frequencies of each word, where we showcase words at different frequencies. Unsurprisingly, the word \emph{the} tops both charts, but further down there are minor differences.
In all cases, the highly frequent words are mostly functional words like \emph{them} or \emph{first}. More informative words such as \emph{investments} are by their nature used much less often.
Such term statistics are very useful to check for noise in the data and get a feeling of the kind of language that is used.
Second, we take a look at the frequency of these same words in four speeches from Truman to Obama. All use words like \emph{the} and \emph{first}, but none of them talk about \emph{defrauded} -- which is not surprising, since it was only used once in all the speeches in the corpus.

However, the words that ranked around 1000 in the top frequency are still used in less than half of the documents.
Since there are about 17\,000 even less frequent words in the corpus, you can imagine that most of the document-term matrix consists of zeros.
The output also noted this \index{sparsity}\emph{sparsity} in the first output above.
In fact, R reports that the dtm is $91\%$ sparse, meaning 91\% percent of all entries is zero.
Python reports a similar figure, namely that there are only just under 150\,000 non-zero entries
out of a possible $8\times22\,219$, which boils down to a 92\% sparse matrix.

Note that to display the matrix we turned it from a \index{sparse matrix}\emph{sparse matrix} representation into a \index{dense matrix}\emph{dense matrix}.
Briefly put, in a dense matrix, all entries are stored as a long list of numbers, including all the zeros.
In a sparse matrix,  only the non-zero entries and their location are stored.
This conversion (using the function \verb|as.matrix| and the method \verb|todense| respectively), however, was only performed after selecting a small subset of the data.
In general,  it is very inefficient to store and work with the matrix in a \concept[dense matrix]{dense} format.
For a reasonably large corpus with tens of thousands of documents and different words, this can quickly run to billions of numbers,
which can cause problems even on modern computers and is, moreover, very inefficient.
With sparsities of often above 99\%, using a sparse matrix representation can easily reduce storage requirements by  a hundred times and in the process speed up calculations by reducing the number of entries that need to be inspected.
Both \index{quanteda}\emph{quanteda}\ and \sklearn\ store DTMs as sparse matrices by default,
and most analysis tools are able to deal with sparse matrices very efficiently
(but see Section~\ref{sec:workflow} for problems with machine learning on sparse matrices in R).

A final note on the difference between Python and R in this example.
The code in R is much simpler and produces nicer results since it also shows the words and the speech names.
In Python, we wrote our own helper function to create the frequency statistics which is built into the R \index{quanteda}\emph{quanteda}\ package.
These differences between Python and R reflect a pattern that is true in many (but not all) cases:
in Python libraries such as \numpy\ and \sklearn\ are setup to maximize performance,
while in R a library such as \index{quanteda}\emph{quanteda}\ or \index{tidyverse}\emph{tidyverse}\ is more geared towards ease of use.
For that reason, the DTM in Python does not ``remember'' the actual words, it uses the index of each word,
so it consumes less memory if you don't need to use the actual words in e.g.\ a machine learning setup.
R, on the other hand, stores the words and also the document IDs and metadata in the DFM object.
This is easier to use if you need to look up a word or document, but it consumes (slightly) more memory.


%\begin{feature}
%  \textbf{R: Why is it a document-feature matrix?}
%The package \index{quanteda}\emph{quanteda} uses the term document-feature matrix because the columns can also contain
%other information instead of terms, such as word pairs, and these are collectively called \emph{features} of the text.
%\end{feature}

\begin{feature}
\noindent\textbf{Python: Why fit\_transform?}
In Python, you don't have a function that directly transforms text into a DTM.
Instead, you create an \emph{transformer} called a CountVectorizer,
which can then be used to ``vectorize'' texts (turn it into a row of numbers)
by counting how often each word occurs.
This uses the \index{fit\_transform}\texttt{fit\_transform} function which is offered by all \sklearn\ transformers.
It ``fits'' the model on the training data, which in this case means learning the vocabulary.
It can then be used to transform other data into a DTM with the exact same columns,
which is often required for algorithms.
Because the feature names (the words themselves) are stored in the CountVectorizer
rather than the document-term matrix, you generally need to keep both objects.
\end{feature}

\subsection{The DTM as a ``Bag of Words''}


As you can see already in these simple examples, the document-term matrix discards quite a lot of information from text.
Specifically, it disregards the order or words in a text: ``John fired Mary'' and ``Mary fired John'' both result in the same DTM,
even though the meaning of the sentences is quite different.
For this reason, a DTM is often called a \index{bag of words}\emph{bag of words}, in the sense that all words in the document are simply put in a big bag
without looking at the sentences or context of these words.

Thus, the DTM can be said to be a specific and ``lossy'' representation of the text, that turns out to be quite useful for certain tasks:
the frequent occurrence of words like ``employment'', ``great'', or ``I'' might well be good indicators that a text is about the economy,
is positive, or contains personal expressions respectively.
As we will see in the next chapter, the DTM representation can be used for many different text analyses, from dictionaries to supervised and unsupervised machine learning.

Sometimes, however, you need information that is encoded in the order of words.
For example, in analyzing conflict coverage it might be quite important to know who attacks whom, not just that an attack took place.
In the Section~\ref{sec:ngram} we will look at some ways to create a richer matrix-representation by using word pairs.
Although it is beyond the scope of this book,
you can also use automatic syntactic analysis to take grammatical relations into account as well.
As is always the case with automatic analyses, it is important to understand what information the computer is looking at,
as the computer cannot find patterns in information that it doesn't have.

\subsection{The (Unavoidable) Word Cloud}

 One of the most famous text visualizations is without doubt the word cloud.
Essentially, a word cloud is an image where each word is displayed in a size that is representative of its frequency.
Depending on preference, word position and color can be random, depending on word frequency, or in a decorative shape.

Word clouds are often criticized since they are (sometimes) pretty but mostly not very informative.
The core reason for that is that only a single aspect of the words is visualized (frequency),
and simple word frequency is often not that informative: the most frequent words are generally uninformative ``stop words'' like ``the'' and ``I''.

For example, Example~\ref{ex:wordcloud} shows the word cloud for the state of the union speeches downloaded above.
In R, this is done using the \index{quanteda}\emph{quanteda}\ function \index{textplot\_wordcloud}\texttt{textplot\_wordcloud}.
In Python we need to work a little harder, since it only has the counts, not the actual words.
So, we sum the DTM columns to get the frequency of each word, and combine that with the feature names (words)
from the |CountVectorized| object |cv|. Then we can create the word cloud and give it the frequencies to use.
Finally, we plot the cloud and remove the axes.

\pyrex[caption=Word cloud of the US State of the Union corpus,format=png]{chapter10/wordcloud}

The results from Python and R look different at first -- for one thing, R is nice and round but Python has more colors!
However, if you look at the cloud you can see both are not very meaningful: the largest words are all punctuation or words like
``a'', ``and'', or ``the''.
You have to look closely to find words like ``federal'' or ``security'' that give a hint on what the texts were actually about.



%\input{chapter10/dtmselect}

\section{Weighting and Selecting Documents and Terms}
\label{sec:dtmselect}

So far, the DTMs you made in this chapter simply show the count of each word in each document.
Many words, however, are not informative for many questions.
This is especially apparent if you look at a \index{word cloud}\emph{word cloud},
essentially a plot of the most frequent words in a  \index{corpus}\emph{corpus} (set of documents).

\note{
  \textbf{Vectors and a geometric interpretation of document-term matrices}
  We said that a document is represented by a ``vector'' of numbers, where each number (for a document-term matrix)
  is the frequency of a specific word in that document. This term is also seen in the name for the tokenizer \sklearn:
  a \index{vectorizer}\emph{vectorizer} or function to turn texts into vectors. \\%
  %
  The term \index{vector}\emph{vector} here can be read as just a fancy word for a group of numbers.
  In this meaning, the term is also often used in R, where a column of a data frame is called a vector,
  and where functions that can be called on a whole vector at once are called \emph{vectorized}. \\%
  %
  More generally, however, a vector in geometry is a point (or line from the origin) in an $n$-dimensional space,
  where $n$ is the length or dimensionality of the vector.
  This is also a very useful interpretation for vectors in text analysis:
  the dimensionality of the space is the number of unique words (columns) in the document-term matrix,
  and each document is a point in that $n$-dimensional space.\\%
  %
  In that interpretation, various geometric distances between documents can be calculated as an indicator for how similar
  two documents are. Techniques that reduce the number of columns in the matrix (such as clustering or topic modeling)
  can then be seen as dimensionality reduction techniques since they turn the DTM into a matrix with lower dimensionality
  (while hopefully retaining as much of the relevant information as possible).
  }\vspace{1em}

More formally, a document-term matrix can be seen as a representation of data points about documents:
each document (row) is represented as a vector containing any count per word (column).
Although it is a simplification compared to the original text,
an unfiltered document-term matrix contains all relevant information.
For example, if a president uses the word ``terrorism'' more often than the word ``economy'', that could be an indication of their policy priorities.

However, there is also a lot of \emph{noise} crowding out this \emph{signal}:
as seen in the word cloud in the previous section the most frequent words are generally quite uninformative.
The same holds for words that hardly occur in any document (but still require a column to be represented)
and noisy ``words'' such as punctuation or technical artifacts like HTML code.

This section will discuss a number of techniques for cleaning a corpus or document-term matrix in order to minimize the amount of noise: removing stop words, cleaning punctuation and other artifacts, and trimming and weighting.
As a running example in this section, we will use a collection of tweets from US president Donald Trump.
Example~\ref{ex:trumptweets} shows how to load these tweets into a data frame containing the ID and text of the tweets.
As you can see, this dataset contains a lot of non-textual features such as hyperlinks and hash tags as well as regular punctuation and stop words.
Before we can start analyzing this data, we need to decide on and perform multiple cleaning steps such as detailed below.

\pyrex[caption=Top words used in Trump Tweets,format=table,output=py]{chapter10/trumptweets}

Please note that although tweets are perhaps overused as a source of scientific information,
we use them here as an example since they nicely exemplify issues around non-textual elements such as hyperlinks.
See Chapter~\ref{chap:scraping} for information on how to use the Twitter and other APIs to collect your own data.

\subsection{Removing stopwords}

A first step in cleaning a DTM is often \index{stop word removal}\emph{stop word removal}.
Words such as ``a'' and ``the'' are often called stop words, i.e.\ words that do not tell us much about the content.
Both \index{quanteda}\emph{quanteda}\ and \sklearn\ include built-in lists of stop words, making it very easy to remove the most common words.
Example~\ref{ex:stopwords} shows the result of specifying ``English'' stop words to be removed for both packages.

\pyrex[caption=Simple stop word removal,format=png,output=r]{chapter10/stopwords}



Note, however, that it might seem easy to list words like ``a'' and ``and'',
but as it turns out there is no single well-defined list of stop words,
and (as always) the best choice depends on your data and your research question.

Linguistically, stop words are generally function words or closed word classes such as determiner or pronoun,
with closed classes meaning that while you can coin new nouns, you can't simply invent  new determiners or prepositions.
However, there are many different stop word lists around which make different choices and are compatible with
different kinds of preprocessing.
The Python word cloud in Example~\ref{ex:stopwords} shows a nice example of the importance of matching stopwords with the used
tokenization: a central ``word'' in the cloud is the contraction \emph{'s}.
We are using the NLTK tokenizer, which splits \emph{'s} from the word it was attached to, but the \sklearn\ stop word list
does not include that term.
So, it is important to make sure that the words created by the tokenization match the way that words appear in the stop word list.

As an example of the substantive choices inherent in using a stop word lists,
consider the word ``will''.
As an auxiliary verb, this is probably indeed a stop word: for most substantive questions, there is no difference
whether you will do something or simply do it.
However, ``will'' can also be a noun (a testament) and a name (e.g.\ Will Smith).
Simply dropping such words from the corpus can be problematic; see Section~\ref{sec:nlp} for ways of telling nouns and verbs apart
for more fine-grained filtering.

Moreover, some research questions might actually be interested in certain stop words.
If you are interesting in references to the future or specific modalities,
the word might actually be a key indicator.
Similarly, if you are studying self-expression on Internet forums, social identity theory, or populist rhetoric,
words like ``I'', ``us'' and ``them'' can actually be very informative.

For this reason, it is always a good idea to understand and inspect what stop word list you are using,
and use a different one or customize it as needed \citep[see also][]{nothman18}.
Example~\ref{ex:stopwords2} shows how you can inspect and customize stop word lists.
For more details on which lists are available and what choices these lists make,
see the package documentation for the \index{stopwords}\emph{stopwords} package in Python (part of NLTK) and R (part of quanteda)


\pyrex[caption=Inspecting and Customizing stop word lists]{chapter10/stopwords2}

%J. Nothman, H. Qin and R. Yurchak (2018). “Stop Word Lists in Free Open-source Software Packages”. In Proc. Workshop for NLP Open Source Software.

\subsection{Removing Punctuation and Noise}


Next to stop words, text often contains punctuation and other things that can be considered ``noise'' for most research questions.
For example, it could contain emoticons or emoji, twitter hashtags or at-mentions, or HTML tags or other annotations.

In both Python and R, we can use regular expressions to remove (parts of) words.
As explained above in Section~\ref{sec:regular}, regular expressions are a powerful way to specify (sequences of) characters which are to be kept or removed.
You can use this, for example, to remove things like punctuation, emoji, or HTML tags.
This can be done either before or after tokenizing (splitting the text into words):
in other words, we can clean the raw texts or the individual words (tokens).

In general, if you only want to keep or remove certain words, it is often easiest to do so after tokenization
using a regular expression to select the words to keep or remove.
If you want to remove parts of words (e.g.\ to remove the leading ``\#'' in hashtags) it is easiest to do that before tokenization,
that is, as a preprocessing step before the tokenization.
Similarly, if you want to remove a term that would be split by the tokenization (such as hyperlinks),
if can be better to remove them before the tokenization occurs.

Example~\ref{ex:noise} shows how we can use regular expressions to remove noise in Python and R.
To more clearly show the results of each step, it shows the result of each processing step on a single tweet that exemplifies many of the problems described above.
To better understand the tokenization process, we print the tokens in that tweet separated by a vertical bar (\verb+|+).
As a first cleaning step, we will use a regular expression to remove hyperlinks and HTML entities like \verb|&amp;| from the untokenized texts.
Since both hyperlinks and HTML entities are split over multiple tokens, it would be hard to remove them after tokenization.

\begin{ccsexample}
\doublecodex{chapter10/noise1}
\codex[caption=Output]{chapter10/noise1.r.out}
\doublecodex{chapter10/noise2}
\codex[caption=Output]{chapter10/noise2.r.out}
\doublecodex{chapter10/noise3}
\codex[caption=Output]{chapter10/noise3.r.out}
\caption{Cleaning a single tweet at the text and token level}\label{ex:noise}
\end{ccsexample}

Regular expressions are explained fully in Section~\ref{sec:regular}, so we will keep the explanation short:
the bar \verb#|# splits the pattern in two parts, i.e.\ it will match if it finds either of the subpatterns.
The first pattern looks for the literal text \verb#http#, followed by an optional \verb#s# and the sequence \verb#://#.
Then, it takes all non-whitespace characters it finds, i.e.\ the pattern ends at the next whitespace or end of the text.
The second pattern looks for an ampersand (\verb#&#) followed by one or more letters (\verb#\\w+#), followed by a semicolon (\verb#;#).
This matches HTML escapes like \verb#&amp;# for an ampersand.

In the next step, we process the tokenized text to remove every token that is either a stopword or does not start with a letter.
In Python, this is done by using a list comprehension (\verb#[process(item) for item in list]#) for tokenizing each document; and a nested list comprehension for filtering each token in each document.
In R this is not needed as the \index{tokens\_*}\texttt{tokens\_*} functions are \index{vectorized}\emph{vectorized}, that is, they directly run over all the tokens.

Comparing R and Python, we see that the different tokenization functions mean that \verb|#trump| is removed in R (since it is a token that does not start with a letter),
but in Python the tokenization splits the \verb|#| from the name and the resulting token \verb|trump| is kept.
If we would have used a different tokenizer for Python (e.g.\ the \index{WhitespaceTokenizer}\texttt{WhitespaceTokenizer}) this would have been different again.
This underscores the importance of inspecting and understanding the results of the specific tokenizer used,
and to make sure that subsequent steps match these tokenization choices.
Concretely, with the \index{TreebankWordtokenizer}\texttt{TreebankWordtokenizer} we would have had to also remove hashtags at the text level rather than the token level.

\pyrex[caption=Cleaning the whole corpus and making a tag cloud,output=r,format=png]{chapter10/tagcloud}

As a final example, Example~\ref{ex:tagcloud} shows how to filter tokens for the whole corpus, but rather than removing hashtags it keeps only the hash tags to produce a tag cloud.
In R, this is mostly a pipeline of \index{quanteda}\emph{quanteda}\ functions to create the corpus, tokenize, keep only hashtags, and create a dfm.
To spice up the output we use the \index{RColorBrewer}\emph{RColorBrewer} package to set random colors for the tags.
In Python, you can see that we now have a nested list comprehension, where the outer loop iterates over the texts and the inner loop iterates over the tokens in each text.
Next, we make a \verb|do_nothing| function for the vectorizer since the results are already tokenized.
Note that we need to disable lowercasing as otherwise it will try to call \verb|.lower()| on the token lists.

\note{\textbf{\index{lambda}\texttt{lambda} functions in Python.} Sometimes, we need to define a function that is very simple and that we need only once. An example for such a throwaway function is \texttt{\small{do\_nothing}} in Example~\ref{ex:tagcloud}. Instead of defining a reusable function with the \index{def}\texttt{def} keyword and then to call it by its name when we need it later, we can therefore also directly define an unnamed function when we need it with the \index{lambda}\texttt{lambda} keyword. The syntax is simple: \texttt{\small{lambda argument: returnvalue}}. A function that maps a value onto itself can therefore be written as \texttt{\small{lambda x: x}}. In Example~\ref{ex:tagcloud}, instead of defining a named function, we could therefore also simply write \texttt{\small{cv = CountVectorizer(tokenizer=lambda x: x, lowercase=False)}}. The advantages are that it saves you two lines of code here and  you don't clutter your environment with functions you do not intend to re-use anyway. The disadvantage is that it may be less clear what is happening, at least for people not familiar with lambda functions.}




\subsection{Trimming a DTM}

The techniques above both drop terms from the DTM based on specific choices or patterns.
It can also be beneficial to trim a DTM by removing words that occur very infrequently or overly frequently.
For the former, the reason is that if a word only occurs in a very small percentage of documents it is unlikely to be very informative.
Overly frequent words, for example occurring in more than half or 75\% of all documents, function basically like stopwords for this corpus.
In many cases, this can be a result of the selection strategy. If we select all tweets containing ``Trump'', the word Trump itself is no longer informative about their content.
It can also be that some words are used as standard phrases, for example ``fellow Americans'' in state of the union speeches.
If every president in the corpus uses those terms, they are no longer informative about differences between presidents.

\pyrex[caption=Trimming a Document-Term Matrix]{chapter10/trimming}

Example~\ref{ex:trimming} shows how you can use the \index{relative document frequency}\emph{relative document frequency} to trim a DTM in Python and R.
We keep only words with a document frequency of between 1\% and 75\%.

Although these are reasonable numbers every choice depends on the corpus and the research question, so it can be a good idea to check which words are dropped.
%Example~\ref{ex:docfreq} shows how you can make a table of all word frequencies in the corpus, and filter for excluded words.
%If you see words that are seemingly relevant to your research question in the top of this list,
%if can be a good idea to tweak the selection criteria.
Note that dropping words that occur almost never should normally not influence the results that much, since those words do not occur anyway.
However, trimming a DTM to e.g.\ at least 1\% document frequency often radically reduces the number of words (columns) in the DTM.
Since many algorithms have to assign weights or parameters to each word, this can provide a significant improvement in computing speed or memory use.

\subsection{Weighting a DTM}

The DTMs created above all use the raw frequencies as cell values.
It can also be useful to weight the words so more informative words have a higher weight than less informative ones.
A common technique for this is \index{tf$\cdot$idf}\emph{tf$\cdot$idf} weighting.
This stands for \emph{term frequency $\cdot$ inverse document frequency} and weights each occurrence by its raw frequency (term frequency) corrected for how often it occurs in all documents (inverse document frequency). In a formula, the most common implementation of this weight is given as follows:

$tf\cdot idf(t,d)=tf(t,d)\cdot idf(t)=f_{t,d}\cdot -\log \frac{n_t}{N}$

Where $f_{t,d}$ is the frequency of term $t$ in document $d$, $N$ is the total number of documents, and $n_t$ is the number of documents in which term $t$ occurs. In other words, the term frequency is weighted by the negative log of the fraction of documents in which that term occurs. Since $\log(1)$ is zero, terms that occur in every document are disregarded, and in general the less frequent a term is, the higher the weight will be.


\pyrex[caption=Tf$\cdot$Idf weighting,output=r,format=table]{chapter10/tfidf}

tf$\cdot$idf weighting is a fairly common technique and can improve the results of subsequent analyses such as supervised machine learning.
As such, it is no surprise that it is easy to apply this in both Python and R, as shown in Example~\ref{ex:tfidf}.
This example uses the same data as Example~\ref{ex:sotu} above, so you can compare the resulting weighted values with the results reported there.
As you can see, the tf$\cdot$idf weighting in both languages have roughly the same effect:
very frequent terms such as \emph{the} are made less important compared to less frequent words such as \emph{submit}.
For example, in the raw frequencies for the 1965 Johnson speech, \emph{the} occurred 355 times compared to \emph{submit} only once.
In the weighted matrix, the weight for \emph{submit} is four times as low as the weight for \emph{the}.

There are two more things to note if you compare the examples from R and Python.
First, to make the two cases somewhat comparable we have to use two options for R, namely to set the term frequency to proportional (\verb|scheme_tf='prop'|),
and to add smoothing to the document frequencies (\verb|smooth=1|).
Without those options, the counts for the first columns would all be zero (since they occur in all documents, and $\log \frac{85}{85}=0$),
and the other counts would be greater than one since they would only be weighted, not normalized.

Even with those options the results are still different (in details if not in proportions),
mainly because R normalizes the frequencies before weighting, while Python normalizes after the weighting.
Moreover, Python by default uses L2 normalization, meaning that the length of the document vectors will be one,
while R uses L1 normalization, that is, the row sums are one (before weighting).
Both R and Python have various parameters to control these choices which are explained in their respective help pages.
However, although the differences in absolute values look large, the relative effect of making more frequent terms less important is the same,
and the specific weighting scheme and options will probably not matter that much for the final results.
However, it is always good to be aware of the specific options available and try out which work best for your specific research question.




%\input{chapter10/ngram}

\section{Advanced Representation of Text}
\label{sec:ngram}

The examples above all created document-term matrices where each column actually represents a word.
There is more information in a text, however, than pure word counts.
The phrases: \emph{the movie was not good, it was in fact quite bad} and \emph{the movie was not bad, in fact it was quite good}
have exactly the same word frequencies, but are quite different in meaning.
Similarly, \emph{the new residents of York} and \emph{the residents of New York} are talking about quite different people.

Of course, in the end which aspect of the meaning of a text is important depends on your research question:
if you want to know the sentiment about the movie, it is important to take a word like ``not'' into account;
but if you are interested in the topic or genre of the review, or the extremity of the language used, this might not be relevant.

The core idea of this section is that in many cases this information can be captured in a DTM by having the columns represent different information than just words, for example word combinations or groups of related words.
This is often called \index{feature engineering}\emph{feature engineering}, as we are using our domain expertise to find the right features (columns, independent variables) to capture the relevant meaning for our research question.
If we are using other columns than words it is also technically more correct to use the name \index{document-feature matrix}\emph{document-feature matrix}, as \index{quanteda}\emph{quanteda}\ does, but we will stick to the most common name here and simply continue using the name DTM.

\subsection{$n$-grams}
The first feature we will discuss are n-grams.
The simplest case is a bigram (or 2-gram), where each feature is a pair of adjacent words.
The example used above, \emph{the movie was not bad}, will yield the following bigrams: \emph{the-movie}, \emph{movie-was}, \emph{was-not}, and \emph{not-bad}.
Each of those bigrams is then treated as a feature, that is, a DTM would contain one column for each word pair.

As you can see in this example, we can now see the difference between \emph{not-bad} and \emph{not-good}.
The downside of using n-grams is that there are many more unique word pairs than unique words,
so the resulting DTM will have many more columns.
Moreover, there is a bigger \index{data scarcity problem}\emph{data scarcity problem}, as each of those pairs will be less frequent,
making it more difficult to find sufficient examples of each to generalize over.

Although bigrams are the most frequent use case, trigrams (3-grams) and (rarely) higher-order n-grams can also be used.
As you can imagine, this will create even bigger DTMs and worse data scarcity problems,
so require even more attention paid to feature selection and/or trimming.


\pyrex[caption=Generating n-grams]{chapter10/ngram}

Example~\ref{ex:ngram} shows how n-grams can be created and used in Python and R.
In Python, you can pass the \verb|ngram=(n, m)| option to the vectorizer,
while R has a \verb|tokens_ngrams(n:m)| function.
Both will post-process the tokens to create all n-grams in the range of n to m.
In this example, we are asking for unigrams (i.e., the words themselves), bigrams and trigrams of a simple example sentence.
Both languages produce the same output, with R separating the words with an underscore while Python uses a simple space.

\pyrex[caption=Words and bigrams containing ``government'',output=r,format=table]{chapter10/ngram2}

Example~\ref{ex:ngram2} shows how you can generate n-grams for a whole corpus.
In this case, we create a DTM of the state of the union matrix with all bigrams included.
A glance at the frequency table for all words containing \emph{government} shows that,
besides the word itself and it's plural and possessive forms, the bigrams include compound words (federal and local government),
phrases with  the government as subject (the government can and must), and nouns for which the government is an adjective
(government spending and government programs).

You can imagine that including all these words as features will add many possibilities for analysis of the DTM
which would not be possible in a normal bag-of-words approach.
The terms local and federal government can be quite important to understand policy positions,
but for e.g.\ sentiment analysis a bigram like \emph{not good} would also be insightful
(but make sure ``not'' is not on your stop word list!).

\subsection{Collocations}

A special case of n-grams are collocations.
In the strict corpus linguistic sense of the word, collocations are pairs of words that occur more frequently than expected
based on their underlying occurrence.
For example, the phrase \emph{crystal clear} presumably occurs much more often than would be expected by chance given
how often \emph{crystal} and \emph{clear} occur separately.
Collocations are important for text analysis since they often have a specific meaning,
for example because they refer to names such as \emph{New York} or disambiguate a term like \emph{sound} in \emph{sound asleep},
a \emph{sound proposal}, or \emph{loud sound}.

Example~\ref{ex:colloc} shows how to identify the most ``surprising'' collocations using R and Python.
For Python, we use the \index{gensim}\emph{gensim} package which we will also use for topic modeling in Section~\ref{sec:unsupervised}.
This package has a \index{Phrases}\texttt{Phrases} class which can identify the bigrams in a list of tokens.
In R, we use the \index{textstat\_collocations}\texttt{textstat\_collocations} function from \index{quanteda}\emph{quanteda}.
These packages each use a different implementation: \index{gensim}\emph{gensim} uses pointwise mutual information, i.e.\
how much information about finding the second word does seeing the first word give you?
Quanteda estimates an interaction parameter in a loglinear model.
Nonetheless, both methods give very similar results, with Saddam Hussein, the Iron Curtain, Al Qaida, and red tape topping the list for each.


\begin{ccsexample}
\doublecodex{chapter10/colloc}
\codexoutputtable{chapter10/colloc.r}
\doublecodex{chapter10/colloc2}
\codexoutputtable{chapter10/colloc2.r}
\caption{Identifying and applying collocations in the US State of the Union.}\label{ex:colloc}
\end{ccsexample}


The next block demonstrates how to use these collocations in further processing.
In R, we filter the collocations list on $lambda>8$ and use the \index{tokens\_compound}\texttt{tokens\_compound} function to compound bigrams from that list.
As you can see in the term frequencies filtered on ``Hussein'', the regular terms (apart from the possessive) are removed and the compounded term now has 26 occurrences.
For Python, we use the \index{PhraseTransformer}\texttt{PhraseTransformer} class, which is an adaptation of the \index{Phrases}\texttt{Phrases} class to the \sklearn\ methodology.
After setting a standard threshold of 0.7, we can use \index{fit\_transform}\texttt{fit\_transform} to change the tokens.
The term statistics again show how the individual terms are now replaced by their compound.


\subsection{Word Embeddings}
\label{sec:wordembeddings}
A recent addition to the text analysis toolbox are \index{word embeddings}\emph{word embeddings}.
Although it is beyond the scope of this book to give a full explanation of the algorithms behind word embeddings,
they are relatively easy to understand and use at an intuitive level.

The first core idea behind word embeddings is that the meaning of a word can be expressed using a relatively small \index{embedding vector}\emph{embedding vector}, generally consisting of around 300 numbers which can be interpreted as dimensions of meaning.
The second core idea is that these embedding vectors can be derived by scanning the context of each word in millions and millions of documents.


These embedding vectors can then be used as features or DTM columns for further analysis.
Using embedding vectors instead of word frequencies has the advantages of strongly reducing the dimensionality of the DTM:
instead of (tens of) thousands of columns for each unique word we only need hundreds of columns for the embedding vectors.
This means that further processing can be more efficient as fewer parameters need to be fit,
or conversely that more complicated models can be used without blowing up the parameter space.
Another advantage is that a model can also give a result for words it never saw before, as these words most likely will have an embedding vector and so can be fed into the model.
Finally, since words with similar meanings should have similar vectors,
a model fit on embedding vectors gets a ``head start'' since the vectors for words like ``great'' and ``fantastic'' will already be relatively close to each other, while all columns in a normal DTM are treated independently.

The assumption that words with similar meanings have similar vectors can also be used directly to extract synonyms.
This can be very useful, for example for (semi-)automatically expanding a dictionary for a concept.
Example~\ref{ex:embedding} shows how to download and use pre-trained embedding vectors to extract synonyms.
First, we download a very small subset of the pre-trained Glove embedding vectors\footnote{The full embedding models can be downloaded from https://nlp.stanford.edu/projects/glove/. To make the file easier to download, we took only the 10\,000 most frequent words of the smallest embeddings file (the 50 dimension version of the 6B tokens model). For serious applications you probably want to download the larger files, in our experience the 300 dimension version usually gives good results. Note that the files on that site are in a slightly different format which lacks the initial header line, so if you want to use other vectors for the examples here you can convert them with the \index{glove2word2vec}\texttt{glove2word2vec} function in the \index{gensim}\emph{gensim} package. For R, you can also simply omit the \verb|skip=1| argument as apart from the header line the formats are identical},
wrapping the download call in a condition to only download it when needed.

Then, for Python, we use the excellent support from the \index{gensim}\emph{gensim} package to load the embeddings into a \index{KeyedVectors}\texttt{KeyedVectors} object.
Although not needed for the rest of the example, we create a pandas DataFrame from the internal embedding values so the internal structure becomes clear: each row is a word, and the columns (in this case 50) are the different (semantic) dimensions that characterize that word according to the embeddings model.
This data frame is sorted on the first dimension, which shows that negative values on that dimension are related to various sports.
Next, we switch back to the \index{KeyedVectors}\texttt{KeyedVectors} object to get the most similar words to the word \emph{fraud}, which is apparently related to similar words like \emph{bribery} and \emph{corruption} but also to words like \emph{charges} and \emph{alleged}.
These similarities are a good way to (semi-)automatically expand a dictionary: start from a small list of words,
find all words that are similar to those words, and if needed manually curate that list.
Finally, we use the embeddings to solve the ``analogies'' that famously showcase the geometric nature of these vectors:
if you take the vector for \emph{king}, subtract the vector for \emph{man} and add that for \emph{woman},
the closest word to the resulting vector is \emph{queen}.
Amusingly, it turns out that soccer is a female form of football, probably showing the American cultural origin of the source material.

For R, there was less support from existing packages so we decided to use the opportunity to show both the conceptual simplicity of embeddings vectors and the power of matrix manipulation in R.
Thus, we directly read in the word vector file which has a head line and then on each line a word followed by its 50 values.
This is converted to a matrix with the row names showing the word,
which we normalize to (Euclidean) length of one for each vector for easier processing.
To determine similarity, we take the cosine distance between the vector representing a word with all other words in the matrix.
As you might remember from algebra, the cosine distance is the dot product between the vectors normalized to have length one
(just like Pearson's product--moment correlation is the dot product between the vectors normalized to z-scores per dimension).
Thus, we can simply multiply the normalized target vector with the normalized matrix to get the similarity scores.
These are then sorted, renamed, and the top values are taken using the basic functions from Chapter~\ref{chap:datawrangling}.
Finally, analogies are solved by simply adding and subtracting the vectors as explained above, and then listing the closest words to the resulting vector
(excluding the words in the analogy itself).

\begin{ccsexample}
\doublecodex{chapter10/embeddings0}
\doublecodex{chapter10/embeddings1}
\codexoutputtable{chapter10/embeddings1.r}
\doublecodex{chapter10/embeddings2}
\codexoutputtable{chapter10/embeddings2.r}
\doublecodex{chapter10/embeddings3}
\codex[caption=Output]{chapter10/embeddings3.r.out}
\caption{Using word embeddings for finding similar and analogous words.}\label{ex:embedding}
\end{ccsexample}





\subsection{Linguistic Preprocessing}
\label{sec:nlp}

A final technique to be discussed here is the use of linguistic preprocessing steps to enrich and filter a DTM.
So far, all techniques discussed here are language independent.
However, there are also many language-specific tools for automatically enriching text developed by computational linguistics communities around the world.
Two techniques will be discussed here as they are relatively widely available for many languages and easy and quick to apply: \index{Part-of-speech tagging}\emph{Part-of-speech tagging} and \index{lemmatizing}\emph{lemmatizing}.

In \index{part-of-speech tagging}\emph{part-of-speech tagging} or POS-tagging, each word is enriched with information on its function in the sentence: verb, noun, determiner etc.
For most languages, this can be determined with very high accuracy, although sometimes text can be ambiguous:
in one famous example, the flies in \emph{fruit flies} is generally a noun (fruit flies are a type of fly), but it can also be a verb (if fruit could fly).
Although there are different sets of POS tags used by different tools, there is broad agreement on the core set of tags listed in Table~\ref{tab:postags}.

%\input{chapter10/tabpostags}

\begin{table}
  \caption{\label{tab:postags}Overview of part-of-speech (POS) tags.}{
  \begin{tabularx}{\textwidth}{llll}
    \toprule
    Part of speech & Example & UDPipe/Spacy Tag & Penn Treebank Tag \\
    \midrule
    Noun            & apple & NOUN & NN, NNS \\
    Proper Name     & Carlos & PROPN & NNP \\
    Verb            & write & VERB & VB, VBD, VBP, .. \\
    Auxilliary verb & be, have & VERB & (same as verb) \\
    Adjective       & quick & ADJ & JJ, JJR, JJS \\
    Adverb          & quickly & ADV & RB \\
    Pronoun         & I, him  & PRON & PRP \\
    Adposition      & of, in  & ADP & IN \\
    Determiner      & the, a & DET & DT \\
\bottomrule
  \end{tabularx}}{}
  \end{table}


POS tags are useful since they allow us for example to analyze only the \textit{nouns} if we care about the things that are discussed, only the \textit{verbs} if we care about actions that are described, or only the \textit{adjectives} if we care about the characteristics given to a noun.
Moreover, knowing the POS tag of a word can help disambiguate it.
For example, like as a verb (I like books) is generally positive, but like as a preposition (a day like no other) has no clear sentiment attached.

\index{Lemmatizing}\emph{Lemmatizing} is a technique for reducing each word to its root or \index{lemma}\emph{lemma} (plural: lemmata).
For example, the lemma of the verb \emph{reads} is (to) \emph{read} and the lemma of the noun \emph{books} is \emph{book}.
Lemmatizing is useful since for most of our research question we do not care about these different conjugations of the same word.
By lemmatizing the texts, we do not need to include all conjugations in a dictionary,
and it reduces the dimensionality of the DTM -- and thus also the data scarcity.

Note that lemmatizing is related to a technique called \emph{stemming}, which removes known suffixes (endings) from words.
For example, for English it will remove the ``s'' from both reads and books.
Stemming is much less sophisticated than lemmatizing, however, and will trip over irregular conjugations
(e.g.\ \emph{are} as a form of to be) and regular word endings that look like conjugations (e.g.\ \emph{virus} will be stemmed to \emph{viru}).
English has relatively simple conjugations and stemming can produce adequate results.
For morphologically richer languages such as German or French, however, it is strongly advised to use lemmatizing instead of stemming.
Even for English we would generally advise lemmatization since it is so easy nowadays and will yield better results than stemming.

For Example~\ref{ex:udpipe}, we use the \index{UDPipe}\emph{UDPipe} natural language processing toolkit \citep{udpipe},
a ``Pipeline'' that parses text into ``Universal Dependencies'', a representation of the syntactic structure of the text.
For R, we can immediately call the \index{udpipe}\texttt{udpipe} function from the package of the same name.
This parses the given text and returns the result as a data frame with one token (word) per row,
and the various features in the columns.
For Python, we need to take some more steps ourselves.
First, we download the English models if they aren't present.
Second, we load the model and create a pipeline with all default settings,
and use that to parse the same sentence.
Finally, we use the \index{conllu}\emph{conllu} package to read the results into a form that can be turned into a data frame.

In both cases, the resulting tokens clearly show some of the potential advantages of linguistic processing:
the lemma column shows that it correctly deals with irregular verbs and plural forms.
Looking at the upos (universal part-of-speech) column, John is recognized as a proper name (PROPN), brought as a verb, and knives as a noun.
Finally, the \verb|head_token_id| and \verb|dep_rel| columns represent the syntactic information in the sentence:
``Bought'' (token 2) is the root of the sentence, and ``John'' is the subject (nsubj) while ``knives'' is the object of the buying.

\pyrex[caption=Using UDPipe to analyze a sentence,format=table,output=r]{chapter10/udpipe}

The syntactic relations can be useful if you need to differentiate between who is doing something and whom it was done to.
For example, one of the authors of this book used syntactic relations to analyze conflict coverage,
where there is an important difference between attacking and getting attacked \citep{clause}.
However, in most cases you probably don't need this information and analyzing dependency graphs is relatively complex.
We would advise you to almost always consider lemmatizing and tagging your texts, as lemmatizing is simply so much better than stemming
(especially for languages other than English), and the part-of-speech can be very useful for analyzing different aspects of a text.

If you only need the lemmatizer and tagger, you can speed up processing by setting \verb|udpipe(.., parser='none')| (R) or setting the third argument to Pipeline (the parser) to \verb|Pipeline.NONE| (Python).
Example~\ref{ex:nouncloud} shows how this can be used to extract only the nouns from the most recent state of the union speeches,
create a DTM with these nouns, and then visualize them as a word cloud.
As you can see, these words (such as student, hero, childcare, healthcare, and terrorism), are much more indicative of the topic of a text than the general words used earlier.
In the next chapter we will show how you can further analyze these data, for example by analyzing usage patterns per person or over time, or using an unsupervised topic model to cluster words into topics.

\pyrex[caption=Nouns used in the most recent State of the Union addresses,format=png,output=py]{chapter10/nouncloud}

As an alternative to UDPipe, you can also use Spacy,
which is another free and popular natural language toolkit.
It is written in Python, but the \index{spacyr}\emph{spacyr} package offers an easy way to use it from R.
For R users, installation of \index{spacyr}\emph{spacyr} on MacOS is easy,
but note that on Windows there are some additional steps, see
\url{https://cran.r-project.org/web/packages/spacyr/readme/README.html} for more details.

Example~\ref{ex:spacy} shows how you can use Spacy to analyze the proverb ``all roads lead to Rome'' in Spanish.
In the first block, the Spanish language model is downloaded (this is only needed once).
The second block loads the language model and parses the sentence.
You can see that the output is quite similar to UDPipe, but one additional feature is the inclusion of
\index{Named Entity Recognition}\emph{Named Entity Recognition}:
Spacy can automatically identify persons, locations, organizations and other entities.
In this example, it identifies ``Rome'' as a location.
This can be very useful to extract e.g.\ all persons from a newspaper corpus automatically.
Note that in R, you can use the \index{quanteda}\emph{quanteda} function \index{as.tokens}\texttt{as.tokens} to directly use the Spacy output in quanteda.

\begin{ccsexample}
  \doublecodex{chapter10/spacymodel}
  \doublecodex{chapter10/spacy}
  \codexoutputtable{chapter10/spacy.r}
  \caption{Using Spacy to analyze a Spanish sentence.}\label{ex:spacy}
\end{ccsexample}

As you can see, nowadays there are a number of good and relatively easy to use linguistic toolkits that can be used.
Especially \index{Stanza}\emph{Stanza} \citep{stanza} is also a very good and flexible toolkit with support for multiple (human) languages and good integration especially with Python.
If you want to learn more about natural language processing, the book \emph{Speech and Language Processing} by Jurafsky and Martin is a very good starting point \citep{jurafsky}\footnote{See \url{https://web.stanford.edu/~jurafsky/slp3/} for their draft of a new edition, which is (at the time of writing) free to download}.


\section{Which Preprocessing to Use?}

This chapter has shown how to create a DTM and especially introduced a number of different steps that can be used to clean and preprocess the DTM before analysis.
All of these steps are used by text analysis practitioners and in the relevant literature.
However, no study ever uses all of these steps on top of each other.
This of courses raises the question of how to know which preprocessing steps to use for your research question.

First, there are a number of things that you should (almost) always do.
If your data contains noise such as boilerplate language, HTML artifacts, etc., you should generally strip these out before proceeding.
Second, text almost always has an abundance of uninformative (stop) words and a very long tail of very rare words.
Thus, it is almost always a good idea to use a combination of stop word removal, trimming based on document frequency, and/or tf$cdot$idf weighting.
Note that when using a stop word list, you should always manually inspect and/or fine-tune the word list to make sure it matches your domain and research question.

The other steps such as n-grams, collocations, and tagging and lemmatization are more optional but can be quite important depending on the specific research.
For this (and for choosing a specific combination of trimming and weighting), it is always good to know your domain well, look at the results, and think whether you think they make sense.
Using the example given above, bigrams can make more sense for sentiment analysis (since \emph{not good} is quite different from \emph{good}),
but for analyzing the topic of texts it may be less important.

Ultimately, however, many of these questions have no good theoretical answer, and the only way to find a good preprocessing ``pipeline'' for your research question is to try many different
options and see which works best.
This might feel like ``cheating'' from a social science perspective, since it is generally frowned upon to just test many different statistical models and report on what works best.
There is a difference, however, between substantive statistical modeling where you actually want to understand the mechanisms,
and technical processing steps where you just want the best possible measurement of an underlying variable (presumably to be used in a subsequent substantive model).
\citet{mousetrap} uses the analogy of the mouse trap and the human condition: in engineering you want to make the best possible mouse trap,
while in social science we want to understand the human condition.
For the mouse trap, it is OK if it is a black box for which we have no understanding of how it works, as long as we are sure that it does work.
For the social science model, this is not the case as it is exactly the inner workings we are interested in.

Technical (pre)processing steps such as those reviewed in this chapter are primarily engineering devices:
we don't really care how something like tf$cdot$idf works, as long as it produces the best possible measurement of the variables we need for our analysis.
In other words, it is an engineering challenge, not a social science research question.
As a consequence, the key criterion by which to judge these steps is validity, not explainability.
Thus, it is fine to try out different options, as long as you validate the results properly.
If you have many different choices to evaluate against some metric such as performance on a subsequent prediction task,
using the split-half or cross-validation techniques discussed in chapter Chapter~\ref{chap:introsml} are also relevant here to avoid biasing the evaluation.

