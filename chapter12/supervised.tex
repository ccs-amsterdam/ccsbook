\section{Supervised text analysis: automatic classification and sentiment analysis}
\label{sec:supervised}

For many applications, there are good reasons to use the dictionary
approach presented in the previous section. First, it is intuitively
understandable and results can -- in principle --
even be varified by hand, which can be an advantage when transparency
or communicatibility is of high importance. Second, it is very easy to
use. But as we have discussed in \refsec{deciding}, dictionary approaches
in general perform less well the more abstract, non-manifest, or
complex a concept becomes. In the next section, we will make the case
that topics, but also sentiment, in fact, are quite a complex concepts
that are often hard to capture with dictionaries (or at least, crafting
a custom dictionary would be  difficult). For instance, while ``positive''
and ``negative'' seem straightforward categories at first sight,
the more we think about it, the more apparent it becomes how context-
dependent it actually is: in a dataset about the economy and stock
market returns, ``increasing'' may indicate something positive,
in a dataset about unemployment rates the same word would be something
negative. 

\subsection{Putting together a workflow}
\label{sec:workflow}

With the knowledge we gained in previous chapters, it is not difficult
to set up a supervised machine learning classifier to automatically
determine, for instance, the topic of a news article.

Let us recap the building blocks that we need. In
Chapter~\ref{chap:introsml}, you learned how to use different
classifiers, how to evaluate them, and how to choose the best
settings. However, in these examples, we used numerical data as
features; now, we have text.  In \refchap{dtm},
you learned how to turn text into numerical
features. And that's all we need to get started!

Typical examples for supervised machine learning in the analysis of
communication include the classification of topics
\citep[e.g.,][]{Scharkow2011}, frames \citep[e.g.,][]{Burscher2014},
user characteristics such as gender or ideology,
or sentiment.

Let us consider the case of sentiment analysis more in
detail. Classical sentiment analysis is done with a dictionary
approach: you take a list of positive words, a list of negative words,
and count what occurs more. Addtionally, one may attach a weight to
each word, such that ``perfect'' gets a higher weight than ``good'',
for instance.  An obvious drawbacks is that these pure
bag-of-words approaches cannot cope with negation (``not good'') and
intensifiers ``very good''), which is why extensions have been
developed that take these (and other features, such as punctuation)
into accout \citep{Thelwall2012,Hutto2014,DeSmedt2012}.
% come back to this in crowddcoding-chapter: \citep{Haselmayer2016}

But while available off-the-shelf packages that implement these
extended dictionary-based methods are very easy to use (in fact, they
spit out a sentiment score with one single line of code), it is
questionable how well they work in practice. After all, ``sentiment''
is not exactly a clear, manifest concept for which we can enumerate a
list of words. It has been shown that results obtained with multiple
of these packages correlate very poorly with each other and with human
annotations \citep{Boukes2019,Chan2021}.

Consequently, it has been suggested that it is better to use
supervised machine learning to automatically code the sentiment of
texts \citep{Gonzalez-Bailon2015,vermeer2019seeing}. In particular,
one may need to annotate a custom, own dataset: training a classifier
on, for instance, movie reviews and then using it to predict sentiment
in political texts violates the assumption that training set, test
set, and the unlabeled data that are to be classified are (at least in
principle and approximately) drawn from the same population.

To illustrate the workflow, we will use the ACL IMDB dataset, a large
dataset that consists out of a training dataset of 25,000 movie
reviews (out of which 12,500 positives ones and 12,500 negative ones)
and an equally sized test dataset \citep{aclimdb}. It can be
downloaded at
\url{https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz}


These data do not come in one file, but rather in a set of textfiles
that are sorted in different folders named after the dataset they
belong to (|test| or |train|) and their label (|pos| and |neg|). This
means that we cannot simply use a pre-defined function to read them,
but we need to think of a way of reading the content into a
datastructure that we can use. One way of doing so is shown in
\refex{readimdb}.

\pyrex[output=none, caption={Reading the ACL IMDB dataset.}]{chapter12/readimdb}

\label{feature:sparse}
\begin{feature}
  \textbf{Sparse versus dense matrices and why it matters a lot for
    choosing between R and Python for machine learning.} In a
  document-term matrix, you would typically find a lot of zeros: most
  words do \emph{not} appear in any given document. For instance, the
  reviews in the IMDB dataset contain more than 100,000 unique
  words. Hence, the matrix has more than 100,000 columns. Yet, most
  reviews only consist of a couple of hundreds of words. As a
  consequence, more than 99\% of the cells in the table contain a
  zero. In a sparse matrix, we do not store all these zeros, but only
  store the values for cells that actually contain a value. This
  drastically reduces the memory needed.  But even if you have a huge
  amount of memory, this does not solve the issue: In R, the number of
  cells in a matrix is limited to 2,147,483,647. It is therefore
  impossible to store a matrix with 100,000 features and 25,000
  documents as a dense matrix. Unfortunately, many models that you can
  run via \pkg{caret} in R will convert your sparse document-term
  matrix to a dense matrix, and hence are effectively only usable for
  very small datasets. An alternative is using the \pkg{quanteda} package,
  which does use sparse matrices throughout. However, at the time of
  writing this book, quanteda only provides a very limited number of
  models. As all of these problems do not arise in \pkg{scikit-learn},
  you may want to consider using Python for many text classification tasks.
\end{feature}


Let us now train our first classifier. We choose a Na\"ive Bayes
classifier with a simple count vectorizer (\refex{imdbbaseline}).  In
the Python example, pay attention to the fitting of the vectorizer: we
fit on the training data \emph{and} transform the training data with
it, but we only transform the test data \emph{without re-fitting the
  vectorizer}. Fitting, here, includes the decision which words to
include (by definition, words that are not present in the training
data are not included; but we could also choose additional
constraints, such as excluding very rare or very common words), but
also assigning an (internally used) identifier (variable name) to each
word. If we would fit the classifier again, these would not be
compatible any more. In R, the same is achieved in a slightly
different way: Two term-document matrices are created independently,
before they are matched in such a way that only the features that are
present in the training matrix are retained in the test matrix.


\note{A word that is not present in the training data, but is present
  in the test data, is thus ignored. If you want to use the
  information such out-of-vocabulary words can entail (e.g., they may
  be synonyms), you need to use a word embedding approach (see \refsec{wordembeddings})}

We do not necessarily expect this first model to be the best
classifier we could come up with, but it provides us with a reasonable
baseline. In fact, even without any further adjustments, it works
reasonably well: precision is higher for positive reviews and recall
is higher for negative reviews (classifying a positive review as
negative happens twice as much as the reverse), but none of the values
is concerningly low.

\pyrex[output=py, caption=Training a Na\"ive Bayes classifier with simple word counts as features]{chapter12/imdbbaseline}



\subsection{Finding the best classifier}

%\todo[inline]{\url{https://github.com/quanteda/quanteda.textmodels} seems to be under heavy development right now. In the CRAN version of quanteda, there is only a NB classifier present. Let's wait for a moment to check which textmodels will be available via quanteda and then incorporate them here.}
% As of 2020-11-10, still only logreg, svm, nb -- better, but much less than scikit lean


%TODO: Beter uitleggen (+ref ch 11)
Let us start by comparing two simple classifiers we know (Na\"ive
Bayes and Logistic Regression (see Section~\ref{sec:nb2dnn}) and two
vectorizers that transform our texts into two numerical
representations that we know: word counts and $tf\cdot idf$ scores
(see \refchap{dtm}).

We can also tune some things in the vectorizer, such as filtering out
stopwords, or specifying a minimum number (or proportion) of documents
in which a word needs to occur in order to be included, or the maximum
number (or proportion) of documents in which it is allowed to
occur. For instance, it could make sense to say that a word that
occurs in less than $n=5$ documents is probably a spelling mistake or
so unusual that it just unnecessarily bloats our feature matrix; and
on the other hand, a word that is so common that it occurs in more
than 50\% of all documents is so common that it does not help us to
distinguish between different classes.

We can try all of these things out by hand by just re-running the code
from \refex{imdbbaseline} and only changing the line in which the
vectorizer is specified and the line in which the classifier is
specified.
However, copy-pasting essentially the
same code is generally not a good idea, as it makes your code unnecessary
long and increases the likelihood of errors creeping in when you, for
instance, need to apply the same changes to multiple copies of the
code.  A more elegant approach is outlined in
\refex{basiccomparisons}: We define a function that gives us a short
summary of only the output we are interested in, and then use a
for-loop to iterate over all configurations we want to evaluate, fit
them and call the function we defined before. In fact, with 23 lines
of code, we manage to compare four different models, while we already
needed 15 lines (in \refex{imdbbaseline}) to evaluate only one model.


%TODO R-code?
\pyrex[input=py, output=py, caption={An example of a custom function to give a brief overview of the performance of four simple vectorizer-classifier combinations.}]{chapter12/basiccomparisons}


The output of this little example gives us already quite a bit of
insight into how to tackle our specific classification tasks: First, we
see that a $tf\cdot idf$ classifier seems to be slightly but
consistently superior to a count classifier (this is often, but not
always the case). Second, we see that the logistic regression performs
better than the Na\"ive Bayes classifier (again, this is often, but not
always, the case). In particular, in our case, the logistic regression
improved on the excessive misclassifcation of positive reviews as
negative, and achieves a very balanced performance.

There may be instances where one nevertheless may want to use a Count
Vectorizer with a Na\"ive Bayes classifier instead (especially if it
is too computationally expensive to estimate the other model), but for
now, we may settle on the best performing combination, logistic
regression with a $tf\cdot idf$ vectorizer. You could also try fitting
a Support Vector Machine instead, but we have little reason to believe
that our data isn't linearly separable, which means that there is
little reason to believe that the SVM will perform better. Given the
good performance we already achieved, we decide to stick to the
logistic regression for now.


We can now go as far as we like, include more models, use
crossvalidation and gridsearch (see
Section~\ref{sec:crossvalidation}), etc. However, our workflow now
consists of \emph{two} steps: fitting/transforming our input data
using a vectorizer, and fitting a classifier. To make things easier,
in scikit-learn, both steps can be combined into a so-called
pipe. \refex{basicpipe} shows how the loop in
\refex{basiccomparisons} can be re-written using pipes (the
result stays the same).

\pyrex[input=py, output=none, caption={Instead of fitting vectorizer and classifier separately, they can be combined in a pipeline.}]{chapter12/basicpipe}

Such a pipeline lends itself very well for performing a
gridsearch. \refex{gridsearchlogreg} gives you an example.  With
|LogisticRegression?| and |TfIdfVectorizer?|, we can get a list of all
possible hyperparameters that we may want to tune. For instance, these
could be the minimum and maximum frequency for words to be included or
whether we want to use only unigrams (single words) or also bigrams
(combinations of two words, see \refsec{ngram}.
For the Logistic Regression, it may be the
regularization hyperparameter C, which applies a penalty for too
complex models.  We can  put all values for these parameters
that we want to consider in a dictionary, with a descriptive key (i.e., a string with the step of the pipeline followed by two underscores and the name of the hyperparameter) and a list of all values we want to consider as corresponding value.

The gridsearch procedure will then estimate all combinations of all
values, using cross-validation (see Section~\ref{sec:validation}). In
our example, we have $2 \cdot 2 \cdot 2 \cdot 2 \cdot 3 = 24$
different models, and $24$ models $\cdot 5$ folds $= 120$ models to
estimate. Hence, it may take you some time to run the code.

\pyrex[input=py, output=py, caption={A gridsearch to find the best hyperparameters for a pipeline consisting of a vectorizer and a classifier. Note that we can tune any parameter that either the vectorizer or the classifier accepts as an input, not only the four hyperparameters we chose in this example.}]{chapter12/gridsearchlogreg}

We see that we could further improve our model to precision and recall
values of .90, by excluding extremely infrequent and extremely
frequent words), including both unigrams and bigrams (which, we may
speculate, helps us accounting for the ``not good'' vs ``not'',
``good'' problem), and changing the default penalty of C=1 to C=100.

Let us, just for the sake of it, compare the performance of our model
with an off-the-shelf sentiment analysis package, in this case Vader
\citep{Hutto2014}. For any text, it will directly estimate sentiment
scores (more specifically, a positivity score, a negativity score, a
neutrality score, and a compound measure that combines them), without
any need to have training data. However, as \refex{vader} shows, such
a method is clearly inferior to a supervised machine learning
approach. While in almost all cases (except for $n=11$ cases), Vader was able to
make a choice (getting scores of 0 is a notorious problem in very
short texts), precision and recall are clearly worse than even the
simple baseline model we started with, and much worse than those of
the final model we finished with. In fact, we miss half (!) of the
negative reviews. There are probably very few applications in the
analysis of communication in which we would find this acceptable.
It is important to highlight that this is not because the off-the-shelf
package we chose is a particularly bad one (on the contrary, it is
actually comparatively good), but because of the inherent limitations
of dictionary-based sentiment analysis.

\pyrex[input=py, output=py, caption={For the sake of comparison, we calculate how an off-the-shelf sentiment analysis package would have performed in this task}]{chapter12/vader}

We need to keep in mind, though, that with this dataset, we chose one
of the easiest sentiment analysis tasks: a set of long, rather formal
texts (compared to informal short social media messages), that
evaluate exactly one entity (one film), and that are not ambigous at
all. Many applications that communication scientists are interested
in are much less straight-forward. Therefore, however tempting it may be
to use an off-the-shelf package, doing so requires a thorough test
based on at least some human-annotated data.



\subsection{Using the model}

So far, we have focused on training and evalualting models, almost
forgetting why we were doing this in the first place: to use them to
predict the label for new data that we did not annotate.

Of course, we could always re-train the model when we need to use it
-- but that has two downsides: first, as you may have seen, it may
actually take considerable time to train it, and second, you need to
have the training data available, which may be a problem both in terms
of storage space and of copyright and/or privacy if you want to share
your classifier with others.

Therefore, it makes sense to save both our classifier and our
vectorizer to a file, so that we can reload them later
(\refex{reuse}). Keep in mind that you have to re-use \emph{both}
-- after all, the columns of your feature matrix will be different (and hence, completely useless for the classifier) when
fitting a new vectorizer. Therefore, as you see, you do not do any fitting any more, and only use the |.transform()| method of the (already fitted) vectorizer and the |.predict()| method of the (already fitted) classifier.

In R, you have no vectorizer you could save -- but because in contrast to Python, both your DTM and your classifier include the feature names, it suffices to save the classifier only (using \verb+saveRDS(myclassifier, "myclassifier.rds")+) and using on a new DTM later on. You do need to remember, though, how you constructed the DTM (e.g., which preprocessing steps you took), to make sure that the features are comparable.

\pyrex[output=py, input=py, caption=Saving and loading a vectorizer and a classifier]{chapter12/reuse}



Another thing that we might want to do is to get a better idea of the
features that the model uses to arrive at its prediction; in our
example, what actually characterizes the best and the worst
reviews. \refex{eli5} shows how this can be done in one line of code
using \pkg{eli5} -- a package that aims to ``\emph{e}xplain [the model]
\emph{l}ike \emph{I}'m \emph{5} years old''. Here, we re-use the
|pipe| we constructed earlier to provide both the vectorizer and the
classifier to \pkg{eli5} -- if we would have only provided the
classifier, then the feature names would have been internal
identifiers (which are meaningless to us) rather than human-readable
words.

\pyrex[output=py, input=py,format=html, caption=Using ELI5 to get the most predictive features]{chapter12/eli5}

We can also use eli5 to explain how the classifier arrived at a
prediction for a specific document, by using different shades of green
and red to explain how much different features contributed to the
classification, and in which direction (\refex{eli5b}).

\pyrex[output=py, input=py, format=html, caption=Using ELI5 to explain a prediction]{chapter12/eli5b}

\subsection{Deep Learning}

Deep learning models were introduced in \refsec{deeplearning} as a (relatively) new class of models in
supervised machine learning.
Using the Python \pkg{keras} package you can define various model architectures such as Convolutional or Recurrent Neural Networks.
Although it is beyond the scope of this chapter to give a detailed treatment of building and training deep learning models, in this section we do give an example of using create a Convolutional Neural Network using pre-trained word embeddings.
We would urge anyone who is interested in machine learning for text analysis to continue studying deep learnings, probably starting with the excellent book by \citet{goldberg2017}.

Impressively, in R you can now also use the \pkg{keras} package to train deep learning models, as shown in the examply.
Similar to how \pkg{spacyr} works (\refsec{nlp}), the R package actually installs and calls Python behind the screens using the \pkg{reticulate} package.
Although the resulting models are relatively similar, it is less easy to build and debug the models in R,
if only because most of the documentation and community examples are writting in Python.
Thus in the end, we probably recommend people who want to dive into deep learning to choose Python rather than R.

\pyrex[caption={Dutch Sentiment Data (Van Atteveldt et al., 2020)},output=r,format=table]{chapter12/rnndata}

\pyrex[output=py,caption=Deep Learning: Definining a Recursive Neural Network]{chapter12/rnnmodel} 

\pyrex[caption=Deep Learning: Training and Testing the model]{chapter12/rnn} 


First, \refex{rnndata} loads a dataset described by \citet{vanatteveldt20}, which consists of Dutch economic news headlines with a sentiment value.
Next, in \refex{rnnmodel} a model is defined consisting of several layers,
corresponding roughly to \reffig{cnn}. 
First, an \emph{embedding} layer transfors the textual input into a semantic vectors for each word.
Next, the \emph{convolutional} layer defines features (filters) over windows of vectors,
which are then pooled in the \emph{max-pooling} layer.
This results in a vector of detected features for each document,
which are then used in a regular (hidden) \emph{dense} layer followed by an output layer.

Finally, in \refex{rnn} we train the model on 4,000 documents and test it against the remaining documents.
The Python model, which uses pre-trained word embeddings (the \texttt{w2v\_320d} file downloaded at the top),
achieves a mediocre accuracy of about 56\% (probably due to the low number of training sentences).
The R model, which trains the embedding layer as part of the model, performs more poorly at 45\% as this model is even more dependent on large training data to properly estimate the embedding layer. 
