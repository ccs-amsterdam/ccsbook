\section{Supervised text analysis: automatic classification and sentiment analysis}
\label{sec:supervised}

For many applications, there are good reasons to use the dictionary
approach presented in the previous section. First, it is intuitively
understandable and results can -- in principle --
even be varified by hand, which can be an advantage when transparency
or communicatibility is of high importance. Second, it is very easy to
use.

%TODO hoort deze beschrijving niet bij 12.1? (of 12.0, of een aparte sectie `choosing a method')?
Dictionary approaches excel under three conditions: First, the
variable we want to code is \emph{manifest and concrete} rather than
\emph{latent and abstract}: names of actors, specific physical
objects, specific phrases, etc., rather than feelings, frames, or
topics. Second, all synonyms to be included must be known
beforehand. And third, the dictionary entries must not have multiple
meanings.

For instance, coding how often gun control is mentioned in political
speeches fits these criteria. There are only so many ways to talk
about it, and it is rather unlikely that speeches about other topics
contain a phrase like ``gun control''. Similarly, if we want to find
references to Angela Merkel, Donald Trump, or any other well-known
politician, we can just directly search for their names -- even though
problems arise when people have very common surnames and are referred
to by their surnames only.

Sadly, most interesting concepts are more complex to code. Take a
seemingly straightforward problem: distinguishing whether a news
article is about the economy or not. This is really easy to do for
humans: there may be some edge cases, but in general, people rarely
need longer than a few seconds to grasp whether an article is about the
economy rather than about sports, culture, etc. Yet, many of these
articles won't directy state that they are about the economy by
explicitly using the word ``economy''.

We may think of extending our dictionary not only with |economi.*| (a
regular expression that includes economists, economic, and so on), but
also come up with other words like ``stock exchange'', ``market'',
``company.'' Unfortunately, we will quickly run into a problem that we also
faced when we dicussed the precision-recall tradeoff in
Section~\ref{sec:validation}: The more terms we add to our
dictionary, the more false positives we will get: articles about
the geographical space called ``market'', about some celebrity being seen
in ``company'' of someone else, and so on.

From this example, we can conclude that often (1) it is easy for
humans to decide to which class a text belongs, but (2) it is very
hard for humans to come op with a list of words (or rules) on which
their judgement is based.

Such a situation is perfect for applying supervised machine
learning: After all, it won't take us much time to annotate, say, 1000
articles based on whether they are about the economy or not (probably
this takes less time than thoroughly finetuning a list of words to in-
or exclude); and the difficult part, deciding on the exact rules
underlying the decision to classify an article as economic is done by
the computer in seconds.


\subsection{Putting together a workflow}
\label{sec:workflow}

With the knowledge we gained in previous chapters, it is not difficult
to set up a supervised machine learning classifier to automatically
determine, for instance, the topic of a news article.

Let us recap the building blocks that we need. In
Chapter~\ref{chap:introsml}, you learned how to use different
classifiers, how to evaluate them, and how to choose the best
settings. However, in these examples, we used numerical data as
features; now, we have text.  In \refchap{dtm},
you learned how to turn text into numerical
features. And that's all we need to get started!

Typical examples for supervised machine learning in the analysis of
communication include the classification of topics
\citep[e.g.,][]{Scharkow2011}, frames \citep[e.g.,][]{Burscher2014},
user characteristics such as gender or ideology,
or sentiment.

Let us consider the case of sentiment analysis more in
detail. Classical sentiment analysis is done with a dictionary
approach: you take a list of positive words, a list of negative words,
and count what occurs more. Addtionally, one may attach a weight to
each word, such that ``perfect'' gets a higher weight than ``good'',
for instance.  An obvious drawbacks is that these pure
bag-of-words approaches cannot cope with negation (``not good'') and
intensifiers ``very good''), which is why extensions have been
developed that take these (and other features, such as punctuation)
into accout \citep{Thelwall2012,Hutto2014,DeSmedt2012}.
% come back to this in crowddcoding-chapter: \citep{Haselmayer2016}

But while available off-the-shelf packages that implement these
extended dictionary-based methods are very easy to use (in fact, they
spit out a sentiment score with one single line of code), it is
questionable how well they work in practice. After all, ``sentiment''
is not exactly a clear, manifest concept for which we can enumerate a
list of words. It has been shown that results obtained with multiple
of these packages correlate very poorly with each other and with human
annotations \citep{Boukes2019,Chan2021}.

Consequently, it has been suggested that it is better to use
supervised machine learning to automatically code the sentiment of
texts \citep{Gonzalez-Bailon2015,vermeer2019seeing}. In particular,
one may need to annotate a custom, own dataset: training a classifier
on, for instance, movie reviews and then using it to predict sentiment
in political texts violates the assumption that training set, test
set, and the unlabeled data that are to be classified are (at least in
principle and approximately) drawn from the same population.

To illustrate the workflow, we will use the ACL IMDB dataset, a large
dataset that consists out of a training dataset of 25,000 movie
reviews (out of which 12,500 positives ones and 12,500 negative ones)
and an equally sized test dataset \citep{aclimdb}. It can be
downloaded at
\url{https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz}


These data do not come in one file, but rather in a set of textfiles
that are sorted in different folders named after the dataset they
belong to (|test| or |train|) and their label (|pos| and |neg|). This
means that we cannot simply use a pre-defined function to read them,
but we need to think of a way of reading the content into a
datastructure that we can use. One way of doing so is shown in
\refex{readimdb}.

\pyrex[output=none, caption={Reading the ACL IMDB dataset.}]{chapter12/readimdb}

\label{feature:sparse}
\begin{feature}
  \textbf{Sparse versus dense matrices and why it matters a lot for
    choosing between R and Python for machine learning.} In a
  document-term matrix, you would typically find a lot of zeros: most
  words do \emph{not} appear in any given document. For instance, the
  reviews in the IMDB dataset contain more than 100,000 unique
  words. Hence, the matrix has more than 100,000 columns. Yet, most
  reviews only consist of a couple of hundreds of words. As a
  consequence, more than 99\% of the cells in the table contain a
  zero. In a sparse matrix, we do not store all these zeros, but only
  store the values for cells that actually contain a value. This
  drastically reduces the memory needed.  But even if you have a huge
  amount of memory, this does not solve the issue: In R, the number of
  cells in a matrix is limited to 2,147,483,647. It is therefore
  impossible to store a matrix with 100,000 features and 25,000
  documents as a dense matrix. Unfortunately, many models that you can
  run via \pkg{caret} in R will convert your sparse document-term
  matrix to a dense matrix, and hence are effectively only usable for
  very small datasets. An alternative is using the \pkg{quanteda} package,
  which does use sparse matrices throughout. However, at the time of
  writing this book, quanteda only provides a very limited number of
  models. As all of these problems do not arise in \pkg{scikit-learn},
  you may want to consider using Python for many text classification tasks.
\end{feature}


Let us now train our first classifier. We choose a Na\"ive Bayes
classifier with a simple count vectorizer (\refex{imdbbaseline}).  In
the Python example, pay attention to the fitting of the vectorizer: we
fit on the training data \emph{and} transform the training data with
it, but we only transform the test data \emph{without re-fitting the
  vectorizer}. Fitting, here, includes the decision which words to
include (by definition, words that are not present in the training
data are not included; but we could also choose additional
constraints, such as excluding very rare or very common words), but
also assigning an (internally used) identifier (variable name) to each
word. If we would fit the classifier again, these would not be
compatible any more. In R, the same is achieved in a slightly
different way: Two term-document matrices are created independently,
before they are matched in such a way that only the features that are
present in the training matrix are retained in the test matrix.


\note{A word that is not present in the training data, but is present
  in the test data, is thus ignored. If you want to use the
  information such out-of-vocabulary words can entail (e.g., they may
  be synonyms), you need to use a word embedding approach (see \refsec{wordembeddings})}

We do not necessarily expect this first model to be the best
classifier we could come up with, but it provides us with a reasonable
baseline. In fact, even without any further adjustments, it works
reasonably well: precision is higher for positive reviews and recall
is higher for negative reviews (classifying a positive review as
negative happens twice as much as the reverse), but none of the values
is concerningly low.

\pyrex[output=py, caption=Training a Na\"ive Bayes classifier with simple word counts as features]{chapter12/imdbbaseline}



\subsection{Finding the best classifier}

%\todo[inline]{\url{https://github.com/quanteda/quanteda.textmodels} seems to be under heavy development right now. In the CRAN version of quanteda, there is only a NB classifier present. Let's wait for a moment to check which textmodels will be available via quanteda and then incorporate them here.}
% As of 2020-11-10, still only logreg, svm, nb -- better, but much less than scikit lean


%TODO: Beter uitleggen (+ref ch 11)
Let us start by comparing two simple classifiers we know (Na\"ive
Bayes and Logistic Regression (see Section~\ref{sec:nb2dnn}) and two
vectorizers that transform our texts into two numerical
representations that we know: word counts and $tf\cdot idf$ scores
(see \refchap{dtm}).

We can also tune some things in the vectorizer, such as filtering out
stopwords, or specifying a minimum number (or proportion) of documents
in which a word needs to occur in order to be included, or the maximum
number (or proportion) of documents in which it is allowed to
occur. For instance, it could make sense to say that a word that
occurs in less than $n=5$ documents is probably a spelling mistake or
so unusual that it just unnecessarily bloats our feature matrix; and
on the other hand, a word that is so common that it occurs in more
than 50\% of all documents is so common that it does not help us to
distinguish between different classes.

We can try all of these things out by hand by just re-running the code
from \refex{imdbbaseline} and only changing the line in which the
vectorizer is specified and the line in which the classifier is
specified.
However, copy-pasting essentially the
same code is generally not a good idea, as it makes your code unnecessary
long and increases the likelihood of errors creeping in when you, for
instance, need to apply the same changes to multiple copies of the
code.  A more elegant approach is outlined in
\refex{basiccomparisons}: We define a function that gives us a short
summary of only the output we are interested in, and then use a
for-loop to iterate over all configurations we want to evaluate, fit
them and call the function we defined before. In fact, with 23 lines
of code, we manage to compare four different models, while we already
needed 15 lines (in \refex{imdbbaseline}) to evaluate only one model.


%TODO R-code?
\pyrex[input=py, output=py, caption={An example of a custom function to give a brief overview of the performance of four simple vectorizer-classifier combinations.}]{chapter12/basiccomparisons}


The output of this little example gives us already quite a bit of
insight into how to tackle our specific classification tasks: First, we
see that a $tf\cdot idf$ classifier seems to be slightly but
consistently superior to a count classifier (this is often, but not
always the case). Second, we see that the logistic regression performs
better than the Na\"ive Bayes classifier (again, this is often, but not
always, the case). In particular, in our case, the logistic regression
improved on the excessive misclassifcation of positive reviews as
negative, and achieves a very balanced performance.

There may be instances where one nevertheless may want to use a Count
Vectorizer with a Na\"ive Bayes classifier instead (especially if it
is too computationally expensive to estimate the other model), but for
now, we may settle on the best performing combination, logistic
regression with a $tf\cdot idf$ vectorizer. You could also try fitting
a Support Vector Machine instead, but we have little reason to believe
that our data isn't linearly separable, which means that there is
little reason to believe that the SVM will perform better. Given the
good performance we already achieved, we decide to stick to the
logistic regression for now.


We can now go as far as we like, include more models, use
crossvalidation and gridsearch (see
Section~\ref{sec:crossvalidation}), etc. However, our workflow now
consists of \emph{two} steps: fitting/transforming our input data
using a vectorizer, and fitting a classifier. To make things easier,
in scikit-learn, both steps can be combined into a so-called
pipe. \refex{basicpipe} shows how the loop in
\refex{basiccomparisons} can be re-written using pipes (the
result stays the same).

\pyrex[input=py, output=none, caption={Instead of fitting vectorizer and classifier separately, they can be combined in a pipeline.}]{chapter12/basicpipe}

Such a pipeline lends itself very well for performing a
gridsearch. \refex{gridsearchlogreg} gives you an example.  With
|LogisticRegression?| and |TfIdfVectorizer?|, we can get a list of all
possible hyperparameters that we may want to tune. For instance, these
could be the minimum and maximum frequency for words to be included or
whether we want to use only unigrams (single words) or also bigrams
(combinations of two words, see \refsec{ngram}.
For the Logistic Regression, it may be the
regularization hyperparameter C, which applies a penalty for too
complex models.  We can  put all values for these parameters
that we want to consider in a dictionary, with a descriptive key (i.e., a string with the step of the pipeline followed by two underscores and the name of the hyperparameter) and a list of all values we want to consider as corresponding value.

The gridsearch procedure will then estimate all combinations of all
values, using cross-validation (see Section~\ref{sec:validation}). In
our example, we have $2 \cdot 2 \cdot 2 \cdot 2 \cdot 3 = 24$
different models, and $24$ models $\cdot 5$ folds $= 120$ models to
estimate. Hence, it may take you some time to run the code.

\pyrex[input=py, output=py, caption={A gridsearch to find the best hyperparameters for a pipeline consisting of a vectorizer and a classifier. Note that we can tune any parameter that either the vectorizer or the classifier accepts as an input, not only the four hyperparameters we chose in this example.}]{chapter12/gridsearchlogreg}

We see that we could further improve our model to precision and recall
values of .90, by excluding extremely infrequent and extremely
frequent words), including both unigrams and bigrams (which, we may
speculate, helps us accounting for the ``not good'' vs ``not'',
``good'' problem), and changing the default penalty of C=1 to C=100.

Let us, just for the sake of it, compare the performance of our model
with an off-the-shelf sentiment analysis package, in this case Vader
\citep{Hutto2014}. For any text, it will directly estimate sentiment
scores (more specifically, a positivity score, a negativity score, a
neutrality score, and a compound measure that combines them), without
any need to have training data. However, as \refex{vader} shows, such
a method is clearly inferior to a supervised machine learning
approach. While in almost all cases (except for $n=11$ cases), Vader was able to
make a choice (getting scores of 0 is a notorious problem in very
short texts), precision and recall are clearly worse than even the
simple baseline model we started with, and much worse than those of
the final model we finished with. In fact, we miss half (!) of the
negative reviews. There are probably very few applications in the
analysis of communication in which we would find this acceptable.
It is important to highlight that this is not because the off-the-shelf
package we chose is a particularly bad one (on the contrary, it is
actually comparatively good), but because of the inherent limitations
of dictionary-based sentiment analysis.

\pyrex[input=py, output=py, caption={For the sake of comparison, we calculate how an off-the-shelf sentiment analysis package would have performed in this task}]{chapter12/vader}

We need to keep in mind, though, that with this dataset, we chose one
of the easiest sentiment analysis tasks: a set of long, rather formal
texts (compared to informal short social media messages), that
evaluate exactly one entity (one film), and that are not ambigous at
all. Many applications that communication scientists are interested
in are much less straight-forward. Therefore, however tempting it may be
to use an off-the-shelf package, doing so requires a thorough test
based on at least some human-annotated data.



\subsection{Using the model}

So far, we have focused on training and evalualting models, almost
forgetting why we were doing this in the first place: to use them to
predict the label for new data that we did not annotate.

Of course, we could always re-train the model when we need to use it
-- but that has two downsides: first, as you may have seen, it may
actually take considerable time to train it, and second, you need to
have the training data available, which may be a problem both in terms
of storage space and of copyright and/or privacy if you want to share
your classifier with others.

Therefore, it makes sense to save both our classifier and our
vectorizer to a file, so that we can reload them later
(\refex{reuse}). Keep in mind that you have to re-use \emph{both}
-- after all, the columns of your feature matrix will be different (and hence, completely useless for the classifier) when
fitting a new vectorizer. Therefore, as you see, you do not do any fitting any more, and only use the |.transform()| method of the (already fitted) vectorizer and the |.predict()| method of the (already fitted) classifier.

In R, you have no vectorizer you could save -- but because in contrast to Python, both your DTM and your classifier include the feature names, it suffices to save the classifier only (using \verb+saveRDS(myclassifier, "myclassifier.rds")+) and using on a new DTM later on. You do need to remember, though, how you constructed the DTM (e.g., which preprocessing steps you took), to make sure that the features are comparable.

\pyrex[output=py, input=py, caption=Saving and loading a vectorizer and a classifier]{chapter12/reuse}



Another thing that we might want to do is to get a better idea of the
features that the model uses to arrive at its prediction; in our
example, what actually characterizes the best and the worst
reviews. \refex{eli5} shows how this can be done in one line of code
using \pkg{eli5} -- a package that aims to ``\emph{e}xplain [the model]
\emph{l}ike \emph{I}'m \emph{5} years old''. Here, we re-use the
|pipe| we constructed earlier to provide both the vectorizer and the
classifier to the eli5 -- if we would have only provided the
classifier, then the feature names would have been internal
identifiers (which are meaningless to us) rather than human-readable
words.

\pyrex[output=py, format=html, caption=Using ELI5 to get the most predictive features]{chapter12/eli5}

We can also use eli5 to explain how the classifier arrived at a
prediction for a specific document, by using different shades of green
and red to explain how much different features contributed to the
classification, and in which direction (\refex{eli5b}).

\pyrex[output=py, input=py, format=html, caption=Using ELI5 to explain a prediction]{chapter12/eli5b}


\todo[inline]{Include an example of RNN to text classification}

