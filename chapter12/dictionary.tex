\section{Dictionary approaches to text analysis}
\label{sec:dictionary}

A straightforward way to automatically analyze text is to compile a
list of terms you are interested in and simply count how often they
occur in each document. For example, if you are interested to find out
in how far mentions of political parties in news articles change over
the years, you only need to compile a list of all party names and
write a small script to count them.

Historically, this is how sentiment analysis was
done. \refex{sentsimple} shows how to do a simple sentiment analysis
based on a list of positive and negative words. The logic is
straightforward: you count how often each positive word occurs in a
text, you do the same for the negative words, and then determine which
occur more often.

\todo[inline]{The example now shows two very differnet approaches. We probably could find a NLTK function or sth to make the python-example more R-like ... but on the other hand, it seems (but maybe that's me (Damian)) more flexible and extensible to build this up as a for loop, especially if you eventually want to support multi-word expressions or regex or so. TO BE DECIDED...}


\pyrex[input=both, output=none, caption={Different approaches to a simple dictionary-based sentiment analysis: counting and summing all words using a for-loop over all reviews (Python) versus constructing a term-document matrix and looking up the words in there (R). Note that both approaches would be possible in either language.}]{chapter12/sentsimple}

As you may already realize, there are a lot of downsides to this
approach. Most notably, our bag-of-words approach does not allow us to
allow for negation: ``not good'' will be counted as
positive. Relatedly, we cannot handle modifiers such as ``very
good''. Also, all words are either positive or negative, while
``great'' should be more positive than ``good''. More advanced
dictionary-based sentiment analysis packages like \cite{vader},
\cite{sentistrength} include such functionality. Yet, as we will
discuss in Section~\ref{sec:supervised}, also these off-the-shelf
packages perform very poorly in many sentiment analysis tasks.

Still, there are many use cases where dictionary approaches work very
well. Because your list of words can contain anything, not just
positive or negative words. Dictionary approaches have been used, for
instance, to measure the use of racist words or swearwords in online
fora. Dictionary approaches are simple to understand and
straightforward, which can be a good argument for using them when it
is important that the method is no black-box but fully transparent
even for a layperson. Especially when the dictionary is already
existing or easy to create, it is also a very cheap method. This goes
at the expense, though, of their limitation to measuring easy to
operationalize concepts. To put it bluntly: it's great for measuring
the visibility of parties or organizations in the news, but it's not
good for measuring frames.

What gave dictionary approaches a bit of a bad name is that many
researchers applied them without validating them. This is especially
problematic when a dictionary is applied in a slightly different
domain than for which it was originally made.

If you want to use a dictionary-based approach, we advise the
following procedure:

\begin{enumerate}
  \item Construct a dictionary based on theoretical considerations and by closely reading a sample of example texts.
  \item Code some articles manually and compare with the automated coding.
  \item Improve your dictionary and check again.
  \item Manually code a validation dataset of sufficient size. The required size depends a bit on how balanced your data is -- if one code occurs very infrequently, you will need more data.
  \item Calculate the agreement. You could use standard intercoder reliability measures used in manual content analysis, but we would also advise you to calculate precision and recall (see Secion~\ref{sec:validation}).
  \end{enumerate}

Very extensive dictionaries will have a high recall (it becomes
increasingly unlikely that you ``miss'' a relevant document), but
often suffer from low precision (more documents will contain one of
the words even though they are irrelevant). Vice versa, a very short
dictionary will often be very precise, but miss a lot of documents.
It depends on your research question where the right balance lies, but
to substantially interpret your results, you need to be able to
quantify the performance of your dictionary-based approach.
