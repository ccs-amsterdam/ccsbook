\section{Dictionary approaches to text analysis}
\label{sec:dictionary}

A straightforward way to automatically analyze text is to compile a
list of terms you are interested in and simply count how often they
occur in each document. For example, if you are interested to find out
whether mentions of political parties in news articles change over
the years, you only need to compile a list of all party names and
write a small script to count them.

Historically, this is how sentiment analysis was
done. \refex{sentsimple} shows how to do a simple sentiment analysis
based on a list of positive and negative words. The logic is
straightforward: you count how often each positive word occurs in a
text, you do the same for the negative words, and then determine which
occur more often.


\pyrex[input=both, output=none, caption={Different approaches to a simple dictionary-based sentiment analysis: counting and summing all words using a for-loop over all reviews (Python) versus constructing a term-document matrix and looking up the words in there (R). Note that both approaches would be possible in either language.}]{chapter12/sentsimple}

As you may already realize, there are a lot of downsides to this
approach. Most notably, our bag-of-words approach does not allow us to
account for negation: ``not good'' will be counted as
positive. Relatedly, we cannot handle modifiers such as ``very
good''. Also, all words are either positive or negative, while
``great'' should be more positive than ``good''. More advanced
dictionary-based sentiment analysis packages like Vader \citep{Hutto2014}
or SentiStrength \citep{Thelwall2012} include such functionality. Yet, as we will
discuss in Section~\ref{sec:supervised}, also these off-the-shelf
packages perform very poorly in many sentiment analysis tasks,
especially outside of the domains they were developed for.
Dictionary-based sentiment analysis has been shown to be problematic
when analyzing news content \citep[e.g.][]{Gonzalez-Bailon2015,
  Boukes2019}. They are problematic when accuracy on the sentence
level is imporant, but may be satisfactory with longer texts for
comparatively easy tasks such as movie review classification
\citep{Reagan2017}, where there is clear ground truth data and the
genre convention implies that the whole text is evaluative and
evaluates one object (the film).


Still, there are many use cases where dictionary approaches work very
well. Because your list of words can contain anything, not just
positive or negative words. Dictionary approaches have been used, for
instance, to measure the use of racist words or swearwords in online
fora \citep[e.g.,][]{Tulkens2016}. Dictionary approaches are simple to understand and
straightforward, which can be a good argument for using them when it
is important that the method is no black-box but fully transparent
even without technical knowledge. Especially when the dictionary is already
existing or easy to create, it is also a very cheap method.
However, this goes at the expense of their limitation to only performing well when measuring easy to operationalize concepts. To put it bluntly: it's great for measuring the visibility of parties or organizations in the news, but it's not
good for measuring concepts such as emotions or frames.

What gave dictionary approaches a bit of a bad name is that many
researchers applied them without validating them. This is especially
problematic when a dictionary is applied in a slightly different
domain than for which it was originally made.

If you want to use a dictionary-based approach, we advise the
following procedure:

\begin{enumerate}
  \item Construct a dictionary based on theoretical considerations and by closely reading a sample of example texts.
  \item Code some articles manually and compare with the automated coding.
  \item Improve your dictionary and check again.
  \item Manually code a validation dataset of sufficient size. The required size depends a bit on how balanced your data is -- if one code occurs very infrequently, you will need more data.
  \item Calculate the agreement. You could use standard intercoder reliability measures used in manual content analysis, but we would also advise you to calculate precision and recall (see Secion~\ref{sec:validation}).
  \end{enumerate}

Very extensive dictionaries will have a high recall (it becomes
increasingly unlikely that you ``miss'' a relevant document), but
often suffer from low precision (more documents will contain one of
the words even though they are irrelevant). Vice versa, a very short
dictionary will often be very precise, but miss a lot of documents.
It depends on your research question where the right balance lies, but
to substantially interpret your results, you need to be able to
quantify the performance of your dictionary-based approach.

\begin{feature}\textbf{How many documents do you need to calculate agreement with human annotators?}
  To determine the amount of documents one needs to determine the agreement between a human and a machine, one can follow the same standards that are recommended for traditional manual content analysis.
  For instance, \citet{Krippendorff2004} provides a convenience table to look up the required sample sizes for determining the agreement between two human coders (p.~240). \citet{riffe2019} provide similar suggestions (p.~114ff). In short, the sample size depends on the level of statistical significance the researcher deems acceptable as well as on the distribution of the data. In an extreme case, if only 5 out of 100 items are to be coded as X, then in a sample of 20 items, such an item may not even occur. In order to determine agreement between the automated method and a human, we suggest to use sample sizes that one would also use for the calculation of aggreement between human coders. For specific calculations, we refer to content analysis books as the two referred here. To give a very rough ballpark figure (that shouldn't replace a careful calculation!), roughly 100 to 200 items will cover many scenarios.
  \end{feature}
