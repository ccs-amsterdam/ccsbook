\section{Deciding on the right method}
\label{sec:deciding}

When thinking about the computational analysis of texts, it is
important to realize that there is not \emph{the one} method to do
so. While there are good choices and bad choices, we also cannot say
that one method is necessarily and always superior to another. Some
methods are more fashionable than others (for instance, there has been
a growing interest in topic models (see \refsec{unsupervised}) in the
last years. There are indeed very good applications for such models,
they are also sometimes applied to research questions and/or data
where they make much less sense.  As always, the choice for method
should follow the research question and not the other way round. We
therefore caution to reading \refchap{text} selectively because you
want, for instance, learn about supervised machine learning or about
unsupervised topic models. Instead, you should be aware of very
different approaches to make an informed decision on what to use when.

\cite{Boumans2016} provide useful guidelines for this. They place
automatic text analysis approaches on a continuum from deductive (or
top-down) to inductive (or bottom-up). At the deductive end of the
spectrum, they place dictionary approaches
(\refsec{dictionary}). Here, the researcher has strong a priori
(theoretical) assumptions (for instance, which topics exist in a news
data set; or which words are positive or negative) and can compile
lists of words or rules based on these assumptions. The computer then
only needs to execute these rules. At the inductive end of the
spectrum, in contrast, lie approaches such as topic models
(\refsec{unsupervised}) where little or no a priori assumptions are
made, and where we exploratively look for patterns in the data. Here,
we typically do not know which topics exist in advance. Supervised
approaches (\refsec{supervised}) can be placed in between: Here, we do
define categories a priori (we do know which topics exist, and given
an article, we know to which topic it belongs), but we do not have
any set of rules: we do not know which words to look for or which
exact rules to follow. These rules are to be ``learned'' by the computer
from the data.

Before we get into details and implementations, let us discuss some
use cases of the three main approaches for the
computational analysis of text: dictionary (or rule-based) approaches,
supervised machine learning, and unsupervised machine learning.

Dictionary approaches excel under three conditions: First, the
variable we want to code is \emph{manifest and concrete} rather than
\emph{latent and abstract}: names of actors, specific physical
objects, specific phrases, etc., rather than feelings, frames, or
topics. Second, all synonyms to be included must be known
beforehand. And third, the dictionary entries must not have multiple
meanings.
For instance, coding how often gun control is mentioned in political
speeches fits these criteria. There are only so many ways to talk
about it, and it is rather unlikely that speeches about other topics
contain a phrase like ``gun control''. Similarly, if we want to find
references to Angela Merkel, Donald Trump, or any other well-known
politician, we can just directly search for their names -- even though
problems arise when people have very common surnames and are referred
to by their surnames only.

Sadly, most interesting concepts are more complex to code. Take a
seemingly straightforward problem: distinguishing whether a news
article is about the economy or not. This is really easy to do for
humans: there may be some edge cases, but in general, people rarely
need longer than a few seconds to grasp whether an article is about the
economy rather than about sports, culture, etc. Yet, many of these
articles won't directy state that they are about the economy by
explicitly using the word ``economy''.

We may think of extending our dictionary not only with |economi.*| (a
regular expression that includes economists, economic, and so on), but
also come up with other words like ``stock exchange'', ``market'',
``company.'' Unfortunately, we will quickly run into a problem that we also
faced when we dicussed the precision-recall tradeoff in
Section~\ref{sec:validation}: The more terms we add to our
dictionary, the more false positives we will get: articles about
the geographical space called ``market'', about some celebrity being seen
in ``company'' of someone else, and so on.

From this example, we can conclude that often (1) it is easy for
humans to decide to which class a text belongs, but (2) it is very
hard for humans to come op with a list of words (or rules) on which
their judgement is based.  Such a situation is perfect for applying
supervised machine learning: After all, it won't take us much time to
annotate, say, 1000 articles based on whether they are about the
economy or not (probably this takes less time than thoroughly
finetuning a list of words to in- or exclude); and the difficult part,
deciding on the exact rules underlying the decision to classify an
article as economic is done by the computer in seconds. Supervised
machine learning, therefore, have replaced dictionary approaches in
many areas.

Both dictionary approaches or rule-based approaches assume that you
know in advance which categories (positive vs. negative; sports
vs. economy vs. politics; \ldots) exist. The big strength of unsupervised
approaches such as topic models is that you can apply them also
without this knowledge. They therefore allow you to find patterns
in data that you did not expect and can generate new insights. This
makes them particularly suitable for explorative research questions.
Using them for confirmatory tests, in contrast, is less defensible:
After all, if we are interested in knowing whether, say, newssite A
published more about the economy than newssite B, then it would be
a bit weird to pretend not to know that the topic ``economy'' exists.
Also practically, mapping the resulting topics that the topic model
produces onto such a priori existing categories can be challenging.

Despite all differences, all approaches share one requirement: You
need to ``Validate. Validate. Validate'' \citep{Grimmer2013}. Though
it has been done in the past, simply applying a dictionary without
comparing the performance to manual coding of the same concepts
is not acceptable; neither is using a supervised machine learning
classifier without doing the same; or blindly trusting a topic model
without at least manually checking whether the scores the model assigns
to documents really capture what the documents are about.

