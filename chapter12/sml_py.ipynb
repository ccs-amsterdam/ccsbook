{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "snippet:readimdb"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# unpack the dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz and store the folder 'aclImdb' in the same folder as this script\n",
    "\n",
    "def read_data(dataset):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        for file in glob(os.path.join('aclImdb',dataset,label,'*.txt')):\n",
    "            with open(file) as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "X_train_fulltext, y_train = read_data('train')\n",
    "X_test_fulltext, y_test= read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "snippet:imdbbaseline"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10995  1505]\n",
      " [ 3003  9497]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.88      0.83     12500\n",
      "         pos       0.86      0.76      0.81     12500\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train_fulltext)\n",
    "X_test = vectorizer.transform(X_test_fulltext)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "snippet:basiccomparisons"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.77\n",
      "negative reviews:\t 0.79 \t\t 0.88\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.78\n",
      "negative reviews:\t 0.80 \t\t 0.88\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.85\n",
      "negative reviews:\t 0.85 \t\t 0.87\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.89 \t\t 0.88\n",
      "negative reviews:\t 0.88 \t\t 0.89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def short_classification_report (y_test, y_pred):\n",
    "    print(\"                \\t precision \\t recall\")\n",
    "    print(f\"positive reviews:\\t {metrics.precision_score(y_test,y_pred, pos_label='pos'):0.2f} \\t\\t {metrics.recall_score(y_test,y_pred, pos_label='pos'):0.2f}\")\n",
    "    print(f\"negative reviews:\\t {metrics.precision_score(y_test,y_pred, pos_label='neg'):0.2f} \\t\\t {metrics.recall_score(y_test,y_pred, pos_label='neg'):0.2f}\")\n",
    "\n",
    "configurations = [('NB with Count', CountVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "                 ('NB with TfIdf', TfidfVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "                 ('LogReg with Count', CountVectorizer(min_df=5, max_df=.5), LogisticRegression(solver='liblinear')),\n",
    "                 ('LogReg with TfIdf', TfidfVectorizer(min_df=5, max_df=.5), LogisticRegression(solver='liblinear'))]\n",
    "\n",
    "for description, vectorizer, classifier in configurations:\n",
    "    print(description)\n",
    "    X_train = vectorizer.fit_transform(X_train_fulltext)\n",
    "    X_test = vectorizer.transform(X_test_fulltext)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "snippet:basicpipe"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.77\n",
      "negative reviews:\t 0.79 \t\t 0.88\n",
      "\n",
      "\n",
      "NB with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.78\n",
      "negative reviews:\t 0.80 \t\t 0.88\n",
      "\n",
      "\n",
      "LogReg with Count\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.87 \t\t 0.85\n",
      "negative reviews:\t 0.85 \t\t 0.87\n",
      "\n",
      "\n",
      "LogReg with TfIdf\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.89 \t\t 0.88\n",
      "negative reviews:\t 0.88 \t\t 0.89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "for description, vectorizer, classifier in configurations:\n",
    "    print(description)\n",
    "    pipe = make_pipeline(vectorizer, classifier)\n",
    "    pipe.fit(X_train_fulltext, y_train)\n",
    "    y_pred = pipe.predict(X_test_fulltext)\n",
    "    short_classification_report(y_test, y_pred)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": [
     "snippet:gridsearchlogreg"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   26.6s\n",
      "/home/damian/.local/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 15.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these hyperparameters {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 2)}, we get the best performance:\n",
      "                \t precision \t recall\n",
      "positive reviews:\t 0.90 \t\t 0.90\n",
      "negative reviews:\t 0.90 \t\t 0.90\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(steps = [('vectorizer', TfidfVectorizer()), ('classifier', LogisticRegression(solver='liblinear'))])\n",
    "grid = {\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier__C': [0.01, 1, 100]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(estimator=pipe,\n",
    "                      param_grid=grid,\n",
    "                      scoring='accuracy',   # all classes are balanced, let's just score on accuracy\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,  # use all cpus\n",
    "                      verbose=10)\n",
    "search.fit(X_train_fulltext, y_train)\n",
    "print(f'Using these hyperparameters {search.best_params_}, we get the best performance:')\n",
    "print(short_classification_report(y_test, search.predict(X_test_fulltext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": [
     "snippet:vader"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0]\n",
      " [    6  6696  5798]\n",
      " [    5  1751 10744]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   dont know       0.00      0.00      0.00         0\n",
      "         neg       0.79      0.54      0.64     12500\n",
      "         pos       0.65      0.86      0.74     12500\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     25000\n",
      "   macro avg       0.48      0.47      0.46     25000\n",
      "weighted avg       0.72      0.70      0.69     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import vader\n",
    "\n",
    "analyzer = vader.SentimentIntensityAnalyzer()\n",
    "y_vader = []\n",
    "for review in X_test_fulltext:\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    if sentiment['compound']>0:\n",
    "        y_vader.append('pos')\n",
    "    elif sentiment['compound']<0:\n",
    "        y_vader.append('neg')\n",
    "    else:\n",
    "        y_vader.append('dont know')\n",
    "print(confusion_matrix(y_test, y_vader))\n",
    "print(classification_report(y_test, y_vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "snippet:reuse"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a great movie' probably belongs to class 'pos'.\n",
      "'I hated this one.' probably belongs to class 'neg'.\n",
      "'What an awful fail' probably belongs to class 'neg'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# let's save the vectorizer and classifier from the first example\n",
    "pickle.dump(vectorizer,open(\"myvectorizer.pkl\",mode=\"wb\"))\n",
    "joblib.dump(nb, \"myclassifier.pkl\")\n",
    "\n",
    "#Then, later on, instead of fitting a new vectorizer, you can simply load the old one and use it:\n",
    "\n",
    "listwithnewdata = ['This is a great movie', 'I hated this one.', 'What an awful fail']\n",
    "\n",
    "myvectorizer = pickle.load(open(\"myvectorizer.pkl\",mode=\"rb\"))\n",
    "new_features = vectorizer.transform(listwithnewdata)\n",
    "\n",
    "myclassifier = joblib.load(\"myclassifier.pkl\")\n",
    "predictions = myclassifier.predict(new_features)\n",
    "\n",
    "for review, label in zip(listwithnewdata, predictions):\n",
    "    print(f\"'{review}' probably belongs to class '{label}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "output:html",
     "snippet:eli5"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 83.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.173\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        great\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +6.101\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        excellent\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        best\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.791\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        perfect\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.20%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13663 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 13574 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.337\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        poor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -5.733\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        boring\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.315\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        waste\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -6.349\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        awful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -7.347\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        bad\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -9.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        worst\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "eli5.show_weights(pipe, top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:eli5b",
     "output:html"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=pos\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.777</b>, score <b>1.249</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.237\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"opacity: 0.80\">it is a </span><span style=\"background-color: hsl(120, 100.00%, 65.86%); opacity: 0.96\" title=\"0.229\">rare</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 80.76%); opacity: 0.87\" title=\"0.101\">fine</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.41%); opacity: 0.80\" title=\"-0.001\">spectacle</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 96.93%); opacity: 0.81\" title=\"-0.007\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.82%); opacity: 0.81\" title=\"0.020\">allegory</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 93.19%); opacity: 0.82\" title=\"0.023\">death</span><span style=\"opacity: 0.80\"> and transfiguration that is </span><span style=\"background-color: hsl(0, 100.00%, 75.46%); opacity: 0.90\" title=\"-0.143\">neither</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.90%); opacity: 0.82\" title=\"-0.029\">preachy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.75%); opacity: 0.82\" title=\"-0.030\">nor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.14%); opacity: 0.82\" title=\"-0.033\">mawkish</span><span style=\"opacity: 0.80\">. a </span><span style=\"background-color: hsl(120, 100.00%, 94.01%); opacity: 0.81\" title=\"0.019\">work</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 84.79%); opacity: 0.85\" title=\"0.072\">mature</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 97.07%); opacity: 0.80\" title=\"0.007\">courageous</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.32%); opacity: 0.80\" title=\"0.003\">insight</span><span style=\"opacity: 0.80\">, northfork </span><span style=\"background-color: hsl(120, 100.00%, 86.66%); opacity: 0.84\" title=\"0.060\">avoids</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.99%); opacity: 0.81\" title=\"-0.015\">arthouse</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.23%); opacity: 0.82\" title=\"0.028\">distinction</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.51%); opacity: 0.81\" title=\"-0.009\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.83%); opacity: 0.82\" title=\"0.035\">refusing</span><span style=\"opacity: 0.80\"> to </span><span style=\"background-color: hsl(0, 100.00%, 92.99%); opacity: 0.82\" title=\"-0.024\">belong</span><span style=\"opacity: 0.80\"> to a </span><span style=\"background-color: hsl(0, 100.00%, 90.80%); opacity: 0.82\" title=\"-0.035\">kind</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 79.48%); opacity: 0.88\" title=\"0.111\">unlike</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(120, 100.00%, 83.43%); opacity: 0.86\" title=\"0.082\">most</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.23%); opacity: 0.86\" title=\"0.083\">memorable</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 88.06%); opacity: 0.84\" title=\"0.051\">accomplished</span><span style=\"opacity: 0.80\"> film to </span><span style=\"background-color: hsl(120, 100.00%, 97.38%); opacity: 0.80\" title=\"0.006\">impose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.93%); opacity: 0.81\" title=\"-0.007\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.26%); opacity: 0.88\" title=\"-0.113\">obvious</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.62%); opacity: 0.80\" title=\"-0.002\">comparison</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 86.59%); opacity: 0.84\" title=\"-0.060\">wim</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.21%); opacity: 0.86\" title=\"-0.090\">wenders</span><span style=\"opacity: 0.80\">&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 92.66%); opacity: 0.82\" title=\"0.026\">1987</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.35%); opacity: 0.83\" title=\"0.043\">wings</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 90.96%); opacity: 0.82\" title=\"0.034\">desire</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(120, 100.00%, 91.95%); opacity: 0.82\" title=\"0.029\">der</span><span style=\"opacity: 0.80\"> himmel Ã¼ber </span><span style=\"background-color: hsl(120, 100.00%, 90.05%); opacity: 0.83\" title=\"0.039\">berlin</span><span style=\"opacity: 0.80\">), it </span><span style=\"background-color: hsl(120, 100.00%, 96.37%); opacity: 0.81\" title=\"0.009\">sustains</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.93%); opacity: 0.81\" title=\"-0.007\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.53%); opacity: 0.81\" title=\"-0.013\">ambivalence</span><span style=\"opacity: 0.80\"> in a </span><span style=\"background-color: hsl(120, 100.00%, 97.19%); opacity: 0.80\" title=\"0.006\">narrative</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.24%); opacity: 0.81\" title=\"0.018\">spectrum</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.53%); opacity: 0.80\" title=\"-0.003\">spanning</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.51%); opacity: 0.81\" title=\"0.009\">from</span><span style=\"opacity: 0.80\"> the </span><span style=\"background-color: hsl(0, 100.00%, 89.26%); opacity: 0.83\" title=\"-0.044\">mundane</span><span style=\"opacity: 0.80\"> to the </span><span style=\"background-color: hsl(0, 100.00%, 97.24%); opacity: 0.80\" title=\"-0.006\">supernatural</span><span style=\"opacity: 0.80\">. this </span><span style=\"background-color: hsl(120, 100.00%, 89.76%); opacity: 0.83\" title=\"0.041\">story</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(0, 100.00%, 98.26%); opacity: 0.80\" title=\"-0.003\">earthly</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 95.44%); opacity: 0.81\" title=\"0.013\">celestial</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.22%); opacity: 0.82\" title=\"0.023\">eminent</span><span style=\"opacity: 0.80\"> domains in the </span><span style=\"background-color: hsl(120, 100.00%, 87.69%); opacity: 0.84\" title=\"0.053\">american</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.64%); opacity: 0.80\" title=\"0.002\">west</span><span style=\"opacity: 0.80\"> withholds the </span><span style=\"background-color: hsl(120, 100.00%, 88.97%); opacity: 0.83\" title=\"0.046\">fairytale</span><span style=\"opacity: 0.80\"> literalness that </span><span style=\"background-color: hsl(0, 100.00%, 93.08%); opacity: 0.82\" title=\"-0.023\">marked</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.06%); opacity: 0.83\" title=\"0.039\">its</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.78%); opacity: 0.82\" title=\"0.035\">german</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.21%); opacity: 0.80\" title=\"0.001\">predecessor</span><span style=\"opacity: 0.80\"> in the </span><span style=\"background-color: hsl(0, 100.00%, 89.40%); opacity: 0.83\" title=\"-0.043\">ad</span><span style=\"opacity: 0.80\"> hoc </span><span style=\"background-color: hsl(120, 100.00%, 79.88%); opacity: 0.87\" title=\"0.108\">genre</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 94.96%); opacity: 0.81\" title=\"0.015\">angels</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.76%); opacity: 0.80\" title=\"0.002\">shedding</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.19%); opacity: 0.82\" title=\"0.023\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.35%); opacity: 0.83\" title=\"0.043\">wings</span><span style=\"opacity: 0.80\"> with obsequious </span><span style=\"background-color: hsl(0, 100.00%, 99.33%); opacity: 0.80\" title=\"-0.001\">sentimentalism</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 90.06%); opacity: 0.83\" title=\"0.039\">its</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.44%); opacity: 0.81\" title=\"0.013\">celestial</span><span style=\"opacity: 0.80\"> transcendence, be it </span><span style=\"background-color: hsl(120, 100.00%, 87.25%); opacity: 0.84\" title=\"0.056\">inspired</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.51%); opacity: 0.81\" title=\"-0.009\">by</span><span style=\"opacity: 0.80\"> doleful </span><span style=\"background-color: hsl(0, 100.00%, 96.31%); opacity: 0.81\" title=\"-0.010\">faith</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.71%); opacity: 0.83\" title=\"-0.041\">or</span><span style=\"opacity: 0.80\"> impelled </span><span style=\"background-color: hsl(0, 100.00%, 96.51%); opacity: 0.81\" title=\"-0.009\">by</span><span style=\"opacity: 0.80\"> a </span><span style=\"background-color: hsl(120, 100.00%, 93.31%); opacity: 0.82\" title=\"0.022\">fever</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.41%); opacity: 0.85\" title=\"0.075\">dream</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 95.34%); opacity: 0.81\" title=\"0.013\">never</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.28%); opacity: 0.83\" title=\"0.050\">parts</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.71%); opacity: 0.87\" title=\"0.102\">ways</span><span style=\"opacity: 0.80\"> with </span><span style=\"background-color: hsl(0, 100.00%, 94.12%); opacity: 0.81\" title=\"-0.019\">crud</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 89.48%); opacity: 0.83\" title=\"-0.043\">rot</span><span style=\"opacity: 0.80\">. this </span><span style=\"background-color: hsl(0, 100.00%, 94.36%); opacity: 0.81\" title=\"-0.018\">firm</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.24%); opacity: 0.80\" title=\"0.001\">grounding</span><span style=\"opacity: 0.80\"> redounds to </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.288\">great</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.30%); opacity: 0.81\" title=\"-0.014\">credit</span><span style=\"opacity: 0.80\"> for </span><span style=\"background-color: hsl(0, 100.00%, 82.94%); opacity: 0.86\" title=\"-0.085\">writers</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(0, 100.00%, 90.81%); opacity: 0.82\" title=\"-0.035\">directors</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.11%); opacity: 0.80\" title=\"0.007\">mark</span><span style=\"opacity: 0.80\"> and </span><span style=\"background-color: hsl(120, 100.00%, 93.68%); opacity: 0.81\" title=\"0.021\">michael</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.91%); opacity: 0.81\" title=\"-0.015\">polish</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_prediction(classifier, X_test_fulltext[0], vec=vectorizer)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
