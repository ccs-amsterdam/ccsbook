
\begin{feature}
  \textbf{How many topics?}
  With topic modeling, the most important researcher choices are the \concept[Number of LDA topics]{number of topics} and the value of \concept[Alpha LDA hyperparameter]{alpha}.
  These choices are called \concept{hyperparameters}, since they determine how the model parameters (e.g. words per topic) are found.

  Similar to supervised topic modeling (see \refsec{hyperparameters}),
  there is no good theoretical solution to determine the `right' number of topics for a given corpus and research question.
  Thus, a sensible approach can be to ask the computer to try many models, and see which works best.
  Unfortunately, and in contrast to supervised learning, because this is an unsupervised (inductive) method,
  there is no single metric that determines how good a topic model is.

  There are a number of such metrics proposed in the literature, of which we will introduce two.
  \concept{Perplexity} is a score of how well the LDA model can fit (predict) the actual word distribution
  (or in other words: how `perplexed' the model is seeing the corpus).
  \concept{Coherence} is a measure of how semantically coherent the topics are by checking how often the top token co-occur in documents in each topic \citep{minmo11}.

  \refex{ldacoherence} below show how these can be calculated for a range of topic counts, and the same code could be used for trying different values of \emph{alpha}.
  For both measures, lower values are better, and both essentially keep decreasing as you add more topics.
  What you are looking for is the \emph{inflection point} (or `elbow point') where it goes from a steep decrease to a more gradual decrease.
  For coherence, this seems to be at 10 topics, while for perplexity this is at 20 topics.

  There are two very important caveats to make here, however.
  First, these metrics are no substitute for human validation and the best model according to these metrics is not always the most interpretable or coherent model.
  In our experience, most metrics give a higher topic count that would be optimal from an interpretability perspective, but of course that also depends on how we operationalize interpretability.
  Nonetheless, these topic numbers are probably more indicative of a range of counts that should be inspected manually, rather than giving a definitive answer.

  Second, the code below was written so it is easy to understand and quick to run.
  For real use in a research project, it is advised to include a broader range of topic counts and also vary the $alpha$.
  Moreover, it is smart to run each count multiple times so you get an indication of the variance as well as a single point
  (it is quite likely that the local minimum for coherence at $k=10$ is an outlier that will disappear if more runs are averaged).
  Finally, especially for a goodness-of-fit measure like perplexity it is better to split the data into a training and test set
  (see \refsec{workflow} for more details).

\end{feature}

\pyrex[caption=Computing perplexity and coherence of topic models,output=r,format=png]{chapter12/ldacoherence}
