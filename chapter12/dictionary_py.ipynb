{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": [
     "snippet:chapter12install"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/wva/ccsbook/env/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: scikit-learn in /home/wva/ccsbook/env/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: pandas in /home/wva/ccsbook/env/lib/python3.8/site-packages (1.1.4)\n",
      "Requirement already satisfied: gensim in /home/wva/ccsbook/env/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: eli5 in /home/wva/ccsbook/env/lib/python3.8/site-packages (0.10.1)\n",
      "Collecting vader\n",
      "  Downloading vader-0.0.2-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex in /home/wva/ccsbook/env/lib/python3.8/site-packages (from nltk) (2020.10.28)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: attrs>16.0.0 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (20.3.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (0.8.7)\n",
      "Requirement already satisfied: jinja2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (2.11.2)\n",
      "Requirement already satisfied: graphviz in /home/wva/ccsbook/env/lib/python3.8/site-packages (from eli5) (0.15)\n",
      "Collecting sonopy\n",
      "  Downloading sonopy-0.1.2.tar.gz (3.3 kB)\n",
      "Requirement already satisfied: requests in /home/wva/ccsbook/env/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from jinja2->eli5) (1.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/wva/ccsbook/env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Building wheels for collected packages: sonopy\n",
      "  Building wheel for sonopy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sonopy: filename=sonopy-0.1.2-py3-none-any.whl size=2880 sha256=b5bacd97cbab06e61225e1dc5580940180e9cb4ac4276ab672a0e01d7148225e\n",
      "  Stored in directory: /home/wva/.cache/pip/wheels/1f/82/ee/3e858c78c0734f6fe30ade1bd3ef040c7f45eedae6669e88f8\n",
      "Successfully built sonopy\n",
      "Installing collected packages: sonopy, vader\n",
      "Successfully installed sonopy-0.1.2 vader-0.0.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk scikit-learn pandas gensim eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "snippet:chapter12library"
    ]
   },
   "outputs": [],
   "source": [
    "# General packages and dictionary analysis\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import bz2\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Supervised text classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import eli5\n",
    "import vader\n",
    "\n",
    "# Topic Modeling\n",
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "snippet:reviewdata"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from http://cssbook.net/d/aclImdb_v1.tar.gz\n",
      "Saving to 25000 training and 25000 test cases to reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "filename = \"reviewdata.pickle.bz2\"\n",
    "if Path(filename).exists():\n",
    "    print(f\"Using cached file {filename}\")\n",
    "    with bz2.BZ2File(filename, 'r') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "else:\n",
    "    url = \"http://cssbook.net/d/aclImdb_v1.tar.gz\"\n",
    "    print(f\"Downloading from {url}\")\n",
    "    fn, _headers = urllib.request.urlretrieve(url, filename=None)\n",
    "    t = tarfile.open(fn, mode=\"r:gz\")\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    for file in t.getmembers():\n",
    "        try:\n",
    "            _imdb, dataset, label, _fn = Path(file.name).parts\n",
    "        except ValueError:\n",
    "            # if the Path cannot be parsed, e.g. because it does not consist of exactly four parts, then it is not a part of the dataset but for instance a folder name. Let's skip it then\n",
    "            continue\n",
    "        if dataset == \"train\" and (label=='pos' or label=='neg'):\n",
    "            X_train.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "            y_train.append(label)\n",
    "        elif dataset == \"test\" and (label=='pos' or label=='neg'):\n",
    "            X_test.append(t.extractfile(file).read().decode(\"utf-8\"))\n",
    "            y_test.append(label)\n",
    "    print(f\"Saving to {len(y_train)} training and {len(y_test)} test cases to {filename}\")\n",
    "    with bz2.BZ2File(filename, 'w') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "snippet:sentsimple"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3, -4, 1, 3, -2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = set(requests.get('http://cssbook.net/d/positive.txt').text.split('\\n'))\n",
    "negative = set(requests.get('http://cssbook.net/d/negative.txt').text.split('\\n'))\n",
    "sentimentdict = {word:+1 for word in positive}\n",
    "sentimentdict.update({word:-1 for word in negative})\n",
    "\n",
    "scores = []\n",
    "mytokenizer = TreebankWordTokenizer()\n",
    "# we only take the first 100 reviews to speed things up\n",
    "for review in X_train[:5]:\n",
    "    words = mytokenizer.tokenize(review)\n",
    "    # we look up each word in the sentiment dict and assign its value (if we don't find it, it gets 0)\n",
    "    scores.append(sum(sentimentdict.get(word,0) for word in words))\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
