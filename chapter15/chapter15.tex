\chapter{Introduction to Multimedia Data}
\label{chap:image}

\begin{abstract}{Abstract}
 
Digitally collected data often does not only contain texts, but also audio, images and videos. Instead of using only textual features as we did in previous chapters, we could as well use, for instance, pixel values
to analyse images. Without going into detail, in this chapter we will show how to store, represent and convert multimedia data in order to use it as an input in our computational analysis. We will discuss how to obtain text from images (OCR) and audio (speech-to-text), which allows to apply text analytics to  multimedia data. We will then focus on image analysis using machine learning classification techniques based on deep learning, and will explain how to build (or fine-tune) a Convolutional Neural Network (CNN) by ourselves, or to use a commercial service (Amazon Rekognition) to conduct this task. 
\end{abstract}

\keywords{Image, Audio, Video, Multimedia, Image Classification, Deep Learning}

\begin{objectives}
\item Learn how to transform multimedia data into useful inputs for computational analysis
\item Understand how to conduct deep learning to automatic classification of images
\end{objectives}

\begin{feature}
This chapter uses \pkg{PIL} (Python) and \pkg{imagemagic} (generic) to convert pictures as inputs; and \pkg{tesseract} (generic) and  Google 'Cloud Speech' API (\pkg{googleLanguageR} in and and \pkg{google-cloud-language} in Python) to convert into text images or audio files, respectively. We will use \pkg{Tensorflow} and \pkg{keras} (both in Python and R) to build and fine-tune CNNs.
\end{feature}

\input{chapter15/storing}
\input{chapter15/ocr}
\input{chapter15/cnn}
\input{chapter15/apivision}