\chapter{Multimedia data}
\label{chap:image}

\begin{abstract}{Abstract}
Digitally collected data often does not only contain texts, but also audio, images and videos. Instead of using only textual features as we did in previous chapters, we could as well use, for instance, pixel values
to analyse images. First, we will see how to use existing libraries, commercial services or API's to conduct multimedia analysis (i.e. optical character recognition, speech-to-text or object recognition). Then we will show how to store, represent and convert image data in order to use it as an input in our computational analysis.  We will focus on image analysis using machine learning classification techniques based on deep learning, and will explain how to build (or fine-tune) a Convolutional Neural Network (CNN) by ourselves. 
\end{abstract}

\keywords{Image, Audio, Video, Multimedia, Image Classification, Deep Learning}

\begin{objectives}
\item Learn how to transform multimedia data into useful inputs for computational analysis
\item Understand how to conduct deep learning to automatic classification of images
\end{objectives}

\begin{feature}
This chapter uses \pkg{tesseract} (generic) and  Google 'Cloud Speech' API (\pkg{googleLanguageR} and \pkg{google-cloud-language} in Python) to convert into text images or audio files, respectively. We will use \pkg{PIL} (Python) and \pkg{imagemagic} (generic) to convert pictures as inputs; and \pkg{Tensorflow} and \pkg{keras} (both in Python and R) to build and fine-tune CNNs. 

You can install these and other auxillary packages with the code below if needed  (see \refsec{installing} for more details):

\doublecodex{chapter15/chapter15install}

\noindent After installing, you need to import (activate) the packages every session:

\doublecodex{chapter15/chapter15library}

\end{feature}





\input{chapter15/beyond}
\input{chapter15/apivision}
\input{chapter15/storing}
\input{chapter15/cnn}
