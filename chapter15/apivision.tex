\section{Using existing libraries and APIs}
\label{sec:apivisions}

In the last sections we have shown you how to deal with multimedia contents with special attention to image classification using state-of-the-art libraries. With these tools you can build from scratch most of the analyses and also use some pre-defined models to execute classification tasks. However, given its complexity, it might happen that you decide to use other existing libraries that directly implements multimedia analysis or to connect to commercial services to deploy certain tasks remotely using their APIs. There is a vast variety of available libraries and APIs, which we cannot cover in this book, but we will briefly mention some of them that may be useful in the computational analysis of communication.

One example in the field of visual analytics is the \textit{optical character recognition} (OCR). It is true that you can train your own models to deploy multi-class classification and predict every letter, number or symbol in an image, but it will be task that will take you a lot of effort. Instead, there are specialized libraries in both R and Python such as \pkg{tesseract} that deploy this task in seconds with high accuracy. It is still possible that you will have apply some pre-processing to the input images in order to put them on shape. This means that you may need to use packages such as \pkg{PIL} or \pkg{Magick} to improve the quality of the image by cropping it or by reducing the background noise.  In the case of PDFs files you will have to convert the first into images and then apply OCR.

In the case of more complex audio and image documents you can use more sophisticated services provided by private companies (i.e. Google, Amazon, Microsoft, etc.). These commercial services have already deployed their own machine learning models with very good results. Some times you can even customize some of their models, but as a rule their internal features and configuration are not transparent to the user. Moreover, these services offer friendly APIs and normally a free quota to deploy your first exercises.

To work with audio files, many social researchers might need to convert long conversations, radio programs or interviews to plain text. For this propose, \textit{Google Cloud} offer the service \textit{Speech-to-Text}\footnote{https://cloud.google.com/speech-to-text}  that remotely transcribe the audio to a text format supporting multiple languages (more than 125!). With this service you can remotely use the advanced deep learning models created by Google Platform from your own local computer (you must have an account and connect with the proper packages such as \pkg{googleLanguageR} or \pkg{google-cloud-language} in Python).

If you apply either OCR to images or Speech-to-Text recognition to audio contents you will have juicy plain text to conduct NLP, sentiment analysis, topic modelling, among other techniques.  Thus, it is very likely that you will have to combine different libraries and services to perform a complete computational pipeline, even jumping from R to Python, and vice versa!

Finally, we would like to mention the existence of commercial services of \textit{autotaggers}, such as Google's Cloud Vision, Microsoft's Computer Vision or Amazon's Recognition. For example, if you connect to the services of Amazon's Recognition you can not detect and classify images, but conduct sentiment analysis over faces or predict sensitive contents within the images. As in the case of Google Could, you will have to obtain commercial credentials to be able to connect to Amazon's Recognition API (also with a free initial quota). This approach has to main advantages. The first is the access to a very well trained and validated model (continuously re-trained) over millions of images and with the participation of thousands of coders. The second is the scalability because you can store and analyse images at scale at a very good speed using cloud computing services.

As an example, if you use Amazon's Recognition to detect objects in our lifeboat picture (\texttt{my\_image2\_RGB}) you will obtain a set of accurate labels: \textit{Clothing} (99.95\%), \textit{Apparel} (99.95\%), \textit{Human} (99.70\%), \textit{Person} (99.70\%), \textit{Lifejacket} (99.43\%) and \textit{Vest} (99.43\%). With a lower confidence you will also find labels such as \textit{Coat} (67.39\%) and \textit{People} (66.78\%). If you compare these results to our original classification model trained with ImageNet you will find that there are new different classes with higher accuracies that help us to obtain better predictions. Moreover, the time to conduct the classification is shorter, since we don't have to train or choose a model. As you may imagine, you cannot neither modify the commercial models nor have access to their internal details, which is a strong limitation if you want to build your own and customized classification system.

So far we have covered the main techniques, methods and services to analyse multimedia data, in special images. It is up to you to choose which library or service to use, and you will find most of the in R and Python, using the basic concepts explained in this chapter. If you are interested in deepen in computational analysis of multimedia contents, we encourage you explore this emerging and exciting field of expertise given the enormous importance it will have in the near future. 