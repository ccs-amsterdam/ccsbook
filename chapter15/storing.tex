\section{Storing, representing, and converting multimedia data}
\label{sec:storing}

As you may have imagined, a book about computational analysis of \textit{communication} would be incomplete without a chapter dedicated to conduct analysis over other unstructured data different from texts. In fact, if you think of the possible contents derived from social, cultural and political dynamics in the current digital landscape, you will realize that written contents are only a limited (though potential) slide of the bigger cake. Humans produce much more oral contents than text messages, and are more agile in deciphering sounds and visual contents. Digitalization of social and political life, as well as the explosion of self-generated digital contents in the web and social media, have provoked an unprecedented amount of multimedia contents that deserve to be include in many types of research.

Just imagine a collection of digital recorded radio stations, or the enormous amount of pictures produced everyday in Instagram, or even the millions of videos of social interest uploaded in Youtube. These are definitely goldmines for social researchers who traditionally used manual techniques to analyse just a very small portion of these multimedia contents. However, it is also true that computational techniques to analyze audio, images or video are still little developed in social sciences given the the difficulty of application for non-computational practitioners and the novelty of the discoveries in fields such as computer vision. 

This section gives a brief overview over different formats of multimedia files, in special images, vector graphics, bitmaps, and related concepts. We explain how to generate useful inputs into our pipeline to perform computational analysis.

You are already probably familiar with digital formats of images (.jpg, .bmp, .gif, etc.), audio (.mp3, .wav, .wma, etc.) or video (.avi, .mov, .wmv, .flv, etc.), which is the very first step to use these contents as input. However, similarly to the case of texts you will need to do some preprocessing to put these formats into shape and get a proper mathematical representation of the contents. 

In the case of audio, there might be many useful computational approaches to do research over these contents: from voice recognition, audio sentiments or sound classification, to automatic generation of music. Recent advances in the field of artificial intelligence have created a prosperous and diversified field with multiple academic and commercial applications. Nevertheless, computational social scientists can obtain great insights just by using specific applications such as speech-to-text transformation and then apply text analytics (already explained in this book) to the results. As you will see in the next section (XXXX), there are some useful libraries in R and Python to use pre-trained models to transcribe voice in different languages. Even when this approach is quite limited (just a small portion of the audio analytics world) and constrained (we will not address how to create the models), it will show how a specific, simple and powerful application of the automatic analysis of audio inputs can help answering many social questions (i.e. what are the topics of a natural conversation, what are the sentiments expressed in the scripts of radio news pieces, or which actors are named in oral speeches of any political party?).

On the other hand, applying computational methods to video inputs is probably the most challenging task in spite of the recent and promising advances in computer vision. For the shake of space, we will not cover specific video analytics in this chapter, but it is important to let you know that most of the computational analysis of video is based on the inspection of image and audio contents. With this standard approach you need to specify which key frames you are going to extract from the video (for example take an still image every 1,000 frames) and then apply computer vision techniques (such as object detection) to those independent images. Check for example the the version 3 of the object detection architecture You Only Look Once Take (YOLOv3)\footnote{https://pjreddie.com/darknet/yolo/} created by \citet{redmon2018yolov3}, which uses a pre-trained Convolutional Neural Network (CNN) (see section XX) to locate objects within the video (fig XXX). For answer many social science questions you might complement this frame to frame analysis with a basic analysis of audio features (what is being said in the video). In any case, this approach will not cover some interesting aspects of the video such as the camera frame shots and movements, or the editing techniques, which certainly give more content information.








we will show how we could substitute our text vectorizer in our examples above by PIL (Python) or imagemagick (generic) to use pictures as input



challenge
The bases of computational video analysis still relies on the specific analysis of images and audio.