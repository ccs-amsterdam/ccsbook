\section{Re-using code: How not to re-invent the wheel}
\label{sec:code}

Just as any human language, also programming languages consist of a vocabulary, syntax rules, and expression. Using the proper words and grammar, you can build from scratch any idea your imagination allows you. That's a wonderful thing! But, let's be honest: The language itself, the expressions, ideas, and all the abstract constructs seldom come originally from you. In fact, that's great as well: Otherwise, you'd have to deeply think of every element before talking and expressing any thought. Instead, you use pre-existing rules, ideas, perceptions and many different narratives to create your own messages to interact with the world. It's the same with coding: You never start from scratch.

Of course you \emph{can} code anything you want from the very beginning, even just using 0's and 1's!
When reading through the previous chapters, maybe you even started to think that complex operations will be very exhausting and will take a really a long time. After all, from the basic operations we did to a useful statistical model seems like a long way to go.

Luckily, this is not the case.
There is almost no project in which computational scientists, data analysts, or developers do not re-use earlier code in order to achieve their goals quicker and more efficiently.
The more common a task is, the greater the chance that you do not have to re-invent the wheel.
Of course, you have to give credit where credit is due, but it is not uncommon to paste code snippets from others into your own code and adapt them. This is especially true for standard operations, for which there are only so-many ways to do achieve the desired result.

There are different ways to re-use earlier code. One is to copy and adapt raw lines of code written by someone else or by yourself in the past. In fact, there many online repositories such as Github or BitBucket that contain many programs and well-documented code examples (see \refsec{practices}). When conducting computational analyses, you will spend a significant part of your time in some such repositories trying understand what others have done and figuring out how can use it your own work.

Another way is to build or import functions that summarizes many lines of code into a simpler command, as we explained in \refsec{functions}. The functions are indeed powerful strategies to re-use the code since you do not have to write over and over again complex lines of code that have to be used in different moments. Packages are probably the most elegant approach to recycle the work made by other colleagues. In \refsec{installing} you already learnt how to install a package and you did probably noticed how easy it is to bring many pre-build functionalities onto your workspace. You can also write and publish your own package in the future to help your colleagues to write less code and to be more efficient in their daily job (see also \refsec{publishingsource})!

A lot of questions can arise here: What to re-use? When to use a function written by someone else instead of writing the code yourself? Which scripts and sources are trustworthy? Which is the best package to choose? How many packages should we use within the same project? Should we care about package versions? And must we know every package that is released in our field? There are of course multiple answers to these questions and it will be probably a matter of practice how to obtain the most appropriate ones. In general, we can say that one premise is to re-use and share code as much as you can. This idea is limited by constraints of quality, availability, parsimony, update and expertise. In other words, when recycling code we should think of the reputation of the source, the difficulty to access it, the risk of having an excessive and messy number of inputs, the need to use the last developments with your colleagues and the fact that you will not be able to know-it-all.

Let's take an example. Imagine you want to compute the Levenshtein distance between two
strings. That's a pretty straightforward metric that answers the question: ``How many
edits (removing/changing/editing characters) do I need to transform string1 into string2?''.
It can be used for plagiarism detection, but may be interesting for us to determine, for instance,
whether a newspaper copied some content from somewhere else, even if small changes have been
applied. You could now try to write some code to calculate that (and we are sure you could
do that if you invested some time in it!), but it is such a common problem that it
has been solved multiple times before. You could, for instance,  look up some
functions that are known to solve the problem and copy-paste them into your code. You can find
a large number of different implementations for both Python and R here:
\url{https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein\_distance}.
You can then choose and copy-paste the one which one is most appropriate for you. One that is very fast, because
you want to compare a huge set of strings? One that is easy to understand? One that uses
only a few lines of code to not distract the reader?
Alternatively, if you look for available packages for Python and R, you see that there are
multiple packages that you can install with \fn{install.packages} (R) or \fn{pip} (Python)
and then import. If you go for that route, you don't need
to care about the internal workings and can ``abstract away'' and outsorce the problem -- on the other
hand, the users of your code now have one more dependency to install before they can
use your code.

In the case of package selection, we understand it can be quite overwhelming,
with so many different packages from different contributors.
In fact, sometimes the same task, such as topic modeling,
can be done using multiple different topics.
So, how to find and choose the best package?
Besides resources like this book, the most important guide is probably the community around you:
using packages that a lot of other people also use mean that the package is probably well maintained and documented,
and that there is a community to ask for help if needed.
Since all packages are free to download and install, however, you can also shop around and see what the various packages do.
When comparing different packages, it is always good to check their documentation and their github page:
packages that are well documented and that are updated frequently are often a good choice.

Just to mention an example, the authors of this book had several intensive discussions of which packages to mention and use in the proposed exercises, an issue that became complex given the variety of topics addressed in the handbook. In the case of text analysis, a library such as \fn{NLTK} for Python was incredibly popular among computational analysts until few years ago becoming a package of reference in the field, but it has -- at least for some applications -- been overpassed by friendly and sophisticated new packages for natural language processing like \fn{SpaCy}. So, which should we have included in this book? The one which is well-known (with excellent documentation by the way) and still used by thousands of practitioners and students around the world?, or the one which is penetrating the market because of its easiness and advantages? Moreover, by choosing the second option, are we sure a more trendy package is going to be stable in time or is it going to be crossed out by a different one in just few months?  

There isn't the one golden way of how to re-use code and packages, but this dynamic scenario also depicts an exciting and provocative field that forces us to keep ourselves updated.
