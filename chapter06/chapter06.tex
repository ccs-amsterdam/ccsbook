%\chapter{Introduction to data frames}
\chapter{From file to data frame and back}
\label{chap:filetodata}

%\section{Data formats: data frames, matrices, and tibbles}
%\subsection{Arranging data}

\section{Why and when do we use data frames?}

In Section~\ref{sec:datatypes}, we introduced basic data types: strings (which contain text), integers (which contain whole numbers, or numbers without anything 'behing the dot'), floats (floating point numbers; numbers with decimals), and bools (Boolean values, True or False). 
We also learned that a series of multiple values (e.g., multiple integers, multiple strings) can be stored in what we call a vector (R) or a list (Python).

In most social-scientific applications, however, we do not deal with isolated series of values. We rather want to link multiple values to each other. One way to achieve this are dictionaries (see Section~\ref{sec:datatypes}).
Such data structures are really useful for nested data: For example, if we would not want to store people's age but their addresses, we could store a dict within a dict.
%\rpyex{chapter06/snippets/listvsdict}{CAPTION}
In fact, as we will see later in this chapter, many data that computational social scientists come in such a format. For instance, a tweet consists of the tweet text (a string), the number of likes (an integer), a list of hashtags it contains, and so on.

But ultimately, for many social-scientific analysis, a tabular data format is preferred. We are used to think of observations (cases) as rows in a table, and as columns containing different things that are measured (e.g., age, gender, days per week of newspaper reading, ...). It also simplifies how we can run many statistical analyses later on.

We could simply construct a list of lists to achieve such a tabular data format.
In fact, this list-of-lists technique is often used to store tabular data or matrices, and you will probably encounter it in some examples in this book or elsewhere. The list-of-lists approach is very low-level, though: If we wanted to, for instance, insert a column or a row at a specific place, writing the code to do so can be cumbersome. There are also no things like column headers, and no consistency checks: nothing would warn us if one row actually contained more 'columns' than another, which should not be the case in a rectangular table.

To make our lifes easier, we can therefore use a data structure called a data frame. 
Data frames can be generated from list-of-list structures, from dictionaries, and many others.
One way of doing so is shown in \refex{createdataframe}, but very often, you'd rather read data from a file or an online resource directly into a dataframe (see Section~\ref{sec:reading}).

\pyrex[output=both, caption=Creating a dataframe from other datastructures]{chapter06/createdataframe}

In this book, we use dataframes a lot, because they are very convenient for handling tabular data, and because they provide a lot of useful convenience functionality, instead of requiring us to re-invent the wheel all the time. In the next section, we will discuss some of them.

Of course, there are some situations when dataframes are \emph{not} a good choice to organize your data:
\begin{itemize}
\item Your data is one-dimensional. Think, for example, of ressources like a list of stopwords, or a list of texts without any meta-information.
\item Your data do not have a tabular structure. Think, for example, of deeply nested data, or of very messy data.
\item Your data is so large that you cannot (or do not want to) load it into memory. For instance, if you want to process the text of all articles on the wikipedia, you probably want to process them one-by-one instead of loading all articles at the same time.
\end{itemize}

%\subsection{Basic operations on data frames}

%When retrieving data for external sources (Section~\ref{sec:gathering}), we can convert the data we retrieved into a dataframe using the techniques outlined in the previous paragraphs. In many other cases, we will read them directly from files instead (Section~\ref{sec:reading}).

Therefore, you will come across (and we will introduce you to) examples in which we do \emph{not} use data frames to organize our data.
But in most cases we will, because they make our life easier:
Once we constructed our data frame, we have a range of handy functions at our disposal, that allow us to select rows or columns, add new rows or columns, apply functions to them, and so on.
We will discuss these in Chapter~\ref{chap:datawrangling}.

But how do we -- toy examples like those in \refex{createdataframe} aside -- get data into and out of dataframes? 




%Table~\ref{tab:dataframecommands} gives an overview of some of them. Many of these commands will come back in subsequent chapters, but we encourage you to already play around a bit with them. That is more fun with a real dataset - and that's why we will load some in the next section.


%\begin{table}[]
%\caption{Basics of data frame handling}
%\label{tab:dataframecommands}
%\begin{tabularx}{\textwidth}{XXXXX}
%\toprule
%                         & pandas data frame                 & R data.frame & R tibble \\ \midrule
%select rows by index     & df.iloc{[}1, ...... ..            &              &          \\
%select columns by number & df.iloc...                        &              &          \\
%select columns by name   & df{[}'mycolumn'{]} or df.mycolumn &              &          \\ \bottomrule
%\end{tabularx}
%\end{table}




\section{Reading and saving data}
\label{sec:reading}

\subsection{The role of files}

In statistical software like SPSS or Stata, or in all typical office applications, you \emph{open} a file, do some work on it, and then \emph{save} the changes to the same file once you are done. You basically ``work on that file''.

That's not how your typical workflow in R or Python looks like. Here, you work on one or multiple data frames (or some other data structures). That means that you might start by \emph{reading} the contents of some file into a data frame, but once that is done, there is no link between the dataframe and that file any more. Once your work is done, you can save your dataframe to a file, of course, but it is a good practice not to overwrite your input file, so that you can always go back to where you started. A typical workflow would rather look like this:
\begin{enumerate}
\item Read raw data from file "myrawdata.csv" into data frame "df"
\item Do some operations and analyses on df
\item Save df to file "myfinaldata.csv"
\end{enumerate}
Note that the last step is not even necessary, but may be handy if running the script takes very long, or if you want to re-distribute the resulting file.

The format in which we read files into a data frame and the format to which we save our final data frame also by no means needs to be identical. We can, for example, read data created by someone else in Stata's proprietary .dta format into a dataframe and later save it to a .csv table.

While we sometimes do not have the choice in which format we get our input data, we have a range of options regarding our output data. We usually prefer formats that are \emph{open} and \emph{interoperable} for this, which ensures that they can be used by as many people as possible, also in the future, and that they are not tied to any specific (proprietary) product.

The most common file formats that are relevant to us are listed in Table~\ref{tab:fileformats}. txt files are particularly useful for long texts (think of one file containing one newspaper article or even a whole book), but they are bad for storing associated meta data. csv files are the default choice for tabular data, and json files allow us to store nested data in a dictionary-like format (see above). 

For the sake of completeness, we also listed the native Python and R formats pickle, RDS, and RDA. Because their lack of interoperability, they are not very suitable for long-term storage or for sharing data, but they can have a place in a workflow as an intermediate step to solve the issue that none of the other formats are able of storing all properties of a dataframe (e.g., the csv file cannot store whether a given column in an R dataframe is to be understood as containing strings such as 'man', 'woman', 'non-binary' or a factor with the three levels man, woman, non-binary). If it is of importance to store an object (such as a dataframe) exactly as-it-is, we can use these formats. 

\begin{table}[]
\caption{Basics of data frame handling}
\label{tab:fileformats}
\begin{tabular}{@{}llll@{}}
\toprule
        & Used for?             & open   & interoperable?\\ \midrule
txt     & plain text            &yes & yes            \\
csv     & tabular data          & yes & yes            \\
json    & nested data, key-value pairs   & yes & yes             \\ 
pickle  & Python objects        & yes & no     \\ 
RDS/RDA & R objects             & yes & no \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Encodings and dialects}
\label{sec:encodings}
txt files, csv files, and json files are all files that are based on text. Unlike binary file formats, you can read them in any text editor. Try it yourself to understand what is going on under the hood. 

Download the example files from XXXX and open them in a text editor of your choice (for example, Notepad++, Atom, emacs, .... [offer some choice here]). 
As you will see (Figure~\ref{fig:csv-in-editor}), a csv file internally just looks like a bunch of text in which each line represents a row and in which the columns are separated by a comma (hence the name comma seperated values (csv)).

Looking at the data in a text editor is a very good way to find out what happens if reading your files into a data frame does not work as expected - which can happen more frequently than you would expect.

Mostly due to historical reasons, not every text based file (which, as we have seen, includes csv files) is internally stored in the same way.

For a long time, it was common to \emph{encode} in such a way that one character mapped to one byte. That was easy from a programming perspective (after all, the n-th character of a text can directly be read from and written to the n-th byte of a file) and also storage-efficient. But given that a byte consists of 8 bits, that means that there are only 256 possible characters. All letters in the alphabet in uppercase, again in lower case, numbers, punctuation, some control characters - and you are out of characters. Due to this limitation, there were different encodings or codepages for different languages that told a program which value should be interpreted as which character.

We all know the phenomenon of garbled special characters, like German umlauts or Scandinavian characters like ø, å, or œ being displayed as something completely different. In these cases, a different encoding was used for saving them than for reading them.

In principle, this issue has been solved due to the advent of Unicode. Unicode allows to handle all characters from all scripts, including emoticons, Korean and Chinese characters, and so on. The most popular encoding for Unicode characters is called UTF-8, and it has been around for decades. 

To avoid any data loss, it is advisable to make sure that your whole workflow uses UTF-8 files. By far most modern applications support UTF-8, even though some still by default use a different encoding (e.g., 'Windows-1252') to store data. As Figure~\ref{fig:csv-in-editor} illustrates, you can use a text editor to find out what encoding your data has, and many editors also offer an option to change the encoding. However, you cannot recover what has been lost (e.g., if at one point you saved your data with an encoding that only allows 256 different characters, it follows logically that you cannot recover that information).


\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figures/ch6_csv-in-editor}
\caption{A csv file opened in a text editor, illustrating that the columns are separated by commas, and showing the encoding and the line endings.}
\label{fig:csv-in-editor}
\end{figure}

As we will show in the practical code examples below, you can also force Python and R to use a specific encoding, which can come in handy if your data arrives in a legacy encoding.

Related to the different encodings a file can have, but less problematical, are different conventions of how a \emph{line ending} is denoted. Windows-based programs have been using a Carriage Return followed by a Line Feed (denoted as \texttt{\textbackslash r\textbackslash n}), very old versions of MacOS used a Carriage Return only (\texttt{\textbackslash r}), and newer versions of MacOS as well as Linux use a Line Feed only (\texttt{\textbackslash n}). In our field, the Linux (or Unix) style line endings have become most dominant, and Python 3 even automatically converts Windows style line endings to Unix style line endings when reading a file - even on Windows itself.

A third difference is the use of so-called \emph{byte-order markers} (BOM). In essence, a BOM is an additional byte added to the beginning of a text file to indicate that it is a utf-encoded file and to indicate in which order the bytes are to be read (the so-called endianness). While informative, this can cause trouble if your program does not expect that byte to be there. In that case, you might either want to remove it or explicitly specify the encoding as such (e.g., 'utf-8-bom' instead of 'utf-8' in the examples below).


In short, the most standard form in which you probably want to encode your data is in UTF-8 with Linux-style line endings without the use of a byte-order marker.


In the case of reading and writing csv files, we thus need to know the encoding, and potentially also the line ending conventions and the presence of a byte-order marker. However, there are also some additional variations that we need to consider. There is no definite definition of how a csv file needs to look like, but there are multiple dialects that are widely used. They mainly differ in to aspects: the delimiter that is chosen, and the quoting an/or escaping of values.

First, even though csv stands for comma separated values, one could use other characters instead of a comma to separate the columns. In fact, because many countries use a comma instead of a dot to as a decimal separator (\$10.30 vs 10,30€), in many countries a semicolon (';')is used instead of a comma as column delimiter. To avoid the possible confusion, others use a tab character (\texttt{\textbackslash t}) to seperate columns. Sometimes, these files are then called a tab-seperated file, and instead of .csv, they may have an ending such as .tsv, .tab, or even .txt. However, this does not change the way how you can read them - but what you need to know is whether your columns are seperated by \texttt{,}, \texttt{;}, or \texttt{\textbackslash t}. 

Second, there may be different ways of how to deal with strings as values in a csv file. For instance, it may be that a specific value contains the same character that is also used as a delimiter. These cases are usually resolved by either putting all strings into quotes, putting only strings that contain such ambiguities in quotes, or by prepending the ambigous character with a specific escape character. Most likely, all of this is just handled automatically under the hood, but in case of problems, you might want to look into this and check out the documentation of the packages you are using on how to specify which strategy is to be used.

Let's get practical and try out reading and writing files into a data frame (\refex{readfiles}).

\pyrex[output=none, caption=Reading files into a dataframe]{chapter06/readfiles}


\subsection{File handling beyond data frames}
Dataframes are a very useful data structure for organizing and analyzing data, and will come back in many examples in this book.
However, not all things that we might want to read from a file needs to go into a dataframe.
Imagine if we have a list of words that we later want to remove from some texts (so-called stopwords, see CHAPTERXXXXXXXXXXX).
We could make a list (or vector) of such words directly in our code. 
But if we have more than a couple of such words, it is easier and more readable to keep them in an external file. We could create a file \texttt{stopwords.txt} in a text editor with one of such words per line:

\begin{lstlisting}
and
or
a
an
\end{lstlisting}


Then, we could direclty read this file into a vector or list  (see \refex{stopwords}).

\pyrex[output=none, caption=Reading files without dataframes]{chapter06/stopwords}


\pyrex[output=both, caption=More examples for reading from and writing to files]{chapter06/extendedfilehandling}

\refex{extendedfilehandling} provides you with some more elaborate code examples that allows us to dig a bit deeper into the general way of handling files.

In the Python example,  we can open a file and assign a handle to it that allows us to refer to it (the name of the handle is arbitrary, let's just call it \texttt{f} here.
Then, we can use a for loop to iterate over all lines in the file and add it to a list

The \texttt{mode = 'r'} specifies that we want to read from the file. \texttt{mode = 'w'} would open the file for writing, create it if necessary, and immediately deletes all content that may have been in there if the file already existed (!).
Note that the \texttt{.strip()} is necessary to remove the line ending itself, and also possible whitespace at the beginning or end of a line.
If we wanted to save our stopwords, we could do this in a similar way: We first open the file (this time, for writing), and then use the file handle's methods to write to it.
We are not limited to plain text files, here. For instance, we can use the same approach to read json files into a python dict or to store a python dict into a json file.

We could also combine this with a for loop that goes over all files in a dictionary.
Imagine we have a folder full of positive movie reviews, and another one full of negative movie reviews that we want to use to train a machine learning classifier (see CHAPTERXXXXXXXXXX). Let's further assume that all these reviews are saved as \texttt{.txt} files.
We can iterate over all of them, as shown in \refex{extendedfilehandling}







\section{Gathering data from online sources}
\label{sec:gathering}

\todo[inline]{WHAT SHOULD WE COVER HERE THAT IS NOT COVERED IN CHAPTER 13 (WEB SCRAPING), given that we also plan to cover API's there?}

Many data that are interesting to those analyzing communication are nowadays gathered online.
We can distinguish several scenarios of very different complexity.

First, data sets may already been compiled as, for instance, csv files or json files and just need to be downloaded.
While we could load these ressources directly from within our R or Python-script, if these are single files, we probably would rather just download them manually and use the methods we already learned in this chapter to read them into a data frame. This also has the advantage that we do not cause any unnecessary load by downloading the same data over and over again when re-running our code.

Second, api with wrapper

Third, general api handling

Lastly, we might want to automatically download large quanties of web pages themselves and extract relevant information. For instance, we might want to retrieve customer reviews from website or articles from news sites. This usually involves a more sophisticated approach called \emph{web scraping}, to which we devote a whole chapter later in this book (ADD REFERENCE CHAPTER WEB SCRAPING)




