\chapter{From file to data frame and back}
\label{chap:filetodata}


\begin{abstract}{Abstract}
  This chapter teaches you basics of file handling, such as different file formats and encodings. It introduces csv files, json files, plain text files, and binary file formats. We discuss different approaches to organizing data in files, and how to use dataframes to read from and write to these files.  Finally, we provide guidance for retrieving example datasets.
\end{abstract}

\keywords{file formats, encodings, reading and writing files, data frames, datasets}

\begin{objectives}
\item Know how to handle different encodings and dialects
\item Make an informed choice for a file format
\item Know how to access existing datasets
\end{objectives}

\begin{feature}
We use basic Python and R functionality to handle files. Additionally, we use \pkg{pandas} (Python) and \pkg{haven}/\pkg{tidyverse} (R) to read and write tables.
\end{feature}


\section{Why and when do we use data frames?}

In Section~\ref{sec:datatypes}, we introduced basic data types: strings (which contain text), integers (which contain whole numbers, or numbers without anything 'behing the dot'), floats (floating point numbers; numbers with decimals), and bools (Boolean values, True or False). 
We also learned that a series of multiple values (e.g., multiple integers, multiple strings) can be stored in what we call a vector (R) or a list (Python).

In most social-scientific applications, however, we do not deal with isolated series of values. We rather want to link multiple values to each other. One way to achieve this are dictionaries (see Section~\ref{sec:datatypes}).
Such data structures are really useful for nested data: For example, if we would not want to store people's age but their addresses, we could store a dict within a dict.
%\rpyex{chapter06/snippets/listvsdict}{CAPTION}
In fact, as we will see later in this chapter, many data that computational social scientists come in such a format. For instance, a tweet consists of the tweet text (a string), the number of likes (an integer), a list of hashtags it contains, and so on.

But ultimately, for many social-scientific analysis, a tabular data format is preferred. We are used to think of observations (cases) as rows in a table, and as columns containing different things that are measured (e.g., age, gender, days per week of newspaper reading, ...). It also simplifies how we can run many statistical analyses later on.

We could simply construct a list of lists to achieve such a tabular data format.
In fact, this list-of-lists technique is often used to store tabular data or matrices, and you will probably encounter it in some examples in this book or elsewhere. The list-of-lists approach is very low-level, though: If we wanted to, for instance, insert a column or a row at a specific place, writing the code to do so can be cumbersome. There are also no things like column headers, and no consistency checks: nothing would warn us if one row actually contained more 'columns' than another, which should not be the case in a rectangular table.

To make our lifes easier, we can therefore use a data structure called a data frame. 
Data frames can be generated from list-of-list structures, from dictionaries, and many others.
One way of doing so is shown in \refex{createdataframe}, but very often, you'd rather read data from a file or an online resource directly into a dataframe (see Section~\ref{sec:reading}).

\pyrex[output=both, caption=Creating a dataframe from other datastructures]{chapter06/createdataframe}

In this book, we use dataframes a lot, because they are very convenient for handling tabular data, and because they provide a lot of useful convenience functionality, instead of requiring us to re-invent the wheel all the time. In the next section, we will discuss some of them.

Of course, there are some situations when dataframes are \emph{not} a good choice to organize your data:
\begin{itemize}
\item Your data is one-dimensional. Think, for example, of ressources like a list of stopwords, or a list of texts without any meta-information.
\item Your data do not have a tabular structure. Think, for example, of deeply nested data, or of very messy data.
\item Your data are so large that you cannot (or do not want to) load it into memory. For instance, if you want to process the text of all articles on Wikipedia, you probably want to process them one-by-one instead of loading all articles at the same time.
\end{itemize}

%\subsection{Basic operations on data frames}

%When retrieving data for external sources (Section~\ref{sec:gathering}), we can convert the data we retrieved into a dataframe using the techniques outlined in the previous paragraphs. In many other cases, we will read them directly from files instead (Section~\ref{sec:reading}).

Therefore, you will come across (and we will introduce you to) examples in which we do \emph{not} use data frames to organize our data.
But in most cases we will, because they make our life easier:
Once we constructed our data frame, we have a range of handy functions at our disposal, that allow us to select rows or columns, add new rows or columns, apply functions to them, and so on.
We will discuss these in Chapter~\ref{chap:datawrangling}.

But how do we -- toy examples like those in \refex{createdataframe} aside -- get data into and out of dataframes? 




%Table~\ref{tab:dataframecommands} gives an overview of some of them. Many of these commands will come back in subsequent chapters, but we encourage you to already play around a bit with them. That is more fun with a real dataset - and that's why we will load some in the next section.


%\begin{table}[]
%\caption{Basics of data frame handling}
%\label{tab:dataframecommands}
%\begin{tabularx}{\textwidth}{XXXXX}
%\toprule
%                         & pandas data frame                 & R data.frame & R tibble \\ \midrule
%select rows by index     & df.iloc{[}1, ...... ..            &              &          \\
%select columns by number & df.iloc...                        &              &          \\
%select columns by name   & df{[}'mycolumn'{]} or df.mycolumn &              &          \\ \bottomrule
%\end{tabularx}
%\end{table}




\section{Reading and saving data}
\label{sec:reading}

\subsection{The role of files}

In statistical software like SPSS or Stata, or in all typical office applications for that matter, you \emph{open} a file, do some work on it, and then \emph{save} the changes to the same file once you are done. You basically ``work on that file''.

That's not how your typical workflow in R or Python looks like. Here, you work on one or multiple data frames (or some other data structures). That means that you might start by \emph{reading} the contents of some file into a data frame, but once that is done, there is no link between the dataframe and that file any more. Once your work is done, you can save your dataframe to a file, of course, but it is a good practice not to overwrite your input file, so that you can always go back to where you started. A typical workflow would rather look like this:
\begin{enumerate}
\item Read raw data from file \ttt{myrawdata.csv} into data frame \ttt{df}"
\item Do some operations and analyses on df
\item Save df to file \ttt{myfinaldata.csv}
\end{enumerate}
Note that the last step is not even necessary, but may be handy if running the script takes very long, or if you want to re-distribute the resulting file.

The format in which we read files into a data frame and the format to which we save our final data frame also by no means needs to be identical. We can, for example, read data created by someone else in Stata's proprietary \ttt{.dta} format into a dataframe and later save it to a .csv table.

While we sometimes do not have the choice in which format we get our input data, we have a range of options regarding our output data. We usually prefer formats that are \emph{open} and \emph{interoperable} for this, which ensures that they can be used by as many people as possible, also in the future, and that they are not tied to any specific (proprietary) product.

The most common file formats that are relevant to us are listed in Table~\ref{tab:fileformats}. \ttt{txt} files are particularly useful for long texts (think of one file containing one newspaper article or even a whole book), but they are bad for storing associated meta data. \ttt{csv} files are the default choice for tabular data, and \ttt{json} files allow us to store nested data in a dictionary-like format (see above). 

For the sake of completeness, we also listed the native Python and R formats pickle, RDS, and RDA. Because their lack of interoperability, they are not very suitable for long-term storage or for sharing data, but they can have a place in a workflow as an intermediate step to solve the issue that none of the other formats are able of storing all properties of a dataframe (e.g., the csv file cannot store whether a given column in an R dataframe is to be understood as containing strings such as 'man', 'woman', 'non-binary' or a factor with the three levels man, woman, non-binary). If it is of importance to store an object (such as a dataframe) exactly as-it-is, we can use these formats. One of the rare instances where we use these formats is in \refex{reuse}, where we store machine learning models for later reuse.

\begin{table}[]
\caption{Basics of data frame handling \label{tab:fileformats}}{%
\begin{tabular}{@{}llll@{}}
\toprule
        & Used for?             & open   & interoperable?\\ \midrule
txt     & plain text            &yes & yes            \\
csv     & tabular data          & yes & yes            \\
json    & nested data, key-value pairs   & yes & yes             \\ 
pickle  & Python objects        & yes & no     \\ 
RDS/RDA & R objects             & yes & no \\ \bottomrule
\end{tabular}}{}
\end{table}


\subsection{Encodings and dialects}
\label{sec:encodings}
\ttt{txt} files, \ttt{csv} files, and \ttt{json} files are all files that are based on text. Unlike binary file formats, you can read them in any text editor. Try it yourself to understand what is going on under the hood. 

Download a csv file (such as \url{http://cssbook.net/d/gun-polls.csv})
and open it in a text editor of your choice. Some people swear that their preferred editor is the best (google to learn about the vi vs. emacs war for some entertainment), but if you have no strong feeling, then Notepad++, Atom, or Sublime may be good choices that you may want to look into.

As you will see (Figure~\ref{fig:csv-in-editor}), a csv file internally just looks like a bunch of text in which each line represents a row and in which the columns are separated by a comma (hence the name comma seperated values (csv)).
Looking at the data in a text editor is a very good way to find out what happens if reading your files into a data frame does not work as expected - which can happen more frequently than you would expect.

Mostly due to historical reasons, not every text based file (which, as we have seen, includes csv files) is internally stored in the same way.
For a long time, it was common to \emph{encode} in such a way that one character mapped to one byte. That was easy from a programming perspective (after all, the n-th character of a text can directly be read from and written to the n-th byte of a file) and also storage-efficient. But given that a byte consists of 8 bits, that means that there are only 256 possible characters. All letters in the alphabet in uppercase, again in lowercase, numbers, punctuation, some control characters -- and you are out of characters. Due to this limitation, there were different encodings or codepages for different languages that told a program which value should be interpreted as which character.

We all know the phenomenon of garbled special characters, like German umlauts or Scandinavian characters like ø, å, or œ being displayed as something completely different. This happens when files are read with a different encoding than the encoding that was used for creating them.

In principle, this issue has been solved due to the advent of Unicode. Unicode allows to handle all characters from all scripts, including emoticons, Korean and Chinese characters, and so on. The most popular encoding for Unicode characters is called UTF-8, and it has been around for decades. 

To avoid any data loss, it is advisable to make sure that your whole workflow uses UTF-8 files. By far most modern applications support UTF-8, even though some still by default use a different encoding (e.g., 'Windows-1252') to store data. As Figure~\ref{fig:csv-in-editor} illustrates, you can use a text editor to find out what encoding your data has, and many editors also offer an option to change the encoding. However, you cannot recover what has been lost (e.g., if at one point you saved your data with an encoding that only allows 256 different characters, it follows logically that you cannot recover that information).


\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figures/ch6_csv-in-editor}
\caption{A csv file opened in a text editor, illustrating that the columns are separated by commas, and showing the encoding and the line endings.}
\label{fig:csv-in-editor}
\end{figure}

As we will show in the practical code examples below, you can also force Python and R to use a specific encoding, which can come in handy if your data arrives in a legacy encoding.

Related to the different encodings a file can have, but less problematical, are different conventions of how a \emph{line ending} is denoted. Windows-based programs have been using a Carriage Return followed by a Line Feed (denoted as \texttt{\textbackslash r\textbackslash n}), very old versions of MacOS used a Carriage Return only (\texttt{\textbackslash r}), and newer versions of MacOS as well as Linux use a Line Feed only (\texttt{\textbackslash n}). In our field, the Linux (or Unix) style line endings have become most dominant, and Python 3 even automatically converts Windows style line endings to Unix style line endings when reading a file -- even on Windows itself.

A third difference is the use of so-called \emph{byte-order markers} (BOM). In essence, a BOM is an additional byte added to the beginning of a text file to indicate that it is a utf-encoded file and to indicate in which order the bytes are to be read (the so-called endianness). While informative, this can cause trouble if your program does not expect that byte to be there. In that case, you might either want to remove it or explicitly specify the encoding as such (e.g., 'utf-8-bom' instead of 'utf-8' in the examples below).


In short, the most standard form in which you probably want to encode your data is in UTF-8 with Linux-style line endings without the use of a byte-order marker.


In the case of reading and writing csv files, we thus need to know the encoding, and potentially also the line ending conventions and the presence of a byte-order marker. However, there are also some additional variations that we need to consider. There is no definite definition of how a csv file needs to look like, but there are multiple dialects that are widely used. They mainly differ in to aspects: the delimiter that is chosen, and the quoting an/or escaping of values.

First, even though csv stands for comma separated values, one could use other characters instead of a comma to separate the columns. In fact, because many countries use a comma instead of a dot to as a decimal separator (\$10.30 vs 10,30€), in many countries a semicolon (';')is used instead of a comma as column delimiter. To avoid the possible confusion, others use a tab character (\texttt{\textbackslash t}) to seperate columns. Sometimes, these files are then called a tab-seperated file, and instead of .csv, they may have a file extension such as \ttt{.tsv}, \ttt{.tab}, or even \ttt{.txt}. However, this does not change the way how you can read them - but what you need to know is whether your columns are seperated by \texttt{,}, \texttt{;}, or \texttt{\textbackslash t}. 

Second, there may be different ways of how to deal with strings as values in a csv file. For instance, it may be that a specific value contains the same character that is also used as a delimiter. These cases are usually resolved by either putting all strings into quotes, putting only strings that contain such ambiguities in quotes, or by prepending the ambigous character with a specific escape character. Most likely, all of this is just handled automatically under the hood, but in case of problems, you might want to look into this and check out the documentation of the packages you are using on how to specify which strategy is to be used.

Let's get practical and try out reading and writing files into a data frame (\refex{readfiles}).

\pyrex[output=none, caption=Reading files into a dataframe]{chapter06/readfiles}

Of course, we can read more than just csv files. In the Python
example, you can use tabcompletion to get an overview of all file
formats Python supports: Type |pd.read| and then press the TAB key to
get a list of all supported files. For instance, you could to
|pd.read_excel('test.xlsx')|, |df3 = pd.read_stata('test.dta')|, or
|df4 = pd.read_json('test.json')| Similarily, for R, you can hit TAB
after typing |haven::| to get an overview over functions such as
|read_spss|.

\subsection{File handling beyond data frames}
Dataframes are a very useful data structure for organizing and analyzing data, and will come back in many examples in this book.
However, not all things that we might want to read from a file needs to go into a dataframe.
Imagine if we have a list of words that we later want to remove from some texts (so-called stopwords, see \refchap{protext}).
We could make a list (or vector) of such words directly in our code. 
But if we have more than a couple of such words, it is easier and more readable to keep them in an external file. We could create a file \texttt{stopwords.txt} in a text editor with one of such words per line:

\begin{lstlisting}
and
or
a
an
\end{lstlisting}

If you are too lazy to create this list yourself, you could also
download one from \url{http://cssbook.net/d/stopwords.txt} and save it
in the same directory as your Python or R script.

Then, you can read this file into a vector or list  (see \refex{readingstopwords}).

\pyrex[output=none, caption=Reading files without dataframes]{chapter06/stopwords}

\pyrex[output=none, caption={More examples for reading from and writing to files. Because dictionaries do not exist in R and handling non-rectangular data is more uncommon in R, we do not show an example for direct interaction with JSON files without involvement of a dataframe in the R example.}]{chapter06/extendedfilehandling}

\refex{extendedfilehandling} provides you with some more elaborate code examples that allows us to dig a bit deeper into the general way of handling files.

In the Python example,  we can open a file and assign a handle to it that allows us to refer to it (the name of the handle is arbitrary, let's just call it \texttt{f} here.
Then, we can use a for loop to iterate over all lines in the file and add it to a list

The \texttt{mode = 'r'} specifies that we want to read from the file. \texttt{mode = 'w'} would open the file for writing, create it if necessary, and immediately deletes all content that may have been in there if the file already existed (!).
Note that the \texttt{.strip()} is necessary to remove the line ending itself, and also possible whitespace at the beginning or end of a line.
If we wanted to save our stopwords, we could do this in a similar way: We first open the file (this time, for writing), and then use the file handle's methods to write to it.
We are not limited to plain text files, here. For instance, we can use the same approach to read json files into a python dict or to store a python dict into a json file.

We could also combine this with a for loop that goes over all files in a dictionary.
Imagine we have a folder full of positive movie reviews, and another one full of negative movie reviews that we want to use to train a machine learning classifier (see Section~\ref{sec:supervised}).
Let's further assume that all these reviews are saved as \texttt{.txt} files.
We can iterate over all of them, as shown in \refex{reviewdata}. If you want to read text files into a dataframe in R, the \pkg{readtext} package may be interesting for you.


\section{Data from online sources}
\label{sec:gathering}

Many data that are interesting to those analyzing communication are
nowadays gathered online.  In Chapter~\ref{chap:scraping}, you will
learn how to use APIs to retrieve data from web services, and how to
write your own web scraper to automatically download large quanties of
web pages and extract relevant information. For instance, you might
want to retrieve customer reviews from website or articles from news
sites.

In this section, however, we will focus on how to re-use existing
datasets that others have made available online. For instance, the
open science movement has led to more and more datasets being shared
openly using repositories such as Dataverse, Figshare, or
others. Re-using existing data can be very good for several reasons:
First, to confirm (or not) the conclusions drawn by others; second, to
not waste ressources by re-collecting very similar or even identical
data all over again; and third, because gathering a large,
high-quality dataset might just not be feasible with your means. This
is especially true when you need annotated (i.e., hand-coded) data for
supervised machine learning purposes (Chapter~\ref{chap:introsml}).

We can distinguish between two types of existing online datasets:
datasets that are inherently interesting, and so-called toy datasets.

Toy datasets may include made-up data, but often, these data are
real. However, they are not analyzed to gain scientific insights (any
more), as they may be too small, outdated, or already analyzed
all-over again. These provide a great way, though, to learn and
explore new techniques: After all, the results and the characteristics
of the data are already known. Hence, such toy datasets are often
even included in R and Python packages. Some of them are really
well-known in teaching (e.g., the iris dataset containing measurements
of some flowers; or the titanic dataset containing statistics on
survival rates of passengers on the Titanic; or the MPG dataset on car fuel consumption). Many of these are included
in packages like \pkg{scikit-learn}, \pkg{seaborn}, or \pkg{ggplot2}-- have a look at their documentation.

For instance, the 20 Newsgroups dataset contains $18,846$ posts from
newsgroups plus the groups where they were posted
(\refex{20newsgroups}). This can be an interesting resource for
exercising with natural language processing, unsupervised, and
supervised machine learning. Another interesting resource are
collections of political speeches, such as the state-of-the-union
speeches from the US, which are available in multiple packages
(\refex{sotudata}).
Other interesting datasets with large collections of textual data may
be the Financial News dataset compiled by \cite{Chen2017} or the
political news dataset compiled by \cite{Horne2018}.


\pyrex[output=py, input=both,caption={In Python, \pkg{scikit-learn} has a convenience function to automatically download the 20 newsgroup dataset and automatically clean it up. In R, you can download the raw version (there are multiple copies floating around on the internet) and perform the cleaning yourself.}]{chapter06/20newsgroups}

\pyrex[output=r, input=both, format=table, caption={A collection of US state-of-the-union speeches is available in multiple packages in various forms.}]{chapter06/sotudata}



There are also some more generic ressources that you may want to consider for finding
more datasets to play around with. On  \url{https://datasetsearch.research.google.com},
you can search for datasets of all kinds, both really interesting ones and toy datasets.
Another great research is \url{https://kaggle.com}, a site that hosts data
science competitions.




