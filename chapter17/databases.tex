\section{Storing data in SQL and noSQL databases}
\label{sec:databases}

\subsection{When to use a database}
In this book, we have so far stored our data in files. In fact, before
covering the wide range of methods for computational analysis, we
discussed some basics of file handling
(\refchap{filetodata}). Probably, you did not experience any major
trouble here (apart from ocassional struggles with non-standard
encodings, or confusion about the delimiters in a csv file). On the
other hand, the examples we used were still modest in size: usually,
you were dealing with a handful of csv files; the maximum you had to
deal with where the 50,000 text files from the IMDB movie review
dataset.

In particular, when loading your data into a dataframe, you copied all
the data from your disk into memory\footnote{In fact, this is
  sometimes a reason to avoid dataframes: for instance, it is possible
  to use a generator that reads data line-by-line from a file and
  yields them to \pkg{scikit-learn}. In this way, only \emph{one} row
  of data is in your memory at the same time (see \refsec{functions}).}

But what if we want to scale up our analyes a bit
\cite[see][]{Trilling2018b}? Maybe we want to build up a larger
datacollection, maybe even share it with multiple team members, search
and filter our data, or collect it over a larger timespan? An
example may illustrate the problems that can arise.

Imagine you do some web scraping (\refchap{scraping}) that goes beyond
a few thousand texts. Maybe you want to visit relevant news sites on a
regular basis (say, once an hour) and retrieve everything that's
new. How do you store your data then? You could append everything to a
huge csv file, but this file would quickly grow so large that you
cannot load it into memory any more. Besides, you may run the risk of
corrupting the file if something goes wrong in one of your attempts to
extend the file. Or you could also write each article to a new, separate file.
That's maybe more failsafe, but you would need to design a good way
to organize the data. In particular, devising a method to search
and find relevant files would be a whole project in itself.

Luckily, you can outsource all these problems to a database that you can
install on your own computer or possibly on a server (in that case, make
sure that it is properly secured!). In the example, the scraper, which
is running once an hour, just sends the scraped data to the database
instead of to a file, and the database will take care of storing it.
Once you want to retrieve a subset of your articles for analysis,
you can send a query to the database and read from it. Both Python and
R offer integration for multiple commonly used databases. It is even
possible to directly get the results of such a database query in
form of a dataframe.

We can distinguish two main categories of databases
that are most relevant to us \citep[see also][]{Gunther2018}:
relational databases (or SQL-databases) and noSQL-databases. Strictly
speaking, SQL (``structured query language'') is a query language for
databases, but it is so widespread that it is used almost synonymously
for relational databases. Even though they have been around for
already 50 years (\cite{Codd1970}), relational databases still are very
powerful and very widely used.  They consist of multiple tables that
are linked by shared columns (keys). For instance, you could imagine a
table with the orders placed in a webshop that has a column
|customer-id|, and a different table with addresses, billing
information, and for each |customer-id|. Using filter and join
operations (\refchap{datawrangling}), one can then easily retrieve
information on where the order has to be shipped. A big advantage of
such a relational database is that, if a customer places 100 orders,
we do not need to store their address 100 times, but only once, which
is not only more efficient in terms of storage, but also prevents
inconsistencies in the data.

In contrast to SQL databases, noSQL databases are not based on tables,
but use concepts such as ``documents'' or key-value pairs, very much
like Python dictionaries or JSON files. These types of databases are
particularly interesting when your data are less well-structured. If
not all of your cases have the same variables, or if the content is not
well-defined (let's say, you don't know exactly in what format the date
of publication on a news site will be written), or if the data structure
may change over time, then it is hard or impossible to come up with a
good table structure for an SQL database. Therefore, in many ``big data''
contexts, noSQL databases are used, as they -- depending on your
configuration -- will happily accept almost any kind of content you dump
in them. This comes, of course, at the expense of giving up advantages
of SQL databases, such as the avoidance of inconsistencies. But often,
you may rather want to store your data first and clean up later, rather
than risking that data collection fails because you enforced a too strict
structure. Also, there are many noSQL databases that are very fast in
searching full text -- something that SQL databases, in general, are
not optimzied for.

Despite all of these differences, both SQL and noSQL databases can play
the same role in the computational analysis of communication. They both
help you to focus on data collection and data analysis without needing
to device an ingenious way to store your data. They both allow for much
more efficient searching and filtering than you could design on your own.
All of this becomes especially interesting when your dataset grows too
large to fit in memory, but also when your data are continuously changed,
for instance because new data are added while scraping.


\subsection{Choosing the right database}
Choosing the right database is not always easy, and has many consequences
for the way you may conduct your analyses. As \citet{Gunther2018} explain,
this is not a purely technical choice, but impacts your social-scientific
workflow. Do you want to enforce a specific structure from the very
start, or do you rather want to collect everything first and clean up
later? What is your tradeoff between avoiding any inconsistency and risking
to throw away too much raw information? 

Acknowledging that there are often many different valid choices, and at
the risk over oversimplifying matters, we will try to give some guidance
in which databases to choose by offering some guiding questions.


\paragraph{How is your data structured?} Ask yourself: Can I organize my data
in a set of relational tables? For instance, think of television
viewing data: There may be a table that gives information on when the
television set was switched on and which channel was watched and by
which user id. A second table can be used to associate personal
characteristics such as age and gender with the user id. And a third
table may be used to map the time stamps to details about a specific
program aired at the time.  If your data looks like this, ask
yourself: Can I determine the columns and the data types for each
column in advance?  If so, then a SQL database such as \pkg{MySQL},
\pkg{PostgreSQL}, or \pkg{MariaDB} is probably what you are looking
for. If, on the other hand, you cannot determine such a structure a
priori, if you believe that the structure of your information will
change over time, or if it is very messy, then you may need a more
flexible, noSQL approach, for instance using \pkg{MongoDB} or
\pkg{ElasticSearch}.

\paragraph{How important is full-text search for you?} SQL databases can handle numeric datatypes as well as text datatypes, but they are usually not optimized for the latter. They handle short strings (such as usernames, addresses, and so on) just fine, but if you are interested in full-text search, they are not the right tool for the job. This is in particular true if you want to be able to do fuzzy searches where, for instance, also documents containing the plural of a word that you searched for as singular are found. Databases of, for instance, news articles, tweets, transcripts of speeches, or other documents are much better accessible in a database such as \pkg{ElasticSearch}.


\paragraph{How flexible does it need to be?} In relational databases, it is relatively hard to change the structure afterwards. In contrast, a noSQL database has no problem whatsoever with adding a new document that contains keys that did not exist before. There is no assumption that all documents contain the same keys. Therefore, if it is hard to tell in advance which ``columns'' or ``keys'' may represent your data best, you should stay clear of SQL databases. In particular, if you think of gradually extending your data and use it on a long timeline for re-use, potentially even by multiple teams, the flexibility of a noSQL database may be a game changer.



\subsection{A brief example using SQLite}

Installing a database server such as \pkg{mysql}, \pkg{mariadb} (an
open-source fork of mysql), \pkg{MongoDB}, or \pkg{Elasticsearch} is
not really difficult (in fact, it may already be come pre-packaged
with your operating system), but the exact configuration and setup may
differ widely depending on your computer and your needs. Most
importantly, especially if you store sensitive data in your database,
you will need to think about authentication, roles, etc. --- all
beyond the scope of this book.

Luckily, there is a compromise between storing your data in files
that you need to manage yourself and setting up a database server,
locally or remotely. The library \pkg{SQlite} offers a self-contained
database engine -- essentially, it allows you to store a whole
database in one file and interact with it using the SQL query language.
Both R and Python offer multiple ways of directly interacting with
sqlite files (\refex{sqlite}). This gives you access to some great
functionality way: after all, you can issue (almost) any SQL command
now, including (and maybe most imporantly) commands for filtering,
joining, and aggregating data. Or you could consider immediately writing
each datapoint you get from an API or a webscraper (\refchap{scraping})
without risking to loose any data if connections time out or scraping
fails halfway.

\pyrex[input=both, output=py, format=table, caption={\pkg{SQLite} offers you database functionality without setting up a database server such as \pkg{mysql}}]{chapter17/sqlite}

Of course, \pkg{SQlite} cannot give you the same performance that a ``real'' mysql (or similar) installation could offer. Therefore, if your project grows bigger, or if you have a lot of read- or
write-operations per second, then you may want to switch at one
point. But as you can see in \refex{sqlite}, Python and R do not
really care about the backend: all you need to do is to change the
connection |conn| such that it points to your new database instead of
to the mysql database.


