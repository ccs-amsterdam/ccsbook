\chapter{Introduction (Damian)}
\label{chap:introduction}


\section{The role of computational analysis in social science}
The use of computers is nothing new in the social sciences. In fact,
one could argue that some disciplines within the social sciences have
even be early adopters of computational approaches. Take the
gathering and analyzing of large-scale survey data, going back until
the use of the Hollerith Machine in the 1890 US census. Long before
every scholar had a personal computer on their desk, social scientists
were using punch cards and mainframe computers to deal with such
data. And also if we think of the analysis of \emph{communication}
more specifically, we see attempts to automate content analysis
already in the 1960's \citep[see, e.g.,]{Scharkow2017}.

Yet, something has profundly changed in the last decades. The amount
and kind of data we can collect as well as the computational power we
have access to have increased dramatically. In particular, digital
traces that we leave when communicating online, from access logs to
comments we place, have required new approaches \citep[see,
  e.g.,]{Trilling2017b}. At the same time, better computational
facilities now allow us to ask questions we could not answer before.

\citet{Gonzalez-Bailon2017}, for instance, argued that the
computational analysis of communication now allows us to test theories
that have been formulated a century ago, such as XXXXXXXXXXXXXXX. And
\citet{Salganik2019} shows that XXXXXXXXXXXXXXXXXXXXXXXXX.

A frequent misunderstanding, then, about computational approaches is
that they would somehow be a-theoretical. This is probably fueled by
clich\'{e}s coied during ``Big Data''-hype in the 2010's, such as the
infamous saying that in the age of Big Data, correlation is enough,
but one could not be more wrong: As the work of
\cite{Kitchin2014,Kitchen2014data} shows, computational approaches can
be well situated within existing epistemologies. The computational
scientists' toolbox includes both more data-driven and more
theory-driven techniques; some are more bottom-up and inductive,
others are more top-down and deductive. What matters here, and what is
often overlooked, is in which stage of the research process they are
employed.

In particular, we suggest to think of the data collection and data
analysis process as a pipeline. To test, for instance, a theoretically
grounded hypothesis about personalization in the news, we could
imagine a pipeline that starts with scraping online news, proceeds
with some natural-language processing techniques such as Named Entity
Recognition, and finally tests whether the mentioning of persons has
an influence on the placement of the stories. We can distinguish here
between parts of the pipeline that are just necessary but not
inherently interesting to us, and parts of the pipeline that answer a
genuiely interesting question. In this example, the inner workings of
the Named Entity Recognition step are not genuinly interesting for us
-- we just need to do it to answer our question. We do care about how
well it works, which biases it has, and so on, but we are indeed not
evaluating any theory here. We are, however, answering a theoretically
interesting when we look at the pipeline as a whole.  Of course, what
is genuinly interesting depends on one's discipline: For a
computational linguist, the workings of the named entity recognition
may be the interesting part, and our research question just one
possible ``downstream task''.

This distinction is also sometimes referred to as ``building a better
mouse trap'' vs. ``understanding'' \todo{Wouter has some lit or so on
  this, right?}  The book here is to some extend about both. When you
are building a supervised machine learning classifier to determine the
topic of each text in a large collection of news articles or
parliamentary speeches, you are building a (better) mouse trap. But as
a social scientists, your work does not stop there. You need to use
the mouse trap to answer some theoretically interesting question.



When planning this book, we needed to make a couple of tough
choices. We aimed to at least give an introduction to all techniques
that students and scholars that want to computationally analyze
communication probably will be confronted with. Of course, specific --
technical -- literature on techniques such as, for instance, machine
learning can go more in-depth, and the interested student may indeed
want to dive into one or several of the techniques we cover more
deeply. Our goal here is to offer enough working knowledge to apply
these techniques and to know what to look for.  While trying to cover
the breadth of the field without sacrificing too much depth when
covering each technique, we still needed to draw some boundaries. One
technique that some readers may miss is agent-based modeling
(ABM). Arguably, such simulation techniques are an important technique
in the computational social sciences more broadly
\citep{cioffi-revilla2014}, and they have recently been applied to the
analysis of communication as well
\citep{Waldherr2014,Wettstein2020}. Nevertheless, when reviewing the
curricula of current courses teaching the computational analysis of
communication, simulation approaches seem not to be at the core of
such analyses (yet).  Instead, when looking at the use of compuational
techniques in fields such as journalism studies
\citep[e.g.,][]{Boumans2016}, media studies \todo{add reference}, or
the text-as-data movement\todo{add reference}, we see a core of
techniques that are used all-over again, and that we therefore
included in our book. In partiuclar, these are techniques for
gathering data such as web scraping or the use of API's; techniques
for dealing with text such as natural language processing and
different ways to turn text into numbers; supervised and unsupervised
machine learning techniques; and network analysis.



\section{Why Python and/or R?}
By far most work in the computational social sciences is done using
Python and/or R. Sure, for some specific tasks there are standalone
programs that are occasionally used; and there are some applications
written in other languages such as C or Java. But we believe it is
fair to say that it very hard to delve into the computational analysis
of communication without learning at least either Python or R, and
preferrably even both of them.

Some people have strong believes which language is ``better'' -- we do
not belong to them. Most techniques that are relevant to us can be
done in either language, and personal preference is a big factor. R
started out as a statistical programming environment, and that
heritage is still visible, for instance in the strong emphasis on
vectors, factors, et cetera, or the possibility to estimate complex
statistical models in just one line of code. Python started out as a
general-purpose programming language, which means that some things we
do feel a bit more `low-level' -- Python abstracts away less of the
underlying programming concepts than R does. This sometimes gives us
more flexibility. In the last years, however, Python and R have been
growing closer to each other: With modules like \pkg{pandas} and
\pkg{statsmodels}, Python now has R-like functionality handling data
frames and estimating common statistical models on them; and with
packages such as \pkg{quanteda}, handling of text -- traditionally a
strong domain of Python -- has become more accessible in R.

This is the main reason why we decided to write this ``bi-lingual''
book. We wanted to teach techniques for the computational analysis of
communication, without enforcing a specific implementation. We hope
that the reader learns from our book, say, how to transform a text
into features and how to choose an appropriate machine learning model,
but find it of less importance in which language this happens.

Yet, sometimes, there are good reasons to choose one language above
the other. For instance, many machine learning models in R under the
hood create a dense matrix, which severly limits the amount of
documents and features one can use; also, some complex web scraping
tasks are maybe easier to realize in Python. On the other hand, R's
data wrangling and visualization techniques in the \pkg{tidyverse}
environment are known for their user-friendliness and quality.  In the
rare cases where we believe that R or Python is clearly superior for a
given task, we indicate so; for the rest, we believe that it is up to
the reader to choose.


\section{How to use this book?}

This book differs from more technically oriented books on the one hand
and more conceptual books on the other hand. We do cover the technical
background that is necessary to understand what is going on, but we
keep both computer science concepts and mathematical concepts to a
minimum. For instance, if we had written a more technical book about
Programming in Python, we would have introduced rather early and in
detail to concepts such as classes, inheritence, and instances of
classes. Instead, we decided to give such information only as
additional background where necessary and focus, rather pragmatically,
on the application of techniques for the computational analysis of
communication. Vice versa, if we had written a more conceptual book on
new methods in our field, we would have given more emphasis to
epistemological aspects, and had skipped the programming examples,
which are now at the core of this book.

We do not expect much prior knowledge from the readers of this
book. Sure, some affinity with computers help, but there is no strict
requirement on what you need to know. Also in terms of statistics, it
has helped if you have heard of terms such as correlation or
regression analysis, but even if your knoweldge here is rather
limited, you should be able to follow along. Also here, a bit more
previous knowledge helps, but you can also aquire it along the way.

This also means that you may be able to skip chapters. For instance,
if you already work with R and/or Python, you may not need our
detailed instructions at the beginning. Still, the book follows a
logical order in which chapters build on previous ones. For instance,
when explaining supervised machine learning on textual data, we expect
you to be familiar with previous chapters that deal with machine
learning in general, or with the handling of textual data.

This book is designed in such a way that it can be used as a text book
for introductory courses on the computational analysis of
communications. Often, such courses will be on the gradutate level,
but it is equally possible to use this book in an undergraduate
course; maybe skipping some parts that may go too deep. All code
examples are not only printed in this book, but also available
online. Students as well as social-scientists who want to brush up
their skillset should therefore also be able to use this book for
self-study, without a formal course around it. Lastly, this book can
also be a reference for readers asking themselves: ``How do I again
have to do this?''. In particular, if the main language you work in is
R, you can look up how to do similar things in Python and vice versa.

But regardless of the context in which you use this book, one thing is
for sure: The main thing you need to do is practice. Try to adapt the
code examples in this book to questions you are interested in, try to
apply the techniques to your data, and try to play with the code. It's
not only satisfying, it's also fun!
